{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952a24a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics.functional import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tdc.multi_pred import DTI\n",
    "\n",
    "kiba = DTI(name=\"kiba\")\n",
    "kiba_split = kiba.get_split()\n",
    "\n",
    "train_df = kiba_split['train']\n",
    "valid_df = kiba_split['valid']\n",
    "test_df = kiba_split['test']\n",
    "\n",
    "with open(\"data/drug/kiba_drug_embeddings.pkl\", \"rb\") as f:\n",
    "    molecule_embeddings = pickle.load(f)\n",
    "    \n",
    "with open(\"data/target/kiba_target_embeddings.pkl\", \"rb\") as f:\n",
    "    protein_embeddings = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775dbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, data, molecule_embedding, protein_embedding):\n",
    "        self.data = data\n",
    "        \n",
    "        self.molecule_embedding = molecule_embedding\n",
    "        self.protein_embedding = protein_embedding\n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # original embedding\n",
    "        molecule_sequence = self.molecule_embedding[self.data.loc[idx, \"Drug_ID\"]][1]\n",
    "        # reduced embedding\n",
    "        protein_sequence = self.protein_embedding[self.data.loc[idx, \"Target_ID\"]][2]\n",
    "        y = self.data.loc[idx, \"Y\"]\n",
    "                \n",
    "        return molecule_sequence, protein_sequence, y\n",
    "\n",
    "    \n",
    "def collate_batch(batch):\n",
    "    molecule_seq, protein_seq, y = [], [], []\n",
    "    \n",
    "    for (molecule_seq_, protein_seq_, y_) in batch:\n",
    "        molecule_seq.append(molecule_seq_)\n",
    "        protein_seq.append(protein_seq_)\n",
    "        y.append(y_)\n",
    "    \n",
    "    molecule_seq = torch.nn.utils.rnn.pad_sequence([torch.Tensor(t) for t in molecule_seq], batch_first=True)\n",
    "    protein_seq = torch.nn.utils.rnn.pad_sequence([torch.Tensor(t) for t in protein_seq], batch_first=True)\n",
    "    y = torch.tensor(y).float()\n",
    "    \n",
    "    return molecule_seq, protein_seq, y\n",
    "\n",
    "\n",
    "train_dataset = DTIDataset(train_df, molecule_embeddings['train'], protein_embeddings['train'])\n",
    "valid_dataset = DTIDataset(valid_df, molecule_embeddings['valid'], protein_embeddings['valid'])\n",
    "test_dataset = DTIDataset(test_df, molecule_embeddings['test'], protein_embeddings['test'])\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=12, \n",
    "                              pin_memory=True, prefetch_factor=10, \n",
    "                              drop_last=True, collate_fn=collate_batch, shuffle=True)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=12, \n",
    "                              pin_memory=True, prefetch_factor=10, \n",
    "                              collate_fn=collate_batch)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=12, \n",
    "                             pin_memory=True, prefetch_factor=10, \n",
    "                             collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f75978c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, input_dim=128, intermediate_dim=512, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        project_out = input_dim\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = (input_dim / heads) ** -0.5\n",
    "\n",
    "        self.key = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "        self.value = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "        self.query = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(intermediate_dim, project_out),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, data):\n",
    "        b, n, d, h = *data.shape, self.heads\n",
    "\n",
    "        k = self.key(data)\n",
    "        k = rearrange(k, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        v = self.value(data)\n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h=h)\n",
    "        \n",
    "        # get only cls token\n",
    "        q = self.query(data[:, 0].unsqueeze(1))\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attention = dots.softmax(dim=-1)\n",
    "\n",
    "        output = einsum('b h i j, b h j d -> b h i d', attention, v)\n",
    "        output = rearrange(output, 'b h n d -> b n (h d)')\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "    \n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 molecule_dim=128, molecule_intermediate_dim=256,\n",
    "                 protein_dim=256, protein_intermediate_dim=512,\n",
    "                 cross_attn_depth=1, cross_attn_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cross_attn_layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(cross_attn_depth):\n",
    "            self.cross_attn_layers.append(nn.ModuleList([\n",
    "                nn.Linear(molecule_dim, protein_dim),\n",
    "                nn.Linear(protein_dim, molecule_dim),\n",
    "                PreNorm(protein_dim, CrossAttention(\n",
    "                    protein_dim, protein_intermediate_dim, cross_attn_heads, dropout\n",
    "                )),\n",
    "                nn.Linear(protein_dim, molecule_dim),\n",
    "                nn.Linear(molecule_dim, protein_dim),\n",
    "                PreNorm(molecule_dim, CrossAttention(\n",
    "                    molecule_dim, molecule_intermediate_dim, cross_attn_heads, dropout\n",
    "                ))\n",
    "            ]))\n",
    "\n",
    "            \n",
    "    def forward(self, molecule, protein):\n",
    "        for i, (f_sl, g_ls, cross_attn_s, f_ls, g_sl, cross_attn_l) in enumerate(self.cross_attn_layers):\n",
    "            \n",
    "            cls_molecule = molecule[:, 0]\n",
    "            x_molecule = molecule[:, 1:]\n",
    "            \n",
    "            cls_protein = protein[:, 0]\n",
    "            x_protein = protein[:, 1:]\n",
    "\n",
    "            # Cross attention for protein sequence\n",
    "            cal_q = f_ls(cls_protein.unsqueeze(1))\n",
    "            cal_qkv = torch.cat((cal_q, x_molecule), dim=1)\n",
    "            # add activation function\n",
    "            cal_out = cal_q + cross_attn_l(cal_qkv)\n",
    "            cal_out = F.gelu(g_sl(cal_out))\n",
    "            protein = torch.cat((cal_out, x_protein), dim=1)\n",
    "\n",
    "            # Cross attention for molecule sequence\n",
    "            cal_q = f_sl(cls_molecule.unsqueeze(1))\n",
    "            cal_qkv = torch.cat((cal_q, x_protein), dim=1)\n",
    "            # add activation function\n",
    "            cal_out = cal_q + cross_attn_s(cal_qkv)\n",
    "            cal_out = F.gelu(g_ls(cal_out))\n",
    "            molecule = torch.cat((cal_out, x_molecule), dim=1)\n",
    "            \n",
    "        return molecule, protein\n",
    "    \n",
    "    \n",
    "class AttentionalDTI(nn.Module):\n",
    "    def __init__(self, \n",
    "                 cross_attention_layer, \n",
    "                 molecule_input_dim=128, protein_input_dim=256, hidden_dim=512, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cross_attention_layer = cross_attention_layer\n",
    "        \n",
    "        self.molecule_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(molecule_input_dim),\n",
    "            nn.Linear(molecule_input_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.protein_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(protein_input_dim),\n",
    "            nn.Linear(protein_input_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, molecule_seq, protein_seq):\n",
    "        molecule_out, protein_out = self.cross_attention_layer(molecule_seq, protein_seq)\n",
    "        \n",
    "        molecule_out = molecule_out[:, 0]\n",
    "        protein_out = protein_out[:, 0]\n",
    "        \n",
    "        # cls token\n",
    "        molecule_projected = self.molecule_mlp(molecule_out)\n",
    "        protein_projected = self.protein_mlp(protein_out)\n",
    "        \n",
    "        out = self.fc_out(molecule_projected + protein_projected)\n",
    "        \n",
    "        return out\n",
    "\n",
    "cross_attention_layer = CrossAttentionLayer()\n",
    "attentional_dti = AttentionalDTI(cross_attention_layer, cross_attn_depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496982f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "class DTI_prediction(pl.LightningModule):\n",
    "    def __init__(self, attentional_dti):\n",
    "        super().__init__()\n",
    "        self.model = attentional_dti\n",
    "\n",
    "        \n",
    "    def forward(self, molecule_sequence, protein_sequence):\n",
    "        return self.model(molecule_sequence, protein_sequence)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"valid_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25)\n",
    "        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "    \n",
    "    \n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor='valid_loss', save_top_k=5, dirpath='weights/Attentional_DTI_cross_attention_kiba_reduced_embedding', filename='attentional_dti-{epoch:03d}-{valid_loss:.4f}-{valid_mae:.4f}'),\n",
    "]\n",
    "\n",
    "model = DTI_prediction(attentional_dti)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=500, gpus=1, enable_progress_bar=True, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ee1465",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/ubuntu/Workspace/Attentional_DTI/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type           | Params\n",
      "-----------------------------------------\n",
      "0 | model | AttentionalDTI | 987 K \n",
      "-----------------------------------------\n",
      "987 K     Trainable params\n",
      "0         Non-trainable params\n",
      "987 K     Total params\n",
      "3.949     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d72588dfd04b91947e8aa41b678c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfcc725b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6d09fd445048208e8425809f6b0559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.16898369789123535, 'test_mae': 0.23121696710586548}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.16898369789123535, 'test_mae': 0.23121696710586548}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_fname = \"attentional_dti-epoch=261-valid_loss=0.1651-valid_mae=0.2343.ckpt\"\n",
    "\n",
    "model = model.load_from_checkpoint(\"weights/Attentional_DTI_cross_attention_kiba/\" + ckpt_fname, attentional_dti=attentional_dti)\n",
    "trainer.test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c310e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fc470ab0db4461b6af7164a19fbdbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 4118it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(model, test_dataloader)\n",
    "\n",
    "true_, pred_ = test_dataloader.dataset.data.Y, []\n",
    "\n",
    "for i in pred:\n",
    "    for j in i.tolist():\n",
    "        pred_.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "108dcac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 23530/23530 [27:19<00:00, 14.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8762425728288034"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cindex(Y, P):\n",
    "    summ = 0\n",
    "    pair = 0\n",
    "    \n",
    "    for i in tqdm(range(1, len(Y))):\n",
    "        for j in range(0, i):\n",
    "            if i is not j:\n",
    "                if(Y[i] > Y[j]):\n",
    "                    pair +=1\n",
    "                    summ +=  1* (P[i] > P[j]) + 0.5 * (P[i] == P[j])\n",
    "        \n",
    "            \n",
    "    if pair is not 0:\n",
    "        return summ/pair\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def r_squared_error(y_obs,y_pred):\n",
    "    y_obs = np.array(y_obs)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_obs_mean = [np.mean(y_obs) for y in y_obs]\n",
    "    y_pred_mean = [np.mean(y_pred) for y in y_pred]\n",
    "\n",
    "    mult = sum((y_pred - y_pred_mean) * (y_obs - y_obs_mean))\n",
    "    mult = mult * mult\n",
    "\n",
    "    y_obs_sq = sum((y_obs - y_obs_mean)*(y_obs - y_obs_mean))\n",
    "    y_pred_sq = sum((y_pred - y_pred_mean) * (y_pred - y_pred_mean) )\n",
    "\n",
    "    return mult / float(y_obs_sq * y_pred_sq)\n",
    "\n",
    "\n",
    "def get_k(y_obs,y_pred):\n",
    "    y_obs = np.array(y_obs)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    return sum(y_obs*y_pred) / float(sum(y_pred*y_pred))\n",
    "\n",
    "\n",
    "def squared_error_zero(y_obs,y_pred):\n",
    "    k = get_k(y_obs,y_pred)\n",
    "\n",
    "    y_obs = np.array(y_obs)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_obs_mean = [np.mean(y_obs) for y in y_obs]\n",
    "    upp = sum((y_obs - (k*y_pred)) * (y_obs - (k* y_pred)))\n",
    "    down= sum((y_obs - y_obs_mean)*(y_obs - y_obs_mean))\n",
    "\n",
    "    return 1 - (upp / float(down))\n",
    "\n",
    "\n",
    "def get_rm2(ys_orig, ys_line):\n",
    "    r2 = r_squared_error(ys_orig, ys_line)\n",
    "    r02 = squared_error_zero(ys_orig, ys_line)\n",
    "\n",
    "    return r2 * (1 - np.sqrt(np.absolute((r2*r2)-(r02*r02))))\n",
    "\n",
    "get_cindex(true_, pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ba116ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.686873981628849"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "get_rm2(true_, pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f4265f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8208179169928685"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "true_label = [1 if t >= 12.1 else 0 for t in true_]\n",
    "\n",
    "average_precision_score(true_label, pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820cd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
