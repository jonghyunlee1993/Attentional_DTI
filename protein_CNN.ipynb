{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16874d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.QED import qed\n",
    "from tdc.multi_pred import DTI\n",
    "\n",
    "from rdkit import RDLogger \n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# davis = DTI(name=\"Davis\")\n",
    "# davis.convert_to_log(form='binding')\n",
    "# davis_split = davis.get_split()\n",
    "\n",
    "kiba = DTI(name=\"KIBA\")\n",
    "kiba_split = kiba.get_split()\n",
    "\n",
    "train_df = kiba_split['train']\n",
    "valid_df = kiba_split['valid']\n",
    "test_df = kiba_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b78cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at weights/molecule_bert_pretrained-masking_rate_30 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import PreTrainedTokenizerFast, PreTrainedTokenizer\n",
    "\n",
    "molecule_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"data/drug/tokenizer_model/vocab.json\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    model_max_length=128\n",
    ")\n",
    "molecule_bert = BertModel.from_pretrained(\"weights/molecule_bert_pretrained-masking_rate_30\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a002759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n",
      "237\n"
     ]
    }
   ],
   "source": [
    "fasta_stoi = {\n",
    "    \"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \"H\": 7,\n",
    "    \"I\": 8, \"J\": 9, \"K\": 10, \"L\": 11, \"M\": 12, \"N\": 13, \"O\": 14,\n",
    "    \"P\": 15, \"Q\": 16, \"R\": 17, \"S\": 18, \"T\": 19, \"U\": 20, \"V\": 21, \n",
    "    \"W\": 22, \"Y\": 23, \"Z\": 24, \"X\": 25, \"*\": 26, \"-\": 27\n",
    "}\n",
    "\n",
    "fasta_itos = {\n",
    "    0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4: \"E\", 5: \"F\", 6: \"G\", 7: \"H\",\n",
    "    8: \"I\", 9: \"J\", 10: \"K\", 11: \"L\", 12: \"M\", 13: \"N\", 14: \"O\",\n",
    "    15: \"P\", 16: \"Q\", 17: \"R\", 18: \"S\", 19: \"T\", 20: \"U\", 21: \"V\", \n",
    "    22: \"W\", 23: \"Y\", 24: \"Z\", 25: \"X\", 26: \"*\", 27: \"-\" \n",
    "}\n",
    "\n",
    "def encode(data, stoi):\n",
    "    return [stoi[d] for d in data]\n",
    "\n",
    "\n",
    "def decode(data, itos):\n",
    "    return [itos[d] for d in data]\n",
    "\n",
    "sample_fasta = \"MTEITAAMVKELRESTGAGMMDCKNALSETNGDFDKAVQLLREKGLGKAAKKADRLAAEGLVSVKVSDDFTIAAMRPSYLSYEDLDMTFVENEYKALVAELEKENEERRRLKDPNKPEHKIPQFASRKQLSDAILKEAEEKIKEELKAQGKPEKIWDNIIPGKMNSFIADNSQLDSKLTLMGQFYVMDDKKTVEQVIAEKEKEFGGKIKIVEFICFEVGEGLEKKTEDFAAEVAAQL\"\n",
    "print(len(sample_fasta))\n",
    "print(len(encode(sample_fasta, fasta_stoi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db3b36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "\n",
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, data, molecule_tokenizer, fasta_stoi):\n",
    "        self.data = data\n",
    "        \n",
    "        self.molecule_tokenizer = molecule_tokenizer\n",
    "        self.fasta_stoi = fasta_stoi\n",
    "    \n",
    "        \n",
    "    def molecule_encode(self, molecule_sequence):\n",
    "        molecule_sequence = self.molecule_tokenizer(\" \".join(molecule_sequence), max_length=128, truncation=True)\n",
    "        \n",
    "        return molecule_sequence\n",
    "    \n",
    "    \n",
    "    def protein_encode(self, protein_sequence):\n",
    "        protein_sequence = torch.tensor([self.fasta_stoi[s] for s in protein_sequence]).long()\n",
    "        \n",
    "        return protein_sequence\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        molecule_sequence = self.molecule_encode(self.data.loc[idx, \"Drug\"])\n",
    "        protein_sequence = self.protein_encode(self.data.loc[idx, \"Target\"])\n",
    "        y = torch.tensor(self.data.loc[idx, \"Y\"]).float()\n",
    "                \n",
    "        return molecule_sequence, protein_sequence, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    molecule_seq, protein_seq, y = [], [], []\n",
    "    \n",
    "    for (molecule_seq_, protein_seq_, y_) in batch:\n",
    "        molecule_seq.append(molecule_seq_)\n",
    "        \n",
    "        if len(protein_seq_) <= 2048:\n",
    "            protein_seq.append(protein_seq_)\n",
    "        else:\n",
    "            protein_seq.append(protein_seq_[:2048])\n",
    "            \n",
    "        y.append(y_)\n",
    "        \n",
    "    molecule_seq = molecule_tokenizer.pad(molecule_seq, return_tensors=\"pt\")\n",
    "    protein_seq = pad_sequence(protein_seq, batch_first=True, padding_value=0)\n",
    "    y = torch.tensor(y).float()\n",
    "    \n",
    "    \n",
    "    return molecule_seq, protein_seq, y\n",
    "\n",
    "\n",
    "train_dataset = DTIDataset(train_df, molecule_tokenizer, fasta_stoi)\n",
    "train_sampler = RandomSampler(train_df, replacement=True, num_samples=12800)\n",
    "valid_dataset = DTIDataset(valid_df, molecule_tokenizer, fasta_stoi)\n",
    "test_dataset = DTIDataset(test_df, molecule_tokenizer, fasta_stoi)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, num_workers=16, \n",
    "                              shuffle=False, pin_memory=True, prefetch_factor=10, \n",
    "                              drop_last=True, collate_fn=collate_batch, sampler=train_sampler)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=128, num_workers=16, \n",
    "                              shuffle=False, pin_memory=True, prefetch_factor=10, \n",
    "                              drop_last=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, num_workers=16, \n",
    "                             shuffle=False, pin_memory=True, prefetch_factor=10, \n",
    "                             drop_last=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee3d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ProteinCNNEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dims=[1024, 512, 256], dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv_block_1_layer_1 = nn.Conv1d(embedding_dim, hidden_dims[0], kernel_size=5, padding=2)\n",
    "        self.conv_block_1_layer_2 = nn.Conv1d(hidden_dims[0], hidden_dims[0], kernel_size=5, padding=2)\n",
    "        \n",
    "        self.conv_block_2_layer_1 = nn.Conv1d(hidden_dims[0], hidden_dims[1], kernel_size=3, padding=1)\n",
    "        self.conv_block_2_layer_2 = nn.Conv1d(hidden_dims[1], hidden_dims[1], kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv_block_3_layer_1 = nn.Conv1d(hidden_dims[1], hidden_dims[2], kernel_size=3, padding=1)\n",
    "        self.conv_block_3_layer_2 = nn.Conv1d(hidden_dims[2], hidden_dims[2], kernel_size=3, padding=1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.moveaxis(1, 2)\n",
    "        \n",
    "        x = F.dropout(F.gelu(self.conv_block_1_layer_1(x)), self.dropout)\n",
    "        x = F.dropout(F.gelu(self.conv_block_1_layer_2(x)), self.dropout)\n",
    "        x = F.max_pool1d(x, 2)       \n",
    "        \n",
    "        x = F.dropout(F.gelu(self.conv_block_2_layer_1(x)), self.dropout)\n",
    "        x = F.dropout(F.gelu(self.conv_block_2_layer_2(x)), self.dropout)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = F.dropout(F.gelu(self.conv_block_3_layer_1(x)), self.dropout)\n",
    "        x = F.dropout(F.gelu(self.conv_block_3_layer_2(x)), self.dropout)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x, _ = torch.max(x, -1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "vocab_size = 1024\n",
    "embedding_dim = 256\n",
    "protein_cnn_encoder = ProteinCNNEncoder(vocab_size, embedding_dim)\n",
    "\n",
    "# seq_len = 100\n",
    "\n",
    "# sample_input = torch.randint(vocab_size, (1, seq_len)).long()\n",
    "# out = protein_cnn_encoder(sample_input)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7699374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIPredictionHead(nn.Module):\n",
    "    def __init__(self, molecule_encoder, protein_encoder, \n",
    "                 molecule_dim=128, protein_dim=256, inner_dim=256, projection=True):\n",
    "        super().__init__()\n",
    "        self.is_projection = projection\n",
    "        self.molecule_encoder = molecule_encoder\n",
    "        self.protein_encoder = protein_encoder\n",
    "        \n",
    "        # model freezing without last layer\n",
    "        for param in self.molecule_encoder.encoder.layer[0:-1].parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        if self.is_projection:\n",
    "            self.mol_proj = nn.Linear(molecule_dim, inner_dim, bias=False)        \n",
    "            self.prot_proj = nn.Linear(protein_dim, inner_dim, bias=False)            \n",
    "            self.fc_1 = nn.Linear(inner_dim * 2, inner_dim)\n",
    "        else:\n",
    "            self.fc_1 = nn.Linear(molecule_dim + protein_dim, inner_dim)\n",
    "        \n",
    "        self.fc_2 = nn.Linear(inner_dim, int(inner_dim / 2))\n",
    "        self.fc_out = nn.Linear(int(inner_dim / 2), 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, molecule, protein):\n",
    "        molecule = self.molecule_encoder(**molecule).pooler_output\n",
    "        protein = self.protein_encoder(protein)\n",
    "        \n",
    "        if self.is_projection:\n",
    "            molecule = self.mol_proj(molecule)\n",
    "            protein = self.prot_proj(protein)\n",
    "            \n",
    "        x = torch.cat((molecule, protein), -1)\n",
    "        x = F.dropout(F.gelu(self.fc_1(x)), 0.1)\n",
    "        x = F.dropout(F.gelu(self.fc_2(x)), 0.1)\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "dti_prediction_head = DTIPredictionHead(molecule_bert, protein_cnn_encoder, projection=True)\n",
    "# dti_prediction_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b804bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics.functional import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "class DTI_prediction(pl.LightningModule):\n",
    "    def __init__(self, dti_prediction_head):\n",
    "        super().__init__()\n",
    "        self.model = dti_prediction_head\n",
    "\n",
    "        \n",
    "    def forward(self, molecule_sequence, protein_sequence):\n",
    "        return self.model(molecule_sequence, protein_sequence)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"valid_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=200)\n",
    "    \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "    \n",
    "    \n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor='valid_loss', save_top_k=30, dirpath='weights/DTI_prediction_CLS_concatenate_with_projection', filename='attentional_dti-{epoch:03d}-{valid_loss:.4f}-{valid_mae:.4f}'),\n",
    "]\n",
    "\n",
    "model = DTI_prediction(dti_prediction_head)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1000, gpus=1, enable_progress_bar=True, callbacks=callbacks, precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c43434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | DTIPredictionHead | 11.7 M\n",
      "--------------------------------------------\n",
      "10.3 M    Trainable params\n",
      "1.4 M     Non-trainable params\n",
      "11.7 M    Total params\n",
      "23.319    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16083bfee32d4500ade07cbaecd56b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1dacb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
