{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9685b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics.functional import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "molecule_tokenizer = molecule_tokenizer = BertTokenizer.from_pretrained(\"data/drug/molecule_tokenizer\", model_max_length=128)\n",
    "protein_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "\n",
    "with open(\"data/drug/molecule_qed_filtered.txt\", 'r') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "314b3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_encode(protein_sequence):\n",
    "    protein_sequence = protein_tokenizer(\n",
    "        \" \".join(protein_sequence), \n",
    "        max_length=512, \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return protein_sequence\n",
    "\n",
    "mlkl = \"GSPGENLKHIITLGQVIHKRCEEMKYCKKQCRRLGHRVLGLIKPLEMLQDQGKRSVPSEKLTTAMNRFKAALEEANGEIEKFSNRSNICRFLTASQDKILFKDVNRKLSDVWKELSLLLQVEQRMPVSPISQGASWAQEDQQDADEDRRAFQMLRRD\"\n",
    "mlkl_ = protein_encode(mlkl)\n",
    "\n",
    "\n",
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, data, molecule_tokenizer, mlkl_):\n",
    "        self.data = data\n",
    "        \n",
    "        self.molecule_max_len = 100\n",
    "        \n",
    "        self.molecule_tokenizer = molecule_tokenizer\n",
    "        self.mlkl_ = mlkl_\n",
    "        \n",
    "    def molecule_encode(self, molecule_sequence):\n",
    "        molecule_sequence = self.molecule_tokenizer(\n",
    "            \" \".join(molecule_sequence), \n",
    "            max_length=self.molecule_max_len, \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return molecule_sequence\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        molecule_sequence = self.molecule_encode(self.data[idx])\n",
    "        protein_sequence = self.mlkl_\n",
    "                \n",
    "        return molecule_sequence, protein_sequence\n",
    "\n",
    "    \n",
    "def collate_batch(batch):\n",
    "    molecule_seq, protein_seq = [], []\n",
    "    \n",
    "    for (molecule_seq_, protein_seq_) in batch:\n",
    "        molecule_seq.append(molecule_seq_)\n",
    "        protein_seq.append(protein_seq_)\n",
    "        \n",
    "    molecule_seq = molecule_tokenizer.pad(molecule_seq, return_tensors=\"pt\")\n",
    "    protein_seq = protein_tokenizer.pad(protein_seq, return_tensors=\"pt\")\n",
    "    \n",
    "    return molecule_seq, protein_seq\n",
    "\n",
    "\n",
    "predict_dataset = DTIDataset(data, molecule_tokenizer, mlkl_)\n",
    "predict_dataloader = DataLoader(predict_dataset, batch_size=3072, num_workers=16, \n",
    "                                pin_memory=True, prefetch_factor=10, \n",
    "                                collate_fn=collate_batch, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1152992b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at weights/molecule_bert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at weights/protein_bert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, input_dim=128, intermediate_dim=512, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        project_out = input_dim\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = (input_dim / heads) ** -0.5\n",
    "\n",
    "        self.key = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "        self.value = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "        self.query = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(intermediate_dim, project_out),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, data):\n",
    "        b, n, d, h = *data.shape, self.heads\n",
    "\n",
    "        k = self.key(data)\n",
    "        k = rearrange(k, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        v = self.value(data)\n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h=h)\n",
    "        \n",
    "        # get only cls token\n",
    "        q = self.query(data[:, 0].unsqueeze(1))\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attention = dots.softmax(dim=-1)\n",
    "\n",
    "        output = einsum('b h i j, b h j d -> b h i d', attention, v)\n",
    "        output = rearrange(output, 'b h n d -> b n (h d)')\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "    \n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 molecule_dim=128, molecule_intermediate_dim=256,\n",
    "                 protein_dim=1024, protein_intermediate_dim=2048,\n",
    "                 cross_attn_depth=1, cross_attn_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cross_attn_layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(cross_attn_depth):\n",
    "            self.cross_attn_layers.append(nn.ModuleList([\n",
    "                nn.Linear(molecule_dim, protein_dim),\n",
    "                nn.Linear(protein_dim, molecule_dim),\n",
    "                PreNorm(protein_dim, CrossAttention(\n",
    "                    protein_dim, protein_intermediate_dim, cross_attn_heads, dropout\n",
    "                )),\n",
    "                nn.Linear(protein_dim, molecule_dim),\n",
    "                nn.Linear(molecule_dim, protein_dim),\n",
    "                PreNorm(molecule_dim, CrossAttention(\n",
    "                    molecule_dim, molecule_intermediate_dim, cross_attn_heads, dropout\n",
    "                ))\n",
    "            ]))\n",
    "\n",
    "            \n",
    "    def forward(self, molecule, protein):\n",
    "        for i, (f_sl, g_ls, cross_attn_s, f_ls, g_sl, cross_attn_l) in enumerate(self.cross_attn_layers):\n",
    "            \n",
    "            cls_molecule = molecule[:, 0]\n",
    "            x_molecule = molecule[:, 1:]\n",
    "            \n",
    "            cls_protein = protein[:, 0]\n",
    "            x_protein = protein[:, 1:]\n",
    "\n",
    "            # Cross attention for protein sequence\n",
    "            cal_q = f_ls(cls_protein.unsqueeze(1))\n",
    "            cal_qkv = torch.cat((cal_q, x_molecule), dim=1)\n",
    "            # add activation function\n",
    "            cal_out = cal_q + cross_attn_l(cal_qkv)\n",
    "            cal_out = F.gelu(g_sl(cal_out))\n",
    "            protein = torch.cat((cal_out, x_protein), dim=1)\n",
    "\n",
    "            # Cross attention for molecule sequence\n",
    "            cal_q = f_sl(cls_molecule.unsqueeze(1))\n",
    "            cal_qkv = torch.cat((cal_q, x_protein), dim=1)\n",
    "            # add activation function\n",
    "            cal_out = cal_q + cross_attn_s(cal_qkv)\n",
    "            cal_out = F.gelu(g_ls(cal_out))\n",
    "            molecule = torch.cat((cal_out, x_molecule), dim=1)\n",
    "            \n",
    "        return molecule, protein\n",
    "    \n",
    "    \n",
    "class AttentionalDTI(nn.Module):\n",
    "    def __init__(self, \n",
    "                 molecule_encoder, protein_encoder, cross_attention_layer, \n",
    "                 molecule_input_dim=128, protein_input_dim=1024, hidden_dim=512, **kwargs):\n",
    "        super().__init__()\n",
    "        self.molecule_encoder = molecule_encoder\n",
    "        self.protein_encoder = protein_encoder\n",
    "        \n",
    "        # model freezing without last layer\n",
    "        for param in self.molecule_encoder.encoder.layer[0:-1].parameters():\n",
    "            param.requires_grad = False        \n",
    "        for param in self.protein_encoder.encoder.layer[0:-1].parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.cross_attention_layer = cross_attention_layer\n",
    "        \n",
    "        self.molecule_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(molecule_input_dim),\n",
    "            nn.Linear(molecule_input_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.protein_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(protein_input_dim),\n",
    "            nn.Linear(protein_input_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, molecule_seq, protein_seq):\n",
    "        encoded_molecule = self.molecule_encoder(**molecule_seq)\n",
    "        encoded_protein = self.protein_encoder(**protein_seq)\n",
    "        \n",
    "        molecule_out, protein_out = self.cross_attention_layer(encoded_molecule.last_hidden_state, encoded_protein.last_hidden_state)\n",
    "        \n",
    "        molecule_out = molecule_out[:, 0]\n",
    "        protein_out = protein_out[:, 0]\n",
    "        \n",
    "        # cls token\n",
    "        molecule_projected = self.molecule_mlp(molecule_out)\n",
    "        protein_projected = self.protein_mlp(protein_out)\n",
    "        \n",
    "        out = self.fc_out(molecule_projected + protein_projected)\n",
    "        \n",
    "        return out\n",
    "\n",
    "molecule_bert = BertModel.from_pretrained(\"weights/molecule_bert\")\n",
    "protein_bert = BertModel.from_pretrained(\"weights/protein_bert\")\n",
    "cross_attention_layer = CrossAttentionLayer()\n",
    "attentional_dti = AttentionalDTI(molecule_bert, protein_bert, cross_attention_layer, cross_attn_depth=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ca89218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "class DTI_prediction(pl.LightningModule):\n",
    "    def __init__(self, attentional_dti):\n",
    "        super().__init__()\n",
    "        self.model = attentional_dti\n",
    "\n",
    "        \n",
    "    def forward(self, molecule_sequence, protein_sequence):\n",
    "        return self.model(molecule_sequence, protein_sequence)\n",
    "    \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "model = DTI_prediction(attentional_dti)\n",
    "trainer = pl.Trainer(max_epochs=50, gpus=1, enable_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63fafbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_fname = \"attentional_dti-epoch=049-valid_loss=0.1788-valid_mae=0.2497.ckpt\"\n",
    "\n",
    "model = model.load_from_checkpoint(\"weights/Attentional_DTI_cross_attention_kiba/\" + ckpt_fname, attentional_dti=attentional_dti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf259555",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053f8ffd60f64b659c84a2cdb16da636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(model, predict_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f187886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "with open(\"data/drug/molecule_qed_filtered.txt\", 'r') as f:\n",
    "    mols = f.readlines()\n",
    "mols = np.array(mols)\n",
    "    \n",
    "with open(\"data/interaction/prediction.pkl\", 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9034bcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11885/11885 [01:18<00:00, 151.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "for d in tqdm(data):\n",
    "    for p in d:\n",
    "        results.append(p)\n",
    "        \n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffe66074",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = results.argsort()[::-1]\n",
    "sample_idx = idx[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09700968",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0683\n",
      "15.0277\n",
      "15.005\n",
      "14.9328\n",
      "14.9293\n",
      "14.9052\n",
      "14.8696\n",
      "14.8621\n",
      "14.8332\n",
      "14.8302\n",
      "14.8173\n",
      "14.7982\n",
      "14.7962\n",
      "14.7945\n",
      "14.7942\n",
      "14.7875\n",
      "14.7864\n",
      "14.7821\n",
      "14.7805\n",
      "14.7757\n",
      "C[N+]1(C)CCC23c4c5ccc(C(N)=O)c4OC2C(=O)CCC3(O)C1C5\n",
      "CCC[N+]1(C)CCC23c4c5ccc(C(N)=O)c4OC2C(=O)CCC3(O)C1C5\n",
      "C[N+]1(C)CCC23c4c5ccc(C(N)=O)c4OC2C(O)CCC3C1C5\n",
      "C#[N+]C(=O)c1nnc2[nH]ccc2c1NC12CC3CC4(O)CC(C1)C32C4\n",
      "CC1CC(C#N)N(C(=O)C(N)C2=C3C4CC(C2)CC3(O)C4)C1\n",
      "CC1CN(c2n[nH]c(C3C4C5CCC(C5)C34)n2)CCN1\n",
      "COC1=C(C)C(=O)OC1=C1OC23OC4CC(C2C1C)N1CCC3C41CNC(C)C\n",
      "CNCc1c(S(=O)(=O)NC2C3C4CCC(C4)C23)n[nH]c1C\n",
      "NC1CCN(c2n[nH]c(C3C4C5CCC(C5)C34)n2)CC1\n",
      "CCn1c(C2C3C4CCC(C4)C23)nnc1S(N)(=O)=O\n",
      "CC1=C(C)c2c(C)c(C)c3c4c(c(C)c(C)c1c24)C([O-])=C3[O-]\n",
      "CC.CC1=CC(C)(C)Nc2ccc3c4c(oc(=O)c3c21)=CCC=4.[HH]\n",
      "CC1CN(c2n[nH]c(C3C4C5CCC(C5)C34)n2)CC(C)N1\n",
      "COC1=C(C)C(=O)OC1=C1OC23OC4CC(C2C1C)N1CCC3C41CNC1CC1\n",
      "C1CNCCN(c2n[nH]c(C3C4C5CCC(C5)C34)n2)C1\n",
      "COC1C(C(C)=O)CC2C3[NH+](C)CC34CC23c2c4ccc(O)c2OC13\n",
      "C[N+]1(CC2CC2)CC23c4c5ccc(O)c4OC2C(=O)CCC3(O)C1C5\n",
      "NCC1CCN(c2n[nH]c(C3C4C5CCC(C5)C34)n2)CC1\n",
      "C[N+]1(C)CCC23c4c5ccc(CO)c4OC2C(=O)CCC3(O)C1C5\n",
      "Cc1ncc2c(n1)C1Oc3c(O)ccc4c3C13CCN(CC1CC1)C(C4)C3(O)C2\n"
     ]
    }
   ],
   "source": [
    "for a in results[sample_idx]:\n",
    "    print(a.round(4))\n",
    "\n",
    "for b in mols[sample_idx]:\n",
    "    print(b.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2743d19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C[N+]1(C)CCC23C4=C5C=CC(C(N)=O)=C4OC2C(O)CCC3C1C5'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "m = Chem.MolFromSmiles('C[N+]1(C)CCC23c4c5ccc(C(N)=O)c4OC2C(O)CCC3C1C5')\n",
    "Chem.MolToSmiles(m, canonical=True, kekuleSmiles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef0576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
