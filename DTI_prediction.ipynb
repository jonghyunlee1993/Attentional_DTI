{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4fe9887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import torchtext\n",
    "\n",
    "import pickle\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"./data/DTI/DTI_train.pickle\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open(\"./data/DTI/DTI_valid.pickle\", \"rb\") as f:\n",
    "    valid_data = pickle.load(f)\n",
    "    \n",
    "with open(\"./data/DTI/DTI_test.pickle\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "    \n",
    "with open(\"./data/molecule_net/MoleculeNet_tokenizer.pickle\", \"rb\") as f:\n",
    "    molecule_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(\"./data/DTI/protein_tokenizer.pickle\", \"rb\") as f:\n",
    "    protein_tokenizer = pickle.load(f)\n",
    "\n",
    "molecule_vocab_dim     = len(molecule_tokenizer.vocab.itos)\n",
    "molecule_seq_len       = 100\n",
    "molecule_embedding_dim = 512\n",
    "\n",
    "protein_vocab_dim     = len(protein_tokenizer.vocab.itos)\n",
    "protein_seq_len       = 1000\n",
    "protein_embedding_dim = 128\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1566e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, molecule_tokenizer, molecule_seq_len, protein_tokenizer, protein_seq_len):\n",
    "        super(DTIDataset, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "        self.molecule_tokenizer = molecule_tokenizer\n",
    "        self.molecule_vocab = molecule_tokenizer.vocab\n",
    "        self.molecule_seq_len = molecule_seq_len\n",
    "        \n",
    "        self.protein_tokenizer = protein_tokenizer\n",
    "        self.protein_vocab = protein_tokenizer.vocab\n",
    "        self.protein_seq_len = protein_seq_len\n",
    "        \n",
    "        self.cls_token_id  = self.molecule_vocab.stoi[self.molecule_tokenizer.init_token]\n",
    "        self.sep_token_id  = self.molecule_vocab.stoi[self.molecule_tokenizer.eos_token]\n",
    "        self.pad_token_id  = self.molecule_vocab.stoi[self.molecule_tokenizer.pad_token]\n",
    "        self.mask_token_id = self.molecule_vocab.stoi[self.molecule_tokenizer.unk_token]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        current_data = self.data.loc[idx]\n",
    "        \n",
    "        molecule_string = current_data['Drug']\n",
    "        protein_string = current_data['Target']\n",
    "        target = current_data['Y']\n",
    "\n",
    "        molecule = self.molecule_tokenizer.numericalize(molecule_string).squeeze()\n",
    "        protein = self.protein_tokenizer.numericalize(protein_string).squeeze()\n",
    "        \n",
    "        if len(molecule) < self.molecule_seq_len - 2:\n",
    "            molecule_pad_length = self.molecule_seq_len - len(molecule) - 2\n",
    "        else:\n",
    "            molecule = molecule[:self.molecule_seq_len - 2]\n",
    "            molecule_pad_length = 0\n",
    "            \n",
    "        if len(protein) < self.protein_seq_len - 2:\n",
    "            protein_pad_length = self.protein_seq_len - len(protein) - 2\n",
    "        else:\n",
    "            protein = protein[:self.protein_seq_len - 2]\n",
    "            protein_pad_length = 0\n",
    "              \n",
    "        molecule = torch.cat([torch.tensor([self.cls_token_id]), molecule, torch.tensor([self.sep_token_id]), torch.tensor([self.pad_token_id] * molecule_pad_length)]).long().contiguous()\n",
    "        protein = torch.cat([torch.tensor([self.cls_token_id]), protein, torch.tensor([self.sep_token_id]), torch.tensor([self.pad_token_id] * protein_pad_length)]).long().contiguous()\n",
    "        \n",
    "        target = torch.tensor(target).type(torch.FloatTensor).contiguous()\n",
    "\n",
    "        segment_embedding = torch.zeros(molecule.size(0))\n",
    "\n",
    "        return molecule, protein, target, segment_embedding\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        for x in self.data:\n",
    "            yield x\n",
    "            \n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    \n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17aef5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, pad_token_id):\n",
    "        super(BERT, self).__init__()\n",
    "        self.pad_token_id  = pad_token_id\n",
    "        self.nhead         = 4\n",
    "        self.embedding     = BERTEmbedding(vocab_dim, seq_len, embedding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=self.nhead, batch_first=True)\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, num_layers=4)\n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        pad_mask  = BERT.get_attn_pad_mask(data, data, self.pad_token_id).repeat(self.nhead, 1, 1)\n",
    "        embedding = self.embedding(data, segment_embedding)\n",
    "        output    = self.encoder_block(embedding, pad_mask) \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "        batch_size, len_q = seq_q.size()\n",
    "        batch_size, len_k = seq_k.size()\n",
    "        pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "        pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "        \n",
    "        return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c1f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, dropout_rate=0.1, device=device):\n",
    "        super(BERTEmbedding, self).__init__()\n",
    "        self.seq_len       = seq_len\n",
    "        self.vocab_dim     = vocab_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate  = dropout_rate\n",
    "        \n",
    "        # vocab --> embedding\n",
    "        self.token_embedding      = nn.Embedding(self.vocab_dim, self.embedding_dim) \n",
    "        self.token_dropout        = nn.Dropout(self.dropout_rate)    \n",
    "        \n",
    "        # seq len --> embedding\n",
    "        self.positional_embedding = nn.Embedding(self.seq_len, self.embedding_dim)\n",
    "        self.positional_dropout   = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        # segment (0, 1) --> embedding\n",
    "        self.segment_embedding    = nn.Embedding(2, self.embedding_dim)\n",
    "        self.segment_dropout      = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        token_embedding      = self.token_embedding(data)\n",
    "        token_embedding      = self.token_dropout(token_embedding)\n",
    "        \n",
    "        positional_encoding  = torch.arange(start=0, end=self.seq_len, step=1).long()\n",
    "        # data의 device 정보 가져와서 처리\n",
    "        positional_encoding  = positional_encoding.unsqueeze(0).expand(data.size()).to(device)\n",
    "        positional_embedding = self.positional_embedding(positional_encoding)\n",
    "        positional_embedding = self.positional_dropout(positional_embedding)\n",
    "        \n",
    "        segment_embedding    = self.segment_embedding(segment_embedding)\n",
    "        segment_embedding    = self.segment_dropout(segment_embedding)\n",
    "        \n",
    "        return token_embedding + positional_embedding + segment_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c58f375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeBranch(nn.Module):\n",
    "    def __init__(self, bert, output_dim):\n",
    "        super(MoleculeBranch, self).__init__()\n",
    "        self.bert = bert\n",
    "        d_model = 100 * 512\n",
    "        self.fc   = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x, segment_embedding):\n",
    "        batch_size = x.shape[0]\n",
    "        output = self.bert(x, segment_embedding)\n",
    "        output = output.reshape(batch_size, -1)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40feec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinBranch(nn.Module):\n",
    "    def __init__(self, seq_len, vocab_dim, embedding_dim, dropout_rate):\n",
    "        super(ProteinBranch, self).__init__()\n",
    "        self.embedding = ProteinEmbedding(seq_len, vocab_dim, embedding_dim, dropout_rate)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=8, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=8, padding=1)        \n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=96, kernel_size=8, padding=1)\n",
    "        self.dropout = nn.Dropout(dropout_rate, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.squeeze(1).moveaxis(1, 2)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.moveaxis(1, 2)\n",
    "\n",
    "        return torch.mean(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac89b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinEmbedding(nn.Module):\n",
    "    def __init__(self, seq_len, vocab_dim, embedding_dim, dropout_rate):\n",
    "        super(ProteinEmbedding, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_dim, self.embedding_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x.long())\n",
    "        embedding = self.dropout(embedding).unsqueeze(1)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b70483c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, moleucle_branch, protein_branch, output_dim=96, dropout_rate=0.1):\n",
    "        super(PredictionHead, self).__init__()\n",
    "        self.molecule_branch = moleucle_branch.to(device)\n",
    "        self.protein_branch = protein_branch.to(device)\n",
    "        \n",
    "        self.input_dim = 96 * 2\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.fc_1 = nn.Linear(self.input_dim, 1024)\n",
    "        self.fc_2 = nn.Linear(1024, 1024)\n",
    "        self.fc_3 = nn.Linear(1024, 512)\n",
    "\n",
    "        self.fc_out = nn.Linear(512, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate, inplace=True)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, molecule, protein, segment_embedding):\n",
    "        molecule_embedding = self.molecule_branch(molecule, segment_embedding)\n",
    "        protein_embedding = self.protein_branch(protein)\n",
    "        \n",
    "        merged_embedding = torch.cat([molecule_embedding, protein_embedding], dim=1)\n",
    "        out = self.dropout(merged_embedding).to(device)\n",
    "        \n",
    "        out = self.activation(self.dropout(self.fc_1(out)))\n",
    "        out = self.activation(self.dropout(self.fc_2(out)))\n",
    "        out = self.activation(self.dropout(self.fc_3(out)))\n",
    "        out = self.fc_out(out).to(device)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c6a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(data, molecule_tokenizer, molecule_seq_len, protein_tokenizer, protein_seq_len, batch_size, collate_fn, shuffle=True, num_workers=6):\n",
    "    dataset = DTIDataset(data=data, molecule_tokenizer=molecule_tokenizer, molecule_seq_len=molecule_seq_len, protein_tokenizer=protein_tokenizer, protein_seq_len=protein_seq_len, pin_memory=True)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n",
    "    \n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5eeb9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device, clip=1):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_corrects = 0\n",
    "    epoch_num_data = 0\n",
    "\n",
    "    for molecule, protein, target, segment_emb in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(molecule.to(device), protein.to(device), segment_emb.long().to(device))\n",
    "        \n",
    "        output = output\n",
    "        target = target.to(device)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "                \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_corrects = 0\n",
    "    epoch_num_data = 0\n",
    "\n",
    "    for molecule, protein, target, segment_emb in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(molecule.to(device), protein.to(device), segment_emb.long().to(device))\n",
    "        \n",
    "        output = output\n",
    "        target = target.to(device)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, iterator, device):\n",
    "    model.eval()\n",
    "    \n",
    "    for batch, (molecule, protein, target, segment_emb) in enumerate(iterator):\n",
    "        output = model(molecule.to(device), protein.to(device), segment_emb.long().to(device))\n",
    "        \n",
    "        molecule = molecule.clone().detach().to(\"cpu\").tolist()\n",
    "        protein = protein.clone().detach().to(\"cpu\").tolist()\n",
    "        output = output.clone().detach().to(\"cpu\").tolist()\n",
    "        target = target.clone().detach().to(\"cpu\").tolist()\n",
    "        \n",
    "    return molecule, protein, output, target\n",
    "\n",
    "\n",
    "def generate_epoch_dataloader(data, molecule_tokenizer, molecule_seq_len, protein_tokenizer, protein_seq_len, batch_size, collate_fn, shuffle=True, num_workers=6):\n",
    "    dataset = DTIDataset(data=data, molecule_tokenizer=molecule_tokenizer, molecule_seq_len=molecule_seq_len, protein_tokenizer=protein_tokenizer, protein_seq_len=protein_seq_len)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n",
    "    \n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad946c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_bert = BERT(vocab_dim=molecule_vocab_dim, seq_len=molecule_seq_len, embedding_dim=molecule_embedding_dim, pad_token_id=1).to(device)\n",
    "for param in molecule_bert.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "molecule_branch = MoleculeBranch(molecule_bert, output_dim=96).to(device)\n",
    "protein_branch = ProteinBranch(seq_len=protein_seq_len, vocab_dim=protein_vocab_dim, embedding_dim=protein_embedding_dim, dropout_rate=0.1).to(device)\n",
    "model = PredictionHead(molecule_branch, protein_branch).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, betas=[0.9, 0.999], weight_decay=0.01)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "scheduler = ReduceLROnPlateau(optimizer)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d93cb99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/110 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      " 99%|█████████▉| 109/110 [00:21<00:00,  5.85it/s]/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([119])) that is different to the input size (torch.Size([119, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 110/110 [00:21<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 6.6879 | Train RMSE: 2.5861\n",
      "Valid MSE: 1.7039 | Valid RMSE: 1.3053\n",
      "Predictions ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([309])) that is different to the input size (torch.Size([309, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 1.6947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([107])) that is different to the input size (torch.Size([107, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:20<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.6532 | Train RMSE: 1.2858\n",
      "Valid MSE: 2.0459 | Valid RMSE: 1.4304\n",
      "Epoch: 0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:20<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.6358 | Train RMSE: 1.2790\n",
      "Valid MSE: 2.2225 | Valid RMSE: 1.4908\n",
      "Epoch: 0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:20<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.6173 | Train RMSE: 1.2717\n",
      "Valid MSE: 2.2381 | Valid RMSE: 1.4960\n",
      "Epoch: 0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:20<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.6238 | Train RMSE: 1.2743\n",
      "Valid MSE: 2.3329 | Valid RMSE: 1.5274\n",
      "Epoch: 0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:20<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.6262 | Train RMSE: 1.2752\n",
      "Valid MSE: 2.2254 | Valid RMSE: 1.4918\n",
      "Predictions ...\n",
      "\n",
      "test loss: 2.1953\n",
      "Epoch: 0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:20<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.6151 | Train RMSE: 1.2709\n",
      "Valid MSE: 2.4769 | Valid RMSE: 1.5738\n",
      "Epoch: 0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:20<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.6211 | Train RMSE: 1.2732\n",
      "Valid MSE: 2.3909 | Valid RMSE: 1.5463\n",
      "Epoch: 0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:20<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.6111 | Train RMSE: 1.2693\n",
      "Valid MSE: 2.1934 | Valid RMSE: 1.4810\n",
      "Epoch: 0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:20<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.6080 | Train RMSE: 1.2681\n",
      "Valid MSE: 2.1920 | Valid RMSE: 1.4805\n",
      "Epoch: 0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:20<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.6066 | Train RMSE: 1.2675\n",
      "Valid MSE: 2.1513 | Valid RMSE: 1.4667\n",
      "Predictions ...\n",
      "\n",
      "test loss: 2.1611\n",
      "Epoch: 0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 73/110 [00:13<00:06,  5.35it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-441f03d98236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch:04}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-e525d278b02e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, device, clip)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS  = 1000\n",
    "PAITIENCE = 30\n",
    "\n",
    "start_epoch = 0\n",
    "if len(glob.glob(\"output/DTI/*.tsv\")) != 0:\n",
    "    print(\"load pretrained model ... \")\n",
    "    start_epoch = len(glob.glob(\"output/DTI/*.tsv\"))\n",
    "    model.load_state_dict(torch.load('weights/DTI_single_bert_best.pt'))\n",
    "\n",
    "n_paitience = 0\n",
    "best_valid_loss = float('inf')\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "    \n",
    "for epoch in range(start_epoch, N_EPOCHS):\n",
    "    train_data_loader = generate_epoch_dataloader(train_data, \n",
    "                                                 molecule_tokenizer=molecule_tokenizer, \n",
    "                                                 molecule_seq_len=molecule_seq_len,\n",
    "                                                 protein_tokenizer=protein_tokenizer,\n",
    "                                                 protein_seq_len=protein_seq_len,\n",
    "                                                 batch_size=batch_size, \n",
    "                                                 collate_fn=collate_fn,\n",
    "                                                 num_workers=8)\n",
    "    \n",
    "    valid_data_loader = generate_epoch_dataloader(valid_data, \n",
    "                                                 molecule_tokenizer=molecule_tokenizer, \n",
    "                                                 molecule_seq_len=molecule_seq_len,\n",
    "                                                 protein_tokenizer=protein_tokenizer,\n",
    "                                                 protein_seq_len=protein_seq_len,\n",
    "                                                 batch_size=batch_size, \n",
    "                                                 collate_fn=collate_fn,\n",
    "                                                 num_workers=8)\n",
    "    \n",
    "    print(f'Epoch: {epoch:04}')\n",
    "    \n",
    "    train_loss = train(model, train_data_loader, optimizer, criterion, device)\n",
    "    valid_loss = evaluate(model, valid_data_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    print(f'Train MSE: {train_loss:.4f} | Train RMSE: {np.sqrt(train_loss):.4f}\\nValid MSE: {valid_loss:.4f} | Valid RMSE: {np.sqrt(valid_loss):.4f}')\n",
    "\n",
    "    with open(\"output/DTI/log.txt\", \"a\") as f:\n",
    "        f.write(f\"Epoch: {epoch:04d} Train MSE: {train_loss:.4f}, Train RMSE: {np.sqrt(train_loss):.4f}, Valid MSE: {valid_loss:.4f}, Valid RMSE: {np.sqrt(valid_loss):.4f}\\n\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Predictions ...\\n\")\n",
    "        test_data_loader = generate_epoch_dataloader(test_data, \n",
    "                                                          molecule_tokenizer=molecule_tokenizer, \n",
    "                                                          molecule_seq_len=molecule_seq_len,\n",
    "                                                          protein_tokenizer=protein_tokenizer,\n",
    "                                                          protein_seq_len=protein_seq_len,\n",
    "                                                          batch_size=batch_size, \n",
    "                                                          collate_fn=collate_fn,\n",
    "                                                          num_workers=8)\n",
    "\n",
    "        test_loss = evaluate(model, test_data_loader, criterion, device)\n",
    "        print(f\"test loss: {test_loss:.4f}\")\n",
    "        molecule, protein, prediction, target = predict(model, test_data_loader, device)\n",
    "        prediction_results = pd.DataFrame({\"molecule\": molecule,\n",
    "                                           \"protein\": protein,\n",
    "                                           \"output\": prediction, \n",
    "                                           \"target\": target})\n",
    "        \n",
    "        prediction_results.to_csv(f\"output/DTI/prediction_results_epoch-{epoch:04d}_mse-{np.round(test_loss, 4)}.tsv\", sep=\"\\t\", index=False)            \n",
    "        \n",
    "    if n_paitience < PAITIENCE:\n",
    "        if best_valid_loss > valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'weights/DTI_single_bert_best.pt')\n",
    "            n_paitience = 0\n",
    "        elif best_valid_loss <= valid_loss:\n",
    "            n_paitience += 1\n",
    "    else:\n",
    "        print(\"Early stop!\")\n",
    "        model.load_state_dict(torch.load('weights/DTI_single_bert_best.pt'))\n",
    "        model.eval()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4f65c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
