{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1fe7b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "To log space...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.QED import qed\n",
    "from tdc.multi_pred import DTI\n",
    "\n",
    "from rdkit import RDLogger \n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "davis = DTI(name=\"Davis\")\n",
    "davis.convert_to_log(form='binding')\n",
    "davis_split = davis.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbdacaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_smiles(df):\n",
    "    for i, smiles in enumerate(df['Drug']):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        canonical_smiles = Chem.MolToSmiles(mol)\n",
    "        \n",
    "        df.loc[i, 'Drug'] = canonical_smiles\n",
    "        \n",
    "    return df\n",
    "\n",
    "train_df = canonicalize_smiles(davis_split['train'])\n",
    "valid_df = canonicalize_smiles(davis_split['valid'])\n",
    "test_df = canonicalize_smiles(davis_split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4668442c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at weights/molecule_bert_pretrained were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at weights/molecule_bert_pretrained and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import PreTrainedTokenizerFast, PreTrainedTokenizer\n",
    "\n",
    "molecule_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"data/drug/tokenizer_model/vocab.json\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    unk_token=\"[UNK]\"\n",
    ")\n",
    "molecule_bert = BertModel.from_pretrained(\"weights/molecule_bert_pretrained\", local_files_only=False)\n",
    "\n",
    "protein_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "protein_bert = BertModel.from_pretrained(\"Rostlab/prot_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e424174",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, data, molecule_tokenizer, protein_tokenizer):\n",
    "        self.data = data\n",
    "        self.molecule_tokenizer = molecule_tokenizer\n",
    "        self.protein_tokenizer = protein_tokenizer\n",
    "        self.molecule_max_length = 128\n",
    "        self.protein_max_length = 2048\n",
    "        \n",
    "    def encode(self, smiles, fasta):\n",
    "        pad_len_smiles = self.molecule_max_length - len(smiles)\n",
    "        pad_len_fasta = self.protein_max_length - len(fasta)\n",
    "        \n",
    "        smiles = smiles + '#' * pad_len_smiles\n",
    "        smiles = \" \".join(smiles).replace(\"#\", \"[PAD]\")\n",
    "        \n",
    "        fasta = fasta + '#' * pad_len_fasta\n",
    "        fasta = \" \".join(fasta).replace(\"#\", \"[PAD]\")\n",
    "        \n",
    "        molecule_seq = self.molecule_tokenizer.encode(smiles, max_length=self.molecule_max_length, truncation=True, return_tensors='pt')\n",
    "        protein_seq = self.protein_tokenizer.encode(fasta, max_length=self.protein_max_length, truncation=True, return_tensors='pt')\n",
    "        \n",
    "        return molecule_seq, protein_seq\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.data.loc[idx, \"Drug\"]\n",
    "        fasta = self.data.loc[idx, \"Target\"]\n",
    "        y = torch.tensor(self.data.loc[idx, \"Y\"]).float()\n",
    "        \n",
    "        molecule_seq, protein_seq = self.encode(smiles, fasta)\n",
    "        \n",
    "        return molecule_seq, protein_seq, y\n",
    "\n",
    "\n",
    "train_dataset = DTIDataset(train_df, molecule_tokenizer, protein_tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, num_workers=16, pin_memory=True, prefetch_factor=10, drop_last=True)\n",
    "\n",
    "valid_dataset = DTIDataset(valid_df, molecule_tokenizer, protein_tokenizer)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, num_workers=16, pin_memory=True, prefetch_factor=10)\n",
    "\n",
    "test_dataset = DTIDataset(test_df, molecule_tokenizer, protein_tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, num_workers=16, pin_memory=True, prefetch_factor=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1bb960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIPredictionHead(nn.Module):\n",
    "    def __init__(self, molecule_encoder, protein_encoder):\n",
    "        super().__init__()\n",
    "        self.molecule_encoder = molecule_encoder\n",
    "        self.protein_encoder = protein_encoder\n",
    "        \n",
    "        # model freezing\n",
    "        for param in self.molecule_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.protein_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        molecule_out = 128\n",
    "        protein_out = 1024\n",
    "        self.fc_1 = nn.Linear(molecule_out + protein_out, 512)        \n",
    "        self.fc_2 = nn.Linear(512, 256)\n",
    "        self.fc_out = nn.Linear(256, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, smiles, fasta):\n",
    "        smiles = smiles.squeeze(1)\n",
    "        fasta = fasta.squeeze(1)\n",
    "        \n",
    "        smiles_vec = self.molecule_encoder(smiles).pooler_output\n",
    "        fasta_vec = self.protein_encoder(fasta).pooler_output\n",
    "        \n",
    "        x = torch.cat((smiles_vec, fasta_vec), -1)\n",
    "        x = F.dropout(F.gelu(self.fc_1(x)), 0.2)\n",
    "        x = F.dropout(F.gelu(self.fc_2(x)), 0.2)\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "dti_prediction_head = DTIPredictionHead(molecule_bert, protein_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e25af617",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for batch in valid_dataloader:\n",
    "#     smiles, fasta, y = batch\n",
    "    \n",
    "#     print(dti_prediction_head(smiles, fasta))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabd3b01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample_smiles = train_df.loc[0, \"Drug\"]\n",
    "# sample_fasta = \" \".join(train_df.loc[0, \"Target\"])\n",
    "\n",
    "# encoded_smiles = molecule_tokenizer.encode(sample_smiles, return_tensors='pt')\n",
    "# encoded_fasta = protein_tokenizer.encode(sample_fasta, return_tensors='pt')\n",
    "\n",
    "# dti_prediction_head(encoded_smiles, encoded_fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2eca3c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "class DTI_prediction(pl.LightningModule):\n",
    "    def __init__(self, dti_prediction_head):\n",
    "        super().__init__()\n",
    "        self.model = dti_prediction_head\n",
    "        \n",
    "        self.train_accuracy = torchmetrics.MeanAbsoluteError()\n",
    "        self.valid_accuracy = torchmetrics.MeanAbsoluteError()\n",
    "        self.test_accuracy = torchmetrics.MeanAbsoluteError()\n",
    "        \n",
    "        \n",
    "    def forward(self, smiles, fasta):\n",
    "        return self.model(smiles, fasta)\n",
    "\n",
    "       \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        smiles, fasta, y = batch\n",
    "        y_hat = self(smiles, fasta).squeeze(-1)\n",
    "        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", self.train_accuracy(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        smiles, fasta, y = batch\n",
    "        y_hat = self(smiles, fasta).squeeze(-1)        \n",
    "        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"valid_accuracy\", self.valid_accuracy(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        smiles, fasta, y = batch\n",
    "        y_hat = self(smiles, fasta).squeeze(-1)\n",
    "\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_accuracy\", self.valid_accuracy(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        smiles, fasta, y = batch\n",
    "        y_hat = self(smiles, fasta).squeeze(-1)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
    "    \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "    \n",
    "    \n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor='valid_loss', save_top_k=10, dirpath='weights/DTI_prediction_CLS_token_concatenate', filename='molecule_bert-{epoch:02d}-{valid_loss:.4f}'),\n",
    "]\n",
    "\n",
    "model = DTI_prediction(dti_prediction_head)\n",
    "trainer = pl.Trainer(max_epochs=100, gpus=1, enable_progress_bar=True, callbacks=callbacks, precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb1685b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "524700e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8e106c04fe472bba82d817bf0fc42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_from_checkpoint(dti_prediction_head\u001b[38;5;241m=\u001b[39mdti_prediction_head, checkpoint_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights/DTI_prediction_CLS_token_concatenate/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m checkpoint_file)\n\u001b[1;32m      6\u001b[0m pred \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(model, test_dataloader)\n\u001b[0;32m----> 7\u001b[0m true \u001b[38;5;241m=\u001b[39m test_dataloader\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mY\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      9\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(true, pred)\n\u001b[1;32m     10\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(true, pred)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "checkpoint_file = \"molecule_bert-epoch=08-valid_loss=0.7020.ckpt\"\n",
    "model.load_from_checkpoint(dti_prediction_head=dti_prediction_head, checkpoint_path=\"weights/DTI_prediction_CLS_token_concatenate/\" + checkpoint_file)\n",
    "\n",
    "pred = trainer.predict(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c9c242a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.53125,\n",
       " 5.50390625,\n",
       " 5.85546875,\n",
       " 4.58984375,\n",
       " 5.5546875,\n",
       " 5.46484375,\n",
       " 5.40234375,\n",
       " 5.484375,\n",
       " 4.93359375,\n",
       " 5.16796875,\n",
       " 5.75,\n",
       " 4.98828125,\n",
       " 5.30078125,\n",
       " 5.296875,\n",
       " 5.5546875,\n",
       " 5.38671875,\n",
       " 5.4453125,\n",
       " 4.8984375,\n",
       " 4.84375,\n",
       " 5.234375,\n",
       " 5.77734375,\n",
       " 5.6015625,\n",
       " 5.22265625,\n",
       " 5.5625,\n",
       " 5.90625,\n",
       " 5.10546875,\n",
       " 5.23046875,\n",
       " 4.91015625,\n",
       " 5.6171875,\n",
       " 5.359375,\n",
       " 5.5546875,\n",
       " 5.23046875,\n",
       " 5.3515625,\n",
       " 5.7109375,\n",
       " 5.4921875,\n",
       " 5.36328125,\n",
       " 5.4453125,\n",
       " 5.390625,\n",
       " 5.6953125,\n",
       " 5.671875,\n",
       " 5.1875,\n",
       " 5.6796875,\n",
       " 5.8828125,\n",
       " 5.28515625,\n",
       " 5.3359375,\n",
       " 5.48046875,\n",
       " 5.234375,\n",
       " 5.03125,\n",
       " 5.80859375,\n",
       " 5.671875,\n",
       " 5.35546875,\n",
       " 5.90234375,\n",
       " 5.1015625,\n",
       " 5.59765625,\n",
       " 5.55859375,\n",
       " 5.74609375,\n",
       " 5.828125,\n",
       " 5.5546875,\n",
       " 4.88671875,\n",
       " 5.00390625,\n",
       " 5.23828125,\n",
       " 5.20703125,\n",
       " 5.796875,\n",
       " 5.6484375,\n",
       " 5.52734375,\n",
       " 5.12890625,\n",
       " 5.140625,\n",
       " 5.59765625,\n",
       " 5.53125,\n",
       " 5.97265625,\n",
       " 5.0703125,\n",
       " 5.5234375,\n",
       " 5.43359375,\n",
       " 4.97265625,\n",
       " 5.0546875,\n",
       " 5.38671875,\n",
       " 5.79296875,\n",
       " 5.56640625,\n",
       " 5.6015625,\n",
       " 5.234375,\n",
       " 5.734375,\n",
       " 5.15234375,\n",
       " 5.48046875,\n",
       " 5.5078125,\n",
       " 5.18359375,\n",
       " 5.6640625,\n",
       " 5.6171875,\n",
       " 5.52734375,\n",
       " 5.1328125,\n",
       " 5.40625,\n",
       " 5.3515625,\n",
       " 5.45703125,\n",
       " 5.4140625,\n",
       " 4.91796875,\n",
       " 5.25390625,\n",
       " 5.24609375,\n",
       " 5.5859375,\n",
       " 5.4765625,\n",
       " 5.39453125,\n",
       " 4.9609375,\n",
       " 5.16796875,\n",
       " 4.79296875,\n",
       " 5.59375,\n",
       " 5.3515625,\n",
       " 5.33203125,\n",
       " 5.640625,\n",
       " 5.66015625,\n",
       " 5.5703125,\n",
       " 5.16015625,\n",
       " 5.18359375,\n",
       " 5.765625,\n",
       " 5.5078125,\n",
       " 5.18359375,\n",
       " 5.08203125,\n",
       " 5.3984375,\n",
       " 4.515625,\n",
       " 5.28125,\n",
       " 5.75390625,\n",
       " 5.76171875,\n",
       " 4.9921875,\n",
       " 5.47265625,\n",
       " 5.5859375,\n",
       " 5.4375,\n",
       " 4.6953125,\n",
       " 5.53125,\n",
       " 5.78125,\n",
       " 5.484375,\n",
       " 5.44140625,\n",
       " 4.96484375,\n",
       " 5.21875,\n",
       " 5.4609375,\n",
       " 5.02734375,\n",
       " 5.43359375,\n",
       " 5.40234375,\n",
       " 5.5625,\n",
       " 5.39453125,\n",
       " 5.46875,\n",
       " 5.1640625,\n",
       " 5.55078125,\n",
       " 5.5546875,\n",
       " 4.88671875,\n",
       " 4.796875,\n",
       " 5.5703125,\n",
       " 5.4375,\n",
       " 5.18359375,\n",
       " 5.56640625,\n",
       " 5.66796875,\n",
       " 5.9765625,\n",
       " 5.67578125,\n",
       " 5.45703125,\n",
       " 4.93359375,\n",
       " 5.48828125,\n",
       " 5.28515625,\n",
       " 5.12109375,\n",
       " 5.08984375,\n",
       " 5.6171875,\n",
       " 5.4609375,\n",
       " 5.6328125,\n",
       " 4.98828125,\n",
       " 5.33984375,\n",
       " 5.62890625,\n",
       " 5.40625,\n",
       " 5.68359375,\n",
       " 5.25,\n",
       " 5.26171875,\n",
       " 5.45703125,\n",
       " 5.34765625,\n",
       " 5.50390625,\n",
       " 5.48828125,\n",
       " 5.25,\n",
       " 5.53125,\n",
       " 5.3671875,\n",
       " 5.3671875,\n",
       " 5.71875,\n",
       " 5.1953125,\n",
       " 5.375,\n",
       " 5.05859375,\n",
       " 5.15234375,\n",
       " 5.5703125,\n",
       " 5.42578125,\n",
       " 5.3046875,\n",
       " 5.96484375,\n",
       " 5.4140625,\n",
       " 5.390625,\n",
       " 5.2421875,\n",
       " 5.2265625,\n",
       " 5.62109375,\n",
       " 5.015625,\n",
       " 6.0546875,\n",
       " 4.68359375,\n",
       " 5.75390625,\n",
       " 5.65625,\n",
       " 5.65234375,\n",
       " 5.69140625,\n",
       " 5.296875,\n",
       " 5.3125,\n",
       " 5.31640625,\n",
       " 5.3671875,\n",
       " 5.0859375,\n",
       " 5.61328125,\n",
       " 5.234375,\n",
       " 4.76171875,\n",
       " 5.234375,\n",
       " 5.265625,\n",
       " 5.1328125,\n",
       " 5.734375,\n",
       " 5.6015625,\n",
       " 5.0703125,\n",
       " 5.49609375,\n",
       " 5.578125,\n",
       " 5.07421875,\n",
       " 5.16796875,\n",
       " 4.71875,\n",
       " 5.30078125,\n",
       " 6.08984375,\n",
       " 5.15234375,\n",
       " 4.953125,\n",
       " 5.8046875,\n",
       " 5.6328125,\n",
       " 5.46875,\n",
       " 5.46484375,\n",
       " 5.671875,\n",
       " 5.5859375,\n",
       " 5.3828125,\n",
       " 5.30078125,\n",
       " 5.4140625,\n",
       " 4.6328125,\n",
       " 5.4140625,\n",
       " 5.69921875,\n",
       " 5.66015625,\n",
       " 5.78515625,\n",
       " 5.10546875,\n",
       " 5.16796875,\n",
       " 5.609375,\n",
       " 5.32421875,\n",
       " 5.82421875,\n",
       " 5.22265625,\n",
       " 5.59375,\n",
       " 5.203125,\n",
       " 4.94921875,\n",
       " 5.58984375,\n",
       " 5.5546875,\n",
       " 5.296875,\n",
       " 5.66796875,\n",
       " 5.703125,\n",
       " 5.7109375,\n",
       " 5.4296875,\n",
       " 5.5546875,\n",
       " 5.73828125,\n",
       " 4.88671875,\n",
       " 5.34765625,\n",
       " 4.9296875,\n",
       " 5.09765625,\n",
       " 5.25,\n",
       " 5.03125,\n",
       " 5.07421875,\n",
       " 5.69921875,\n",
       " 5.68359375,\n",
       " 5.46875,\n",
       " 5.44140625,\n",
       " 5.859375,\n",
       " 4.9296875,\n",
       " 5.79296875,\n",
       " 5.5234375,\n",
       " 5.6953125,\n",
       " 5.03125,\n",
       " 5.671875,\n",
       " 5.69140625,\n",
       " 5.0078125,\n",
       " 5.5546875,\n",
       " 5.56640625,\n",
       " 5.3828125,\n",
       " 5.6953125,\n",
       " 5.32421875,\n",
       " 5.48046875,\n",
       " 5.140625,\n",
       " 5.26953125,\n",
       " 4.92578125,\n",
       " 5.2421875,\n",
       " 5.51171875,\n",
       " 5.18359375,\n",
       " 5.41796875,\n",
       " 5.30078125,\n",
       " 5.2578125,\n",
       " 5.33203125,\n",
       " 5.75390625,\n",
       " 5.39453125,\n",
       " 5.68359375,\n",
       " 5.65625,\n",
       " 5.26171875,\n",
       " 6.13671875,\n",
       " 5.56640625,\n",
       " 5.42578125,\n",
       " 4.98828125,\n",
       " 4.99609375,\n",
       " 5.4609375,\n",
       " 5.4296875,\n",
       " 5.390625,\n",
       " 5.4453125,\n",
       " 4.87109375,\n",
       " 5.234375,\n",
       " 5.69921875,\n",
       " 5.03125,\n",
       " 5.7578125,\n",
       " 4.94140625,\n",
       " 5.6953125,\n",
       " 5.5078125,\n",
       " 5.55078125,\n",
       " 5.47265625,\n",
       " 5.19921875,\n",
       " 5.64453125,\n",
       " 5.57421875,\n",
       " 5.30078125,\n",
       " 5.23046875,\n",
       " 5.76171875,\n",
       " 5.671875,\n",
       " 6.12890625,\n",
       " 5.46875,\n",
       " 5.078125,\n",
       " 5.50390625,\n",
       " 5.2890625,\n",
       " 5.7578125,\n",
       " 5.34765625,\n",
       " 5.79296875,\n",
       " 5.62109375,\n",
       " 5.7421875,\n",
       " 5.88671875,\n",
       " 5.265625,\n",
       " 5.96875,\n",
       " 5.375,\n",
       " 5.7734375,\n",
       " 5.83984375,\n",
       " 5.19140625,\n",
       " 4.61328125,\n",
       " 5.53125,\n",
       " 4.8125,\n",
       " 5.34765625,\n",
       " 5.4375,\n",
       " 5.203125,\n",
       " 5.671875,\n",
       " 5.6015625,\n",
       " 5.703125,\n",
       " 5.73828125,\n",
       " 5.83984375,\n",
       " 5.98046875,\n",
       " 5.68359375,\n",
       " 5.18359375,\n",
       " 4.85546875,\n",
       " 5.04296875,\n",
       " 5.43359375,\n",
       " 4.9765625,\n",
       " 4.9609375,\n",
       " 5.81640625,\n",
       " 5.4296875,\n",
       " 5.31640625,\n",
       " 4.953125,\n",
       " 5.02734375,\n",
       " 5.50390625,\n",
       " 5.3984375,\n",
       " 5.54296875,\n",
       " 5.16015625,\n",
       " 4.9609375,\n",
       " 5.66015625,\n",
       " 5.91796875,\n",
       " 5.55078125,\n",
       " 5.6484375,\n",
       " 5.5625,\n",
       " 5.40234375,\n",
       " 5.35546875,\n",
       " 5.76171875,\n",
       " 5.0390625,\n",
       " 5.45703125,\n",
       " 5.23828125,\n",
       " 5.47265625,\n",
       " 5.6953125,\n",
       " 5.37890625,\n",
       " 5.75390625,\n",
       " 5.43359375,\n",
       " 5.0078125,\n",
       " 5.25390625,\n",
       " 5.36328125,\n",
       " 5.515625,\n",
       " 4.98046875,\n",
       " 5.1953125,\n",
       " 5.7265625,\n",
       " 5.5078125,\n",
       " 5.71484375,\n",
       " 5.29296875,\n",
       " 5.68359375,\n",
       " 5.07421875,\n",
       " 5.20703125,\n",
       " 5.05078125,\n",
       " 4.9375,\n",
       " 5.546875,\n",
       " 5.42578125,\n",
       " 5.81640625,\n",
       " 5.67578125,\n",
       " 5.7109375,\n",
       " 5.26953125,\n",
       " 5.7890625,\n",
       " 5.79296875,\n",
       " 5.28125,\n",
       " 5.18359375,\n",
       " 5.703125,\n",
       " 4.796875,\n",
       " 5.19921875,\n",
       " 5.2734375,\n",
       " 5.90625,\n",
       " 5.86328125,\n",
       " 5.71875,\n",
       " 5.078125,\n",
       " 5.47265625,\n",
       " 5.109375,\n",
       " 5.4765625,\n",
       " 5.18359375,\n",
       " 5.68359375,\n",
       " 5.23046875,\n",
       " 5.3984375,\n",
       " 5.3984375,\n",
       " 5.73046875,\n",
       " 5.96484375,\n",
       " 5.484375,\n",
       " 5.5234375,\n",
       " 5.03515625,\n",
       " 5.12109375,\n",
       " 5.45703125,\n",
       " 5.484375,\n",
       " 5.55859375,\n",
       " 5.546875,\n",
       " 5.37890625,\n",
       " 5.12109375,\n",
       " 5.12109375,\n",
       " 5.5546875,\n",
       " 5.578125,\n",
       " 5.3046875,\n",
       " 5.42578125,\n",
       " 5.02734375,\n",
       " 5.33203125,\n",
       " 5.25,\n",
       " 5.46875,\n",
       " 5.265625,\n",
       " 5.2890625,\n",
       " 5.625,\n",
       " 5.22265625,\n",
       " 5.375,\n",
       " 5.66796875,\n",
       " 5.421875,\n",
       " 5.5859375,\n",
       " 5.8515625,\n",
       " 5.171875,\n",
       " 5.6875,\n",
       " 5.44140625,\n",
       " 5.3671875,\n",
       " 5.38671875,\n",
       " 5.23046875,\n",
       " 5.546875,\n",
       " 5.39453125,\n",
       " 5.61328125,\n",
       " 4.95703125,\n",
       " 4.96484375,\n",
       " 5.6015625,\n",
       " 5.28515625,\n",
       " 5.21875,\n",
       " 5.62890625,\n",
       " 5.2890625,\n",
       " 5.203125,\n",
       " 5.2578125,\n",
       " 5.2109375,\n",
       " 5.30859375,\n",
       " 5.3515625,\n",
       " 4.74609375,\n",
       " 5.390625,\n",
       " 5.5,\n",
       " 4.94921875,\n",
       " 5.62109375,\n",
       " 4.97265625,\n",
       " 5.75390625,\n",
       " 5.32421875,\n",
       " 5.296875,\n",
       " 4.73828125,\n",
       " 5.8515625,\n",
       " 5.53515625,\n",
       " 5.65625,\n",
       " 5.703125,\n",
       " 5.12890625,\n",
       " 5.640625,\n",
       " 4.63671875,\n",
       " 5.35546875,\n",
       " 4.89453125,\n",
       " 5.671875,\n",
       " 5.6484375,\n",
       " 5.453125,\n",
       " 5.39453125,\n",
       " 5.44921875,\n",
       " 5.8984375,\n",
       " 5.53515625,\n",
       " 5.609375,\n",
       " 4.9921875,\n",
       " 5.66015625,\n",
       " 5.609375,\n",
       " 5.36328125,\n",
       " 5.26171875,\n",
       " 5.36328125,\n",
       " 5.14453125,\n",
       " 5.78515625,\n",
       " 5.21484375,\n",
       " 5.53125,\n",
       " 5.51953125,\n",
       " 5.30859375,\n",
       " 5.5859375,\n",
       " 5.4296875,\n",
       " 5.16796875,\n",
       " 5.48828125,\n",
       " 5.8125,\n",
       " 5.40625,\n",
       " 5.26953125,\n",
       " 5.4921875,\n",
       " 5.390625,\n",
       " 5.79296875,\n",
       " 5.25,\n",
       " 5.5390625,\n",
       " 6.1875,\n",
       " 5.33203125,\n",
       " 5.19140625,\n",
       " 5.80859375,\n",
       " 5.48046875,\n",
       " 5.796875,\n",
       " 5.39453125,\n",
       " 5.46484375,\n",
       " 5.5859375,\n",
       " 5.36328125,\n",
       " 5.48046875,\n",
       " 5.390625,\n",
       " 5.4765625,\n",
       " 5.64453125,\n",
       " 5.60546875,\n",
       " 5.16015625,\n",
       " 5.53515625,\n",
       " 5.33984375,\n",
       " 5.375,\n",
       " 5.3515625,\n",
       " 5.375,\n",
       " 5.359375,\n",
       " 5.8828125,\n",
       " 5.375,\n",
       " 5.47265625,\n",
       " 5.61328125,\n",
       " 5.4921875,\n",
       " 5.37109375,\n",
       " 5.4296875,\n",
       " 5.20703125,\n",
       " 5.44140625,\n",
       " 5.45703125,\n",
       " 5.265625,\n",
       " 5.2265625,\n",
       " 5.328125,\n",
       " 5.52734375,\n",
       " 5.61328125,\n",
       " 5.86328125,\n",
       " 5.265625,\n",
       " 6.09765625,\n",
       " 5.078125,\n",
       " 5.359375,\n",
       " 5.1875,\n",
       " 5.55078125,\n",
       " 5.66796875,\n",
       " 5.5078125,\n",
       " 5.73828125,\n",
       " 5.015625,\n",
       " 5.25390625,\n",
       " 5.5390625,\n",
       " 5.4140625,\n",
       " 5.13671875,\n",
       " 5.76953125,\n",
       " 6.02734375,\n",
       " 5.328125,\n",
       " 5.10546875,\n",
       " 4.96484375,\n",
       " 5.375,\n",
       " 5.48828125,\n",
       " 5.6953125,\n",
       " 5.53125,\n",
       " 5.38671875,\n",
       " 5.625,\n",
       " 5.48046875,\n",
       " 5.70703125,\n",
       " 5.3984375,\n",
       " 5.26171875,\n",
       " 5.265625,\n",
       " 5.37890625,\n",
       " 5.6875,\n",
       " 5.4921875,\n",
       " 5.25,\n",
       " 5.578125,\n",
       " 5.828125,\n",
       " 5.43359375,\n",
       " 5.5234375,\n",
       " 5.41015625,\n",
       " 4.99609375,\n",
       " 5.2734375,\n",
       " 5.5859375,\n",
       " 5.33984375,\n",
       " 5.59765625,\n",
       " 5.68359375,\n",
       " 5.5703125,\n",
       " 5.57421875,\n",
       " 5.33984375,\n",
       " 5.17578125,\n",
       " 5.51953125,\n",
       " 5.24609375,\n",
       " 5.30078125,\n",
       " 5.31640625,\n",
       " 5.62109375,\n",
       " 5.16796875,\n",
       " 5.51171875,\n",
       " 4.90625,\n",
       " 5.44140625,\n",
       " 5.53125,\n",
       " 5.53515625,\n",
       " 5.3671875,\n",
       " 5.62890625,\n",
       " 5.51953125,\n",
       " 5.28125,\n",
       " 5.37109375,\n",
       " 5.62109375,\n",
       " 5.54296875,\n",
       " 5.2265625,\n",
       " 5.3515625,\n",
       " 5.265625,\n",
       " 5.94921875,\n",
       " 5.16015625,\n",
       " 5.00390625,\n",
       " 4.9921875,\n",
       " 5.328125,\n",
       " 5.33203125,\n",
       " 5.40625,\n",
       " 5.01953125,\n",
       " 5.5625,\n",
       " 5.64453125,\n",
       " 5.2109375,\n",
       " 5.79296875,\n",
       " 5.72265625,\n",
       " 5.34765625,\n",
       " 5.46484375,\n",
       " 5.109375,\n",
       " 4.84375,\n",
       " 5.4765625,\n",
       " 5.53515625,\n",
       " 5.19921875,\n",
       " 5.1171875,\n",
       " 5.33203125,\n",
       " 5.3203125,\n",
       " 4.97265625,\n",
       " 5.38671875,\n",
       " 5.38671875,\n",
       " 5.04296875,\n",
       " 5.22265625,\n",
       " 5.4140625,\n",
       " 5.43359375,\n",
       " 5.57421875,\n",
       " 5.07421875,\n",
       " 5.51953125,\n",
       " 5.46875,\n",
       " 5.48828125,\n",
       " 5.0625,\n",
       " 5.25,\n",
       " 5.484375,\n",
       " 5.109375,\n",
       " 6.16015625,\n",
       " 5.59765625,\n",
       " 6.0390625,\n",
       " 5.33984375,\n",
       " 5.296875,\n",
       " 5.39453125,\n",
       " 5.53515625,\n",
       " 5.2578125,\n",
       " 5.44921875,\n",
       " 5.1953125,\n",
       " 5.25,\n",
       " 5.45703125,\n",
       " 5.27734375,\n",
       " 5.34765625,\n",
       " 5.39453125,\n",
       " 5.09765625,\n",
       " 5.2421875,\n",
       " 5.71484375,\n",
       " 5.5,\n",
       " 5.6796875,\n",
       " 5.50390625,\n",
       " 5.484375,\n",
       " 5.53515625,\n",
       " 5.125,\n",
       " 5.4609375,\n",
       " 5.484375,\n",
       " 5.3203125,\n",
       " 5.0859375,\n",
       " 5.63671875,\n",
       " 5.53515625,\n",
       " 5.703125,\n",
       " 5.3828125,\n",
       " 5.28125,\n",
       " 5.34375,\n",
       " 5.43359375,\n",
       " 5.453125,\n",
       " 5.33203125,\n",
       " 5.46875,\n",
       " 5.69140625,\n",
       " 5.390625,\n",
       " 5.4765625,\n",
       " 5.10546875,\n",
       " 5.30078125,\n",
       " 5.64453125,\n",
       " 5.07421875,\n",
       " 5.6875,\n",
       " 5.65234375,\n",
       " 5.4296875,\n",
       " 5.046875,\n",
       " 5.16015625,\n",
       " 5.6484375,\n",
       " 5.60546875,\n",
       " 5.10546875,\n",
       " 5.47265625,\n",
       " 5.48046875,\n",
       " 5.17578125,\n",
       " 5.52734375,\n",
       " 5.73046875,\n",
       " 5.26171875,\n",
       " 5.1875,\n",
       " 5.62890625,\n",
       " 5.53125,\n",
       " 5.50390625,\n",
       " 5.69921875,\n",
       " 5.08203125,\n",
       " 5.05078125,\n",
       " 5.7109375,\n",
       " 5.55859375,\n",
       " 4.7421875,\n",
       " 5.5859375,\n",
       " 5.3828125,\n",
       " 5.2109375,\n",
       " 5.1484375,\n",
       " 4.8984375,\n",
       " 5.3671875,\n",
       " 5.30859375,\n",
       " 5.5546875,\n",
       " 5.7109375,\n",
       " 5.7734375,\n",
       " 5.39453125,\n",
       " 5.72265625,\n",
       " 5.17578125,\n",
       " 5.4296875,\n",
       " 5.72265625,\n",
       " 5.87109375,\n",
       " 4.9609375,\n",
       " 5.203125,\n",
       " 5.6640625,\n",
       " 5.546875,\n",
       " 4.82421875,\n",
       " 4.96484375,\n",
       " 5.52734375,\n",
       " 5.609375,\n",
       " 5.60546875,\n",
       " 5.81640625,\n",
       " 5.05078125,\n",
       " 5.81640625,\n",
       " 5.44140625,\n",
       " 5.28515625,\n",
       " 5.0859375,\n",
       " 5.28125,\n",
       " 6.1328125,\n",
       " 5.53515625,\n",
       " 5.12109375,\n",
       " 5.37890625,\n",
       " 5.08203125,\n",
       " 5.05859375,\n",
       " 5.6015625,\n",
       " 4.88671875,\n",
       " 5.59375,\n",
       " 5.48046875,\n",
       " 5.20703125,\n",
       " 5.64453125,\n",
       " 5.19921875,\n",
       " 4.796875,\n",
       " 5.54296875,\n",
       " 4.86328125,\n",
       " 4.91796875,\n",
       " 5.4453125,\n",
       " 5.34375,\n",
       " 5.2890625,\n",
       " 4.91015625,\n",
       " 5.12890625,\n",
       " 4.76953125,\n",
       " 5.265625,\n",
       " 5.2890625,\n",
       " 5.4453125,\n",
       " 5.203125,\n",
       " 5.4765625,\n",
       " 4.92578125,\n",
       " 5.3125,\n",
       " 5.2890625,\n",
       " 5.12109375,\n",
       " 6.140625,\n",
       " 4.9140625,\n",
       " 5.45703125,\n",
       " 5.4921875,\n",
       " 5.0234375,\n",
       " 5.4140625,\n",
       " 5.15234375,\n",
       " 5.19921875,\n",
       " 5.07421875,\n",
       " 4.9375,\n",
       " 5.60546875,\n",
       " 6.015625,\n",
       " 5.421875,\n",
       " 5.20703125,\n",
       " 5.5,\n",
       " 5.62109375,\n",
       " 5.38671875,\n",
       " 4.90234375,\n",
       " 5.1875,\n",
       " 5.4140625,\n",
       " 5.3359375,\n",
       " 4.83203125,\n",
       " 5.515625,\n",
       " 5.5625,\n",
       " 4.94140625,\n",
       " 5.51171875,\n",
       " 5.41015625,\n",
       " 5.27734375,\n",
       " 5.2734375,\n",
       " 5.19140625,\n",
       " 5.5390625,\n",
       " 5.2734375,\n",
       " 5.47265625,\n",
       " 5.140625,\n",
       " 5.2734375,\n",
       " 5.2421875,\n",
       " 5.28125,\n",
       " 5.90625,\n",
       " 5.26171875,\n",
       " 5.4453125,\n",
       " 5.2265625,\n",
       " 5.0234375,\n",
       " 5.296875,\n",
       " 5.515625,\n",
       " 5.98046875,\n",
       " 4.7734375,\n",
       " 5.3359375,\n",
       " 5.4375,\n",
       " 5.5390625,\n",
       " 5.3984375,\n",
       " 4.98046875,\n",
       " 5.6953125,\n",
       " 5.65625,\n",
       " 5.546875,\n",
       " 5.76953125,\n",
       " 5.05078125,\n",
       " 5.3515625,\n",
       " 5.375,\n",
       " 5.6171875,\n",
       " 5.69921875,\n",
       " 5.26171875,\n",
       " 5.07421875,\n",
       " 4.7734375,\n",
       " 5.5546875,\n",
       " 5.20703125,\n",
       " 5.0625,\n",
       " 5.859375,\n",
       " 5.3828125,\n",
       " 5.37890625,\n",
       " 5.36328125,\n",
       " 4.99609375,\n",
       " 5.6484375,\n",
       " 5.859375,\n",
       " 5.2890625,\n",
       " 5.484375,\n",
       " 4.98828125,\n",
       " 5.56640625,\n",
       " 5.66796875,\n",
       " 5.359375,\n",
       " 5.5625,\n",
       " 5.01171875,\n",
       " 5.66796875,\n",
       " 5.38671875,\n",
       " 5.72265625,\n",
       " 5.44140625,\n",
       " 5.03515625,\n",
       " 5.16015625,\n",
       " 5.47265625,\n",
       " 5.55859375,\n",
       " 5.5546875,\n",
       " 4.953125,\n",
       " 4.9609375,\n",
       " 5.4296875,\n",
       " 5.0703125,\n",
       " 5.59765625,\n",
       " 5.375,\n",
       " 5.33984375,\n",
       " 4.99609375,\n",
       " 5.88671875,\n",
       " 5.25,\n",
       " 5.4375,\n",
       " 5.421875,\n",
       " 5.51171875,\n",
       " 5.39453125,\n",
       " 5.8671875,\n",
       " 5.7109375,\n",
       " 5.05078125,\n",
       " 5.06640625,\n",
       " 5.64453125,\n",
       " 5.5,\n",
       " 5.79296875,\n",
       " 5.4375,\n",
       " 5.62890625,\n",
       " 5.73828125,\n",
       " 5.875,\n",
       " 4.91015625,\n",
       " 5.54296875,\n",
       " 5.234375,\n",
       " 5.3046875,\n",
       " 5.26953125,\n",
       " 5.35546875,\n",
       " 4.70703125,\n",
       " 5.2265625,\n",
       " 5.62890625,\n",
       " 5.8515625,\n",
       " 5.61328125,\n",
       " 5.6171875,\n",
       " 5.140625,\n",
       " 5.76171875,\n",
       " 5.359375,\n",
       " 5.4453125,\n",
       " 5.6015625,\n",
       " 5.60546875,\n",
       " 5.23828125,\n",
       " 5.1875,\n",
       " 5.25390625,\n",
       " 5.22265625,\n",
       " 5.5234375,\n",
       " 5.2421875,\n",
       " 5.40625,\n",
       " 4.9765625,\n",
       " 6.07421875,\n",
       " 5.19140625,\n",
       " 5.5703125,\n",
       " 5.46875,\n",
       " 5.87109375,\n",
       " 5.8046875,\n",
       " 5.30078125,\n",
       " 5.52734375,\n",
       " 5.609375,\n",
       " 5.01171875,\n",
       " 5.078125,\n",
       " 5.55859375,\n",
       " 5.3984375,\n",
       " 5.7734375,\n",
       " 4.859375,\n",
       " 5.65234375,\n",
       " 5.52734375,\n",
       " 4.79296875,\n",
       " 5.8515625,\n",
       " 5.42578125,\n",
       " 4.86328125,\n",
       " 5.36328125,\n",
       " 5.5,\n",
       " 4.99609375,\n",
       " 5.5703125,\n",
       " 5.2578125,\n",
       " 5.23828125,\n",
       " 5.56640625,\n",
       " 5.46484375,\n",
       " 5.33984375,\n",
       " 5.640625,\n",
       " 5.3046875,\n",
       " 5.2734375,\n",
       " 5.3046875,\n",
       " 5.52734375,\n",
       " 5.6484375,\n",
       " 5.11328125,\n",
       " 5.234375,\n",
       " 4.78125,\n",
       " 5.765625,\n",
       " 5.4140625,\n",
       " 5.2734375,\n",
       " 5.640625,\n",
       " 5.60546875,\n",
       " 5.38671875,\n",
       " 5.6953125,\n",
       " 5.3515625,\n",
       " 5.109375,\n",
       " 5.359375,\n",
       " 5.51171875,\n",
       " 5.05078125,\n",
       " 5.28515625,\n",
       " 5.49609375,\n",
       " 6.00390625,\n",
       " 5.05859375,\n",
       " 5.05078125,\n",
       " 5.28125,\n",
       " 5.1875,\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "for p in pred:\n",
    "    res += p.tolist()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa896a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.5773\tMSE: 0.6834\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as metrics\n",
    "\n",
    "true = test_dataloader.dataset.data.Y.values\n",
    "\n",
    "mae = metrics.mean_absolute_error(true, res).round(4)\n",
    "mse = metrics.mean_squared_error(true, res).round(4)\n",
    "print(f\"MAE: {mae}\\tMSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61195293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
