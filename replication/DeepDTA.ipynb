{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYHrhl0s2Dwh"
   },
   "outputs": [],
   "source": [
    "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install pytorch-lightning\n",
    "!pip install PyTDC\n",
    "# !pip install torch==1.9\n",
    "# !pip install torchtext==0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZfTQ8az2B5E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import torch_xla.core.xla_model as xm\n",
    "\n",
    "from torchmetrics import MeanAbsoluteError\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tdc.multi_pred import DTI\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "davis = DTI(name=\"DAVIS\")\n",
    "davis.convert_to_log(form=\"binding\")\n",
    "davis_split = davis.get_split()\n",
    "\n",
    "train_df = davis_split[\"train\"].reset_index(drop=True)\n",
    "valid_df = davis_split[\"valid\"].reset_index(drop=True)\n",
    "test_df = davis_split[\"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"train: {train_df.shape} valid: {valid_df.shape} test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HzC3HPD9MKF7"
   },
   "outputs": [],
   "source": [
    "def generate_vocab(corpus):\n",
    "    token_index = 1\n",
    "    stoi, itos = {}, {}\n",
    "    stoi[\"<PAD>\"] = 0\n",
    "    itos[0] = \"<PAD>\"\n",
    "\n",
    "    for line in corpus:\n",
    "        for token in line:\n",
    "            if token not in stoi:\n",
    "                itos[token_index] = token\n",
    "                stoi[token] = token_index\n",
    "                token_index += 1\n",
    "\n",
    "    return stoi, itos\n",
    "\n",
    "drug_stoi, drug_itos = generate_vocab(train_df['Drug'].values)\n",
    "target_stoi, target_itos = generate_vocab(train_df['Target'].values)\n",
    "\n",
    "drug_vocab_dim = len(drug_stoi)\n",
    "target_vocab_dim = len(target_stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4-R4KrIb3NW5"
   },
   "outputs": [],
   "source": [
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, drug, target, y, drug_stoi, target_stoi):\n",
    "        self.drug = drug\n",
    "        self.target = target\n",
    "\n",
    "        self.drug_max_seq_len = 85\n",
    "        self.target_max_seq_len = 1200\n",
    "\n",
    "        self.y = y\n",
    "        self.drug_stoi = drug_stoi\n",
    "        self.target_stoi = target_stoi\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.drug.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        drug = [self.drug_stoi[s] for s in self.drug[idx]]\n",
    "        if len(drug) < self.drug_max_seq_len:\n",
    "            drug_padding = self.drug_max_seq_len - len(drug)\n",
    "            drug += [0] * drug_padding\n",
    "        else:\n",
    "            drug = drug[:self.drug_max_seq_len]\n",
    "\n",
    "        target = [self.target_stoi[s] for s in self.target[idx]]\n",
    "        if len(target) < self.target_max_seq_len:\n",
    "            target_padding = self.target_max_seq_len - len(target)\n",
    "            target += [0] * target_padding\n",
    "        else:\n",
    "            target = target[:self.target_max_seq_len]\n",
    "\n",
    "        y = self.y[idx]\n",
    "\n",
    "        return torch.tensor(drug).float(), torch.tensor(target).float(), torch.tensor(y).float()\n",
    "    \n",
    "train_dataset = DTIDataset(train_df[\"Drug\"], train_df[\"Target\"], train_df[\"Y\"], drug_stoi, target_stoi)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=12, pin_memory=True, shuffle=True)\n",
    "\n",
    "valid_dataset = DTIDataset(valid_df[\"Drug\"], valid_df[\"Target\"], valid_df[\"Y\"], drug_stoi, target_stoi)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=12, pin_memory=True, shuffle=True)\n",
    "\n",
    "test_dataset = DTIDataset(test_df[\"Drug\"], test_df[\"Target\"], test_df[\"Y\"], drug_stoi, target_stoi)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=12, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OaKYAm7g7TH3"
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_dim, embedding_dim, dropout_rate):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_dim, self.embedding_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x.long())\n",
    "        embedding = self.dropout(embedding)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_dim, embedding_dim, kernel_size, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_dim, embedding_dim, dropout_rate)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=kernel_size[0], padding=1)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=kernel_size[1], padding=1)        \n",
    "        self.activation2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=96, kernel_size=kernel_size[2], padding=1)\n",
    "        self.activation3 = nn.ReLU()\n",
    "\n",
    "        self.fc = nn.Linear(96, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.embedding(x)\n",
    "        x = x.moveaxis(1, 2)\n",
    "\n",
    "        x = self.activation1(self.conv1(x))\n",
    "        x = self.activation2(self.conv2(x))\n",
    "        x = self.activation3(self.conv3(x))\n",
    "        x = F.adaptive_max_pool1d(x, output_size=1)\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        x = self.fc(x.float())\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DTI(nn.Module):\n",
    "    def __init__(self, drug_vocab_dim, target_vocab_dim, embedding_dim, dropout_rate=0.1):\n",
    "        super(DTI, self).__init__()\n",
    "        drug_encoder_kernel_size = [4, 6, 8]\n",
    "        target_encoder_kernel_size = [4, 8, 12]\n",
    "\n",
    "        self.drug_encoder = Encoder(drug_vocab_dim, embedding_dim, drug_encoder_kernel_size, dropout_rate=dropout_rate)\n",
    "        self.target_encoder = Encoder(drug_vocab_dim, embedding_dim, target_encoder_kernel_size, dropout_rate=dropout_rate)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 2, 1024)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.activation3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.out = nn.Linear(512, 1)\n",
    "\n",
    "\n",
    "    def forward(self, drug, target):\n",
    "        drug_encoding = self.drug_encoder(drug)\n",
    "        target_encoding = self.target_encoder(target)\n",
    "\n",
    "        x = torch.cat((drug_encoding, target_encoding), axis=1)\n",
    "\n",
    "        x = self.activation1(self.fc1(x))\n",
    "        x = self.activation2(self.fc2(x))\n",
    "        x = self.dropout(self.activation3(self.fc3(x)))\n",
    "        \n",
    "        out = self.out(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "iwP8dQzLEm9u"
   },
   "outputs": [],
   "source": [
    "class DeepDTI(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super(DeepDTI, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        drug, target, y = batch\n",
    "        y_hat = self.model(drug, target)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        drug, target, y = batch\n",
    "        y_hat = self.model(drug, target)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"valid_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    " \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        drug, target, y = batch\n",
    "        y_hat = self.model(drug, target)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)\n",
    "        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"valid_loss\"}\n",
    "\n",
    "    \n",
    "def define_callbacks(patience):\n",
    "    return EarlyStopping('valid_loss', patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdldNIMK9J2r"
   },
   "outputs": [],
   "source": [
    "prediction_head = DTI(drug_vocab_dim, target_vocab_dim, embedding_dim=32)\n",
    "model = DeepDTI(prediction_head, 0.001)\n",
    "callbacks = define_callbacks(patience=30)\n",
    "# trainer = pl.Trainer(accelerator=\"cpu\", num_processes=1, max_epochs=1, enable_progress_bar=True)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=100, enable_progress_bar=True, callbacks=callbacks, default_root_dir=\"drive/MyDrive/DeepDTA_CKPT\")\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiVZ1yjtVKts"
   },
   "outputs": [],
   "source": [
    "trainer.test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "80A5aotm6ecA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepDTA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
