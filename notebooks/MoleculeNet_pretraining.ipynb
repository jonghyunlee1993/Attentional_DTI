{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc8dbaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import torchtext\n",
    "\n",
    "import pickle\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# with open(\"data/molecule_small.pickle\", 'rb') as f:\n",
    "#     molecules = pickle.load(f)\n",
    "\n",
    "with open(\"data/molecule_total.pickle\", 'rb') as f:\n",
    "    molecules = pickle.load(f)\n",
    "\n",
    "train_data = molecules[int(len(molecules) * 0.2):]\n",
    "test_data  = molecules[:int(len(molecules) * 0.2)]\n",
    "\n",
    "tokenizer = torchtext.legacy.data.Field(tokenize=None,\n",
    "                                        init_token='<CLS>',\n",
    "                                        eos_token='<SEP>',\n",
    "                                        pad_token='<PAD>',\n",
    "                                        unk_token='<MASK>',\n",
    "                                        lower=False,\n",
    "                                        batch_first=False,\n",
    "                                        include_lengths=False)\n",
    "\n",
    "tokenizer.build_vocab(train_data, min_freq=1)\n",
    "\n",
    "with open(\"data/MoleculeNet_tokenizer.pickle\", 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# with open(\"data/MoleculeNet_tokenizer.pickle\", \"rb\") as f:\n",
    "#     tokenizer = pickle.load(f)\n",
    "\n",
    "vocab_dim     = len(tokenizer.vocab.itos)\n",
    "seq_len       = 256\n",
    "embedding_dim = 512\n",
    "device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size    = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82d6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeLangaugeModelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, seq_len=128, masking_rate=0.15):\n",
    "        super(MoleculeLangaugeModelDataset, self).__init__()\n",
    "\n",
    "        self.data          = data        \n",
    "        self.tokenizer     = tokenizer\n",
    "        self.vocab         = tokenizer.vocab\n",
    "        self.seq_len       = seq_len\n",
    "        self.masking_rate  = masking_rate\n",
    "        \n",
    "        self.cls_token_id  = self.tokenizer.vocab.stoi[self.tokenizer.init_token]\n",
    "        self.sep_token_id  = self.tokenizer.vocab.stoi[self.tokenizer.eos_token]\n",
    "        self.pad_token_id  = self.tokenizer.vocab.stoi[self.tokenizer.pad_token]\n",
    "        self.mask_token_id = self.tokenizer.vocab.stoi[self.tokenizer.unk_token]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            target = self.tokenizer.numericalize(self.data[idx]).squeeze()\n",
    "\n",
    "            if len(target) < self.seq_len - 2:\n",
    "                pad_length = self.seq_len - len(target) - 2\n",
    "            else:\n",
    "                target = target[:self.seq_len-2]\n",
    "                pad_length = 0\n",
    "\n",
    "            masked_sent, masking_label = self.masking(target)\n",
    "\n",
    "            # MLM\n",
    "            train = torch.cat([\n",
    "                torch.tensor([self.cls_token_id]), \n",
    "                masked_sent,\n",
    "                torch.tensor([self.sep_token_id]),\n",
    "                torch.tensor([self.pad_token_id] * pad_length)\n",
    "            ]).long().contiguous()\n",
    "\n",
    "            target = torch.cat([\n",
    "                torch.tensor([self.cls_token_id]), \n",
    "                target,\n",
    "                torch.tensor([self.sep_token_id]),\n",
    "                torch.tensor([self.pad_token_id] * pad_length)\n",
    "            ]).long().contiguous()\n",
    "\n",
    "            masking_label = torch.cat([\n",
    "                torch.zeros(1), \n",
    "                masking_label,\n",
    "                torch.zeros(1),\n",
    "                torch.zeros(pad_length)\n",
    "            ])\n",
    "\n",
    "            segment_embedding = torch.zeros(target.size(0))\n",
    "        \n",
    "            return train, target, segment_embedding, masking_label\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        for x in self.data:\n",
    "            yield x\n",
    "            \n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    \n",
    "    # TODO mask 안에서 random 으로 바꿔주는 것 추가\n",
    "    def masking(self, x):\n",
    "        x             = torch.tensor(x).long().contiguous()\n",
    "        masking_idx   = torch.randperm(x.size()[0])[:round(x.size()[0] * self.masking_rate) + 1]       \n",
    "        masking_label = torch.zeros(x.size()[0])\n",
    "        masking_label[masking_idx] = 1\n",
    "        x             = x.masked_fill(masking_label.bool(), self.mask_token_id)\n",
    "        \n",
    "        return x, masking_label\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f57e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://inhyeokyoo.github.io/project/nlp/bert-issue/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, pad_token_id):\n",
    "        super(BERT, self).__init__()\n",
    "        self.pad_token_id  = pad_token_id\n",
    "        self.nhead         = 4\n",
    "        self.embedding     = BERTEmbedding(vocab_dim, seq_len, embedding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=self.nhead, batch_first=True)\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, num_layers=4)\n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        pad_mask  = BERT.get_attn_pad_mask(data, data, self.pad_token_id).repeat(self.nhead, 1, 1)\n",
    "        embedding = self.embedding(data, segment_embedding)\n",
    "        output    = self.encoder_block(embedding, pad_mask) \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "        batch_size, len_q = seq_q.size()\n",
    "        batch_size, len_k = seq_k.size()\n",
    "        pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "        pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "        \n",
    "        return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa52cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, dropout_rate=0.1, device=device):\n",
    "        super(BERTEmbedding, self).__init__()\n",
    "        self.seq_len       = seq_len\n",
    "        self.vocab_dim     = vocab_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate  = dropout_rate\n",
    "        \n",
    "        # vocab --> embedding\n",
    "        self.token_embedding      = nn.Embedding(self.vocab_dim, self.embedding_dim) \n",
    "        self.token_dropout        = nn.Dropout(self.dropout_rate)    \n",
    "        \n",
    "        # seq len --> embedding\n",
    "        self.positional_embedding = nn.Embedding(self.seq_len, self.embedding_dim)\n",
    "        self.positional_dropout   = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        # segment (0, 1) --> embedding\n",
    "        self.segment_embedding    = nn.Embedding(2, self.embedding_dim)\n",
    "        self.segment_dropout      = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        token_embedding      = self.token_embedding(data)\n",
    "        token_embedding      = self.token_dropout(token_embedding)\n",
    "        \n",
    "        positional_encoding  = torch.arange(start=0, end=self.seq_len, step=1).long()\n",
    "        # data의 device 정보 가져와서 처리\n",
    "        positional_encoding  = positional_encoding.unsqueeze(0).expand(data.size()).to(device)\n",
    "        positional_embedding = self.positional_embedding(positional_encoding)\n",
    "        positional_embedding = self.positional_dropout(positional_embedding)\n",
    "        \n",
    "        segment_embedding    = self.segment_embedding(segment_embedding)\n",
    "        segment_embedding    = self.segment_dropout(segment_embedding)\n",
    "        \n",
    "        return token_embedding + positional_embedding + segment_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69529541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModeling(nn.Module):\n",
    "    def __init__(self, bert, output_dim):\n",
    "        super(MaskedLanguageModeling, self).__init__()\n",
    "        self.bert = bert\n",
    "        d_model   = bert.embedding.token_embedding.weight.size(1)\n",
    "        self.rnn  = nn.GRU(d_model, d_model)\n",
    "        self.fc   = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x, segment_embedding):\n",
    "        output    = self.bert(x, segment_embedding)\n",
    "        output, _ = self.rnn(output)\n",
    "        output    = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eceeb718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device, clip=1):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_corrects = 0\n",
    "    epoch_num_data = 0\n",
    "\n",
    "    for batch, (X, target, segment_emb, masking_label) in enumerate(tqdm(iterator)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(X.to(device), segment_emb.long().to(device))\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "#         output = output[masking_label.bool().to(device)].reshape(-1, output_dim)\n",
    "#         target = target.reshape(-1)[masking_label.reshape(-1).bool()].to(device)\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        target = target.reshape(-1).to(device)\n",
    "        loss   = criterion(output, target)\n",
    "        loss.backward()\n",
    "                \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss     += loss.item()\n",
    "        unpad_mask      = (target != 1)\n",
    "        epoch_corrects += (output.argmax(1)[unpad_mask] == target[unpad_mask]).float().sum()\n",
    "        epoch_num_data += len(unpad_mask)\n",
    "        \n",
    "    return epoch_loss / len(iterator), 100 * epoch_corrects / epoch_num_data\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, iterator, optimizer, criterion, device):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_corrects = 0\n",
    "    epoch_num_data = 0\n",
    "\n",
    "    for batch, (X, target, segment_emb, masking_label) in enumerate(tqdm(iterator)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(X.to(device), segment_emb.long().to(device))\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "#         output = output[masking_label.bool().to(device)].reshape(-1, output_dim)\n",
    "#         target = target.reshape(-1)[masking_label.reshape(-1).bool()].to(device)\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        target = target.reshape(-1).to(device)\n",
    "        loss   = criterion(output, target)\n",
    "        \n",
    "        epoch_loss     += loss.item()\n",
    "        unpad_mask      = (target != 1)\n",
    "        epoch_corrects += (output.argmax(1)[unpad_mask] == target[unpad_mask]).float().sum()\n",
    "        epoch_num_data += len(unpad_mask)\n",
    "        \n",
    "    return epoch_loss / len(iterator), 100 * epoch_corrects / epoch_num_data\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, iterator, device, tokenizer):\n",
    "    model.eval()\n",
    "    \n",
    "    for batch, (X, target, segment_emb, masking_label) in enumerate(iterator):\n",
    "        output = model(X.to(device), segment_emb.long().to(device))\n",
    "    \n",
    "        output_ = torch.argmax(output.clone().detach().to(\"cpu\"), axis=-1)\n",
    "        target_ = target.clone().detach().to(\"cpu\")\n",
    "\n",
    "        output_list = decode(output_, tokenizer)\n",
    "        target_list = decode(target_, tokenizer)\n",
    "\n",
    "    return output_list, target_list\n",
    "\n",
    "\n",
    "def decode(x, tokenizer):\n",
    "    results = []\n",
    "    for line in x:\n",
    "        decoded = \"\"\n",
    "        for s in line:\n",
    "            decoded += tokenizer.vocab.itos[s]\n",
    "        results.append(decoded)\n",
    "        \n",
    "    return results \n",
    "\n",
    "\n",
    "def generate_epoch_dataloader(data, seq_len, tokenizer, masking_rate, batch_size, collate_fn, shuffle=True, num_workers=6):\n",
    "    dataset    = MoleculeLangaugeModelDataset(data=data, seq_len=seq_len, tokenizer=tokenizer, masking_rate=masking_rate)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def generate_epoch_prediction_dataloader(data, seq_len, tokenizer, masking_rate, batch_size, collate_fn, shuffle=True, num_workers=5):    \n",
    "    dataset    = MoleculeLangaugeModelDataset(data=data, seq_len=seq_len, tokenizer=tokenizer, masking_rate=masking_rate)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c77d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "bert_base = BERT(vocab_dim=vocab_dim, seq_len=seq_len, embedding_dim=embedding_dim, pad_token_id=1).to(device)\n",
    "model     = MaskedLanguageModeling(bert_base, output_dim=vocab_dim).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=[0.9, 0.999], weight_decay=0.01)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786f02e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 Masking rate: 0.6 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.8520 | Train Acc: 14.4685\n",
      "Valid Loss: 0.1999 | Valid Acc: 20.2760\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0001 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.4932 | Train Acc: 16.7142\n",
      "Valid Loss: 0.1629 | Valid Acc: 20.5015\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0002 Masking rate: 0.3 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.2051 | Train Acc: 18.6572\n",
      "Valid Loss: 0.1469 | Valid Acc: 20.6192\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0003 Masking rate: 0.6 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.6306 | Train Acc: 15.7760\n",
      "Valid Loss: 0.1458 | Valid Acc: 20.6058\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0004 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.4262 | Train Acc: 17.1502\n",
      "Valid Loss: 0.1364 | Valid Acc: 20.7331\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0005 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.4170 | Train Acc: 17.2246\n",
      "Valid Loss: 0.1321 | Valid Acc: 20.6863\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0006 Masking rate: 0.6 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.5945 | Train Acc: 16.0096\n",
      "Valid Loss: 0.1324 | Valid Acc: 20.6935\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0007 Masking rate: 0.3 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.1772 | Train Acc: 18.8609\n",
      "Valid Loss: 0.1271 | Valid Acc: 20.7004\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0008 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.4037 | Train Acc: 17.2961\n",
      "Valid Loss: 0.1275 | Valid Acc: 20.7206\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0009 Masking rate: 0.3 Train dataset: 1000000 Valid dataset: 100000\n",
      "Epoch: 0010 Masking rate: 0.6 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.5902 | Train Acc: 16.0458\n",
      "Valid Loss: 0.1262 | Valid Acc: 20.7850\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0011 Masking rate: 0.3 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.1735 | Train Acc: 18.8905\n",
      "Valid Loss: 0.1250 | Valid Acc: 20.7629\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0012 Masking rate: 0.6 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.5856 | Train Acc: 16.0502\n",
      "Valid Loss: 0.1270 | Valid Acc: 20.7359\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0013 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.4011 | Train Acc: 17.2989\n",
      "Valid Loss: 0.1262 | Valid Acc: 20.7071\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0014 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.4003 | Train Acc: 17.3121\n",
      "Valid Loss: 0.1259 | Valid Acc: 20.7638\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0015 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.3998 | Train Acc: 17.3415\n",
      "Valid Loss: 0.1253 | Valid Acc: 20.7253\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0016 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.3981 | Train Acc: 17.3456\n",
      "Valid Loss: 0.1238 | Valid Acc: 20.7432\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0017 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.3951 | Train Acc: 17.3664\n",
      "Valid Loss: 0.1225 | Valid Acc: 20.7537\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0018 Masking rate: 0.3 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.1681 | Train Acc: 18.9233\n",
      "Valid Loss: 0.1188 | Valid Acc: 20.7965\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0019 Masking rate: 0.6 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.5751 | Train Acc: 16.1112\n",
      "Valid Loss: 0.1229 | Valid Acc: 20.7577\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0020 Masking rate: 0.4 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.2575 | Train Acc: 18.2898\n",
      "Valid Loss: 0.1179 | Valid Acc: 20.8120\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0021 Masking rate: 0.3 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.1629 | Train Acc: 18.9573\n",
      "Valid Loss: 0.1156 | Valid Acc: 20.8085\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0022 Masking rate: 0.3 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.1609 | Train Acc: 18.9625\n",
      "Valid Loss: 0.1126 | Valid Acc: 20.8305\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0023 Masking rate: 0.6 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.5624 | Train Acc: 16.2066\n",
      "Valid Loss: 0.1158 | Valid Acc: 20.8330\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0024 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.3736 | Train Acc: 17.4875\n",
      "Valid Loss: 0.1121 | Valid Acc: 20.8212\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0025 Masking rate: 0.3 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.1557 | Train Acc: 19.0023\n",
      "Valid Loss: 0.1083 | Valid Acc: 20.8753\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0026 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.3690 | Train Acc: 17.5453\n",
      "Valid Loss: 0.1087 | Valid Acc: 20.8807\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0027 Masking rate: 0.4 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.2401 | Train Acc: 18.4090\n",
      "Valid Loss: 0.1077 | Valid Acc: 20.8926\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0028 Masking rate: 0.4 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.2390 | Train Acc: 18.4214\n",
      "Valid Loss: 0.1064 | Valid Acc: 20.8673\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0029 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.3632 | Train Acc: 17.5634\n",
      "Valid Loss: 0.1068 | Valid Acc: 20.8788\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0030 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.3631 | Train Acc: 17.5777\n",
      "Valid Loss: 0.1075 | Valid Acc: 20.8886\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0031 Masking rate: 0.3 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.1509 | Train Acc: 19.0495\n",
      "Valid Loss: 0.1063 | Valid Acc: 20.8721\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0032 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.3634 | Train Acc: 17.5651\n",
      "Valid Loss: 0.1066 | Valid Acc: 20.8996\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0033 Masking rate: 0.5 Train dataset: 1000000 Valid dataset: 100000\n",
      "Train Loss: 0.3636 | Train Acc: 17.5717\n",
      "Valid Loss: 0.1078 | Valid Acc: 20.8753\n",
      "Predictions ...\n",
      "\n",
      "Epoch: 0034 Masking rate: 0.6 Train dataset: 1000000 Valid dataset: 100000\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "if len(glob.glob(\"output/MoleculeNet/*.tsv\")) != 0:\n",
    "    print(\"load pretrained model ... \")\n",
    "    start_epoch = len(glob.glob(\"output/MoleculeNet/*.tsv\"))\n",
    "    model.load_state_dict(torch.load('weights/MoleculeNet_LM_best.pt'))\n",
    "    \n",
    "N_EPOCHS  = 1000\n",
    "PAITIENCE = 30\n",
    "\n",
    "n_paitience = 0\n",
    "best_valid_loss = float('inf')\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "\n",
    "for epoch in range(start_epoch, N_EPOCHS):\n",
    "    epoch_masking_rate = np.random.choice([0.3, 0.4, 0.5, 0.6])\n",
    "    epoch_train_data   = shuffle(train_data, n_samples=1000000)\n",
    "    epoch_valid_data   = shuffle(test_data, n_samples=100000)\n",
    "    train_dataloader   = generate_epoch_dataloader(\n",
    "                                                    epoch_train_data, \n",
    "                                                    seq_len=seq_len, \n",
    "                                                    tokenizer=tokenizer, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    masking_rate=epoch_masking_rate,\n",
    "                                                    collate_fn=collate_fn,\n",
    "                                                    num_workers=12\n",
    "                                                    )\n",
    "    \n",
    "    valid_dataloader   = generate_epoch_dataloader(\n",
    "                                                    epoch_valid_data, \n",
    "                                                    seq_len=seq_len, \n",
    "                                                    tokenizer=tokenizer, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    masking_rate=0.3,\n",
    "                                                    collate_fn=collate_fn,\n",
    "                                                    num_workers=12\n",
    "                                                    )\n",
    "    \n",
    "    print(f'Epoch: {epoch:04} Masking rate: {epoch_masking_rate} Train dataset: {len(epoch_train_data)} Valid dataset: {len(epoch_valid_data)}')\n",
    "    \n",
    "    train_loss, train_accuracy = train(model, train_dataloader, optimizer, criterion, device)\n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_dataloader, optimizer, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.4f}\\nValid Loss: {valid_loss:.4f} | Valid Acc: {valid_accuracy:.4f}')\n",
    "\n",
    "    with open(\"output/MoleculeNet/log.txt\", \"a\") as f:\n",
    "        f.write(\"epoch: {0:04d} train loss: {1:.4f}, train acc: {2:.4f}, test loss: {3:.4f}, test acc: {4:.4f}\\n\".format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Predictions ...\\n\")\n",
    "        samples_for_prediction = shuffle(epoch_valid_data, n_samples=20)\n",
    "        prediction_dataloader  = generate_epoch_prediction_dataloader(\n",
    "                                                                        samples_for_prediction, \n",
    "                                                                        seq_len=seq_len, \n",
    "                                                                        tokenizer=tokenizer, \n",
    "                                                                        batch_size=len(samples_for_prediction), \n",
    "                                                                        masking_rate=0.3, \n",
    "                                                                        collate_fn=collate_fn\n",
    "                                                                        )\n",
    "        output_list, target_list = predict(model, prediction_dataloader, device, tokenizer)\n",
    "        prediction_results = pd.DataFrame({\"output\": output_list, \"target\": target_list})\n",
    "        prediction_results.to_csv(\"output/MoleculeNet/prediction_results_epoch-{0:04d}.tsv\".format(epoch), sep=\"\\t\", index=False)            \n",
    "        \n",
    "    if n_paitience < PAITIENCE:\n",
    "        if best_valid_loss > valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'weights/MoleculeNet_LM_best.pt')\n",
    "            n_paitience = 0\n",
    "        elif best_valid_loss <= valid_loss:\n",
    "            n_paitience += 1\n",
    "    else:\n",
    "        print(\"Early stop!\")\n",
    "        model.load_state_dict(torch.load('weights/MoleculeNet_LM_best.pt'))\n",
    "        model.eval()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b97c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
