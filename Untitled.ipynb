{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e0c0662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n_batch = 1\n",
    "dim_large = 1024\n",
    "dim_small = 128\n",
    "dim_project = 512\n",
    "n_heads = 8\n",
    "\n",
    "small_seq = torch.rand((1, dim_small))\n",
    "large_seq = torch.rand((1, dim_large))\n",
    "\n",
    "large_to_small = nn.Linear(dim_large, dim_small)\n",
    "small_to_large = nn.Linear(dim_small, dim_large)\n",
    "\n",
    "large_seq_projected = large_to_small(large_seq)\n",
    "small_seq_projected = small_to_large(small_seq)\n",
    "\n",
    "print(large_seq_projected.shape)\n",
    "print(small_seq_projected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6920c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "class CrossAttention(nn.Modules):\n",
    "    def __init__(self, d_model, inner_dim=512, n_heads=8, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_batch = n_batch\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.inner_dim = inner_dim\n",
    "        self.scale = dim_head ** -0.5\n",
    "        \n",
    "        self.to_q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(self.d_model, self.inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(self.d_model, self.inner_dim, bias=False)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        q = self.to_q(q)\n",
    "        k = self.to_k(k)\n",
    "        v = self.to_v(v)\n",
    "        \n",
    "        q = q.view(-1, self.n_heads, self.inner_dim // self.n_heads)\n",
    "        k = k.view(-1, self.n_heads, self.inner_dim // self.n_heads)\n",
    "        v = v.view(-1, self.n_heads, self.inner_dim // self.n_heads)\n",
    "        \n",
    "        dim = q.shape[-1]\n",
    "        score = torch.matmul(q, k.T) / torch.sqrt(dim)\n",
    "        score = F.softmax(score, dim=-1)\n",
    "        out = torch.matmul(score, v)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22dea48f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 4 required positional arguments: 'd_model', 'h', 'qkv_fc_layer', and 'fc_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4460/3753226117.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmulti_head_attention_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttentionLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmulti_head_attention_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 4 required positional arguments: 'd_model', 'h', 'qkv_fc_layer', and 'fc_layer'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6eb2642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at weights/molecule_bert_pretrained/ were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at weights/molecule_bert_pretrained/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"data/drug/tokenizer_model/vocab.json\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    unk_token=\"[UNK]\"\n",
    ")\n",
    "\n",
    "vocab_size = len(fast_tokenizer.get_vocab().keys())\n",
    "\n",
    "# prot_bert = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "molecule_bert = BertModel.from_pretrained(\"weights/molecule_bert_pretrained/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86ff70c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2529, -0.1541, -0.3178, -0.2939,  0.0102,  0.0807,  0.1587, -0.0029,\n",
       "         -0.2492,  0.0649,  0.0317, -0.2974,  0.1286,  0.3392, -0.1417, -0.0793,\n",
       "          0.2037,  0.3811, -0.0537,  0.1650, -0.1294, -0.1952, -0.0131, -0.1350,\n",
       "          0.2100,  0.2240, -0.0620, -0.2068, -0.3132,  0.2256,  0.0541,  0.3479,\n",
       "         -0.3182,  0.3028,  0.2635,  0.1498, -0.1914, -0.1550,  0.2509,  0.1575,\n",
       "          0.0092,  0.0112, -0.2377,  0.4385, -0.2409, -0.1302,  0.2638,  0.0248,\n",
       "         -0.0245,  0.1838, -0.2324,  0.1214, -0.0582,  0.2001, -0.4337,  0.1611,\n",
       "          0.1092,  0.2030,  0.1474,  0.1470, -0.3941,  0.0382, -0.1341, -0.0460,\n",
       "          0.0069,  0.1797, -0.1230,  0.2369,  0.1034,  0.0167, -0.2657,  0.2654,\n",
       "         -0.2949,  0.0114, -0.1491,  0.2392, -0.0574,  0.1602,  0.2748, -0.0705,\n",
       "         -0.1063,  0.0791,  0.0172,  0.1818,  0.1234, -0.1206, -0.2765, -0.1249,\n",
       "          0.0361,  0.1667, -0.2323, -0.2915,  0.1291, -0.1358, -0.3952, -0.0519,\n",
       "         -0.0555,  0.1175,  0.0693, -0.0869,  0.0144,  0.2332,  0.4323, -0.2059,\n",
       "         -0.1441,  0.3298, -0.0412, -0.0069,  0.1736, -0.3695,  0.2993,  0.1621,\n",
       "          0.2240, -0.0726, -0.0571, -0.1750, -0.1504,  0.0318, -0.1319, -0.0187,\n",
       "          0.0184, -0.2053, -0.1922, -0.0482,  0.2633, -0.1119, -0.0416, -0.0472]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_Example = \"CN(C)CCOC(C1=CC=CC=C1)C1=CC=CC=C1\"\n",
    "tokenizer.encode(sequence_Example, max_length=128, truncation=True)\n",
    "\n",
    "encoded_input = tokenizer(sequence_Example, return_tensors='pt')\n",
    "output = molecule_bert(**encoded_input)\n",
    "output.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "794ee190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 128])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5164f026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tokenizer\n",
      "vocab size: 69\n",
      "special tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bert(\n",
       "  (model): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(69, 128, padding_idx=0)\n",
       "        (position_embeddings): Embedding(128, 128)\n",
       "        (token_type_embeddings): Embedding(1, 128)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=128, out_features=69, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (train_accuracy): Accuracy()\n",
       "  (valid_accuracy): Accuracy()\n",
       "  (test_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"data/drug/tokenizer_model/vocab.json\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    unk_token=\"[UNK]\"\n",
    ")\n",
    "\n",
    "vocab_size = len(fast_tokenizer.get_vocab().keys())\n",
    "\n",
    "print(f\"load tokenizer\\nvocab size: {vocab_size}\\nspecial tokens: {fast_tokenizer.all_special_tokens}\")\n",
    "\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BertConfig, BertForMaskedLM\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=512,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=128,\n",
    "    type_vocab_size=1,\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\"\n",
    ")\n",
    "\n",
    "class Bert(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = BertForMaskedLM(config)\n",
    "        \n",
    "        self.train_accuracy = torchmetrics.Accuracy()\n",
    "        self.valid_accuracy = torchmetrics.Accuracy()\n",
    "        self.test_accuracy = torchmetrics.Accuracy()\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, labels):\n",
    "        return self.model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "       \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        output = self(input_ids, labels)\n",
    "\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        \n",
    "        self.log('train_loss', float(loss), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", self.train_accuracy(preds[labels > 0], labels[labels > 0]), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        output = self(input_ids, labels)\n",
    "\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        \n",
    "        self.log('valid_loss', float(loss), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"valid_accuracy\", self.valid_accuracy(preds[labels > 0], labels[labels > 0]), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        output = self(input_ids, labels)\n",
    "\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        \n",
    "        self.log('test_loss', float(loss), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_accuracy\", self.test_accuracy(preds[labels > 0], labels[labels > 0]), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
    "    \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "    \n",
    "\n",
    "model = Bert(config).load_from_checkpoint(\"weights/molecule_bert/exp7_pretraining_done/molecule_bert-epoch=896-valid_loss=0.0995.ckpt\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb129fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
