{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4152a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Stores the tokens and their conversion to vocabulary indexes.\"\"\"\n",
    "\n",
    "    def __init__(self, tokens=None, starting_id=0):\n",
    "        self._tokens = {}\n",
    "        self._current_id = starting_id\n",
    "\n",
    "        if tokens:\n",
    "            for token, idx in tokens.items():\n",
    "                self._add(token, idx)\n",
    "                self._current_id = max(self._current_id, idx + 1)\n",
    "\n",
    "    def __getitem__(self, token_or_id):\n",
    "        return self._tokens[token_or_id]\n",
    "\n",
    "    def add(self, token):\n",
    "        \"\"\"Adds a token.\"\"\"\n",
    "        if not isinstance(token, str):\n",
    "            raise TypeError(\"Token is not a string\")\n",
    "        if token in self:\n",
    "            return self[token]\n",
    "        self._add(token, self._current_id)\n",
    "        self._current_id += 1\n",
    "        return self._current_id - 1\n",
    "\n",
    "    def update(self, tokens):\n",
    "        \"\"\"Adds many tokens.\"\"\"\n",
    "        return [self.add(token) for token in tokens]\n",
    "\n",
    "    def __delitem__(self, token_or_id):\n",
    "        other_val = self._tokens[token_or_id]\n",
    "        del self._tokens[other_val]\n",
    "        del self._tokens[token_or_id]\n",
    "\n",
    "    def __contains__(self, token_or_id):\n",
    "        return token_or_id in self._tokens\n",
    "\n",
    "    def __eq__(self, other_vocabulary):\n",
    "        return self._tokens == other_vocabulary._tokens  # pylint: disable=W0212\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._tokens) // 2\n",
    "\n",
    "    def encode(self, tokens, max_length=128, masking_rate=0.2):\n",
    "        \"\"\"Encodes a list of tokens as vocabulary indexes.\"\"\"\n",
    "        vocab_index = np.repeat(0, max_length) # initialize with PAD token\n",
    "        vocab_index[0] = 1 # CLS token\n",
    "        \n",
    "        for i, token in enumerate(tokens):   \n",
    "            if i <= max_length - 2:\n",
    "                try:\n",
    "                    vocab_index[i + 1] = self._tokens[token]\n",
    "                except:\n",
    "                    vocab_index[i + 1] = 3 # UNK token\n",
    "            elif i > max_lenght - 2:\n",
    "                break\n",
    "        \n",
    "        vocab_index[i + 1] = 2 # SEP token    \n",
    "        \n",
    "        masked_index = copy.deepcopy(vocab_index)\n",
    "        mask = np.random.permutation(list(range(1, 1 + min(len(tokens), max_length))))[:round(max_length*masking_rate)]\n",
    "        masked_index[mask] = 4 # MASK token\n",
    "        \n",
    "        return masked_index, vocab_index\n",
    "\n",
    "    \n",
    "    def decode(self, vocab_index):\n",
    "        \"\"\"Decodes a vocabulary index matrix to a list of tokens.\"\"\"\n",
    "        tokens = []\n",
    "        for idx in vocab_index:\n",
    "            tokens.append(self[idx])\n",
    "        return tokens\n",
    "\n",
    "    def _add(self, token, idx):\n",
    "        if idx not in self._tokens:\n",
    "            self._tokens[token] = idx\n",
    "            self._tokens[idx] = token\n",
    "        else:\n",
    "            raise ValueError(\"IDX already present in vocabulary\")\n",
    "\n",
    "    def tokens(self):\n",
    "        \"\"\"Returns the tokens from the vocabulary\"\"\"\n",
    "        return [t for t in self._tokens if isinstance(t, str)]\n",
    "    \n",
    "    \n",
    "import re\n",
    "\n",
    "class SMILESTokenizer:\n",
    "    \"\"\"Deals with the tokenization and untokenization of SMILES.\"\"\"\n",
    "\n",
    "    REGEXPS = {\n",
    "        \"brackets\": re.compile(r\"(\\[[^\\]]*\\])\"),\n",
    "        \"2_ring_nums\": re.compile(r\"(%\\d{2})\"),\n",
    "        \"brcl\": re.compile(r\"(Br|Cl)\")\n",
    "    }\n",
    "    REGEXP_ORDER = [\"brackets\", \"2_ring_nums\", \"brcl\"]\n",
    "\n",
    "    def tokenize(self, data):\n",
    "        \"\"\"Tokenizes a SMILES string.\"\"\"\n",
    "        def split_by(data, regexps):\n",
    "            if not regexps:\n",
    "                return list(data)\n",
    "            regexp = self.REGEXPS[regexps[0]]\n",
    "            splitted = regexp.split(data)\n",
    "            tokens = []\n",
    "            for i, split in enumerate(splitted):\n",
    "                if i % 2 == 0:\n",
    "                    tokens += split_by(split, regexps[1:])\n",
    "                else:\n",
    "                    tokens.append(split)\n",
    "            return tokens\n",
    "\n",
    "        tokens = split_by(data, self.REGEXP_ORDER)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def untokenize(self, tokens):\n",
    "        \"\"\"Untokenizes a SMILES string.\"\"\"\n",
    "        smi = \"\"\n",
    "        for token in tokens:\n",
    "            if token == \"[SEP]\":\n",
    "                break\n",
    "            if token != \"[CLS]\":\n",
    "                smi += token\n",
    "        return smi\n",
    "\n",
    "\n",
    "def create_vocabulary(smiles_list, tokenizer):\n",
    "    \"\"\"Creates a vocabulary for the SMILES syntax.\"\"\"\n",
    "    tokens = set()\n",
    "    for smi in smiles_list:\n",
    "        tokens.update(tokenizer.tokenize(smi))\n",
    "\n",
    "    vocabulary = Vocabulary()\n",
    "    vocabulary.update(['[PAD]', '[CLS]', '[SEP]', '[UNK]', '[MASK]'] + sorted(tokens))  # end token is 0 (also counts as padding)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173371d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "\n",
    "vocab_path = \"data/drug/vocabulary.pkl\"\n",
    "\n",
    "if not os.path.exists(vocab_path):\n",
    "    from reinvent_chemistry.file_reader import FileReader\n",
    "\n",
    "    reader = FileReader([], None)\n",
    "#     smiles_list = reader.read_delimited_file(\"data/drug/molecule_qed_filtered.txt\")\n",
    "    smiles_list = reader.read_delimited_file(\"data/drug/molecule_small.txt\")\n",
    "\n",
    "    tokenizer = SMILESTokenizer()\n",
    "    vocabulary = create_vocabulary(smiles_list, tokenizer=tokenizer)\n",
    "    \n",
    "    with open(vocab_path, \"wb\") as f:\n",
    "        pickle.dump(vocabulary, f)    \n",
    "\n",
    "elif os.path.exists(vocab_path):\n",
    "    with open(vocab_path, \"rb\") as f:\n",
    "        vocabulary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f479e08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC1=CC=C(C=C1)N2C(=CC(=C2C)C(=O)CN3CCN(CC3)CC(=O)NC4=C(C=CC=C4C)C)C',\n",
       " 'CCCN1C(=NN=N1)CN2CCN(CC2)CC(=O)NC3=C(C=CC=C3C)C',\n",
       " 'CC1=C(C(=CC=C1)C)NC(=O)CN2CCN(CC2)CC(=O)NCCC3=CC=C(C=C3)OC(F)F',\n",
       " 'CC(C(=O)NC1=CC(=CC=C1)F)N2CCN(CC2)CC3=CC=CO3',\n",
       " 'CC(C(=O)NC1=CC(=CC=C1)F)N2CCN(CC2)CC3=CC=CO3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/drug/molecule_small.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "data = [d.strip() for d in data]\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707c87d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8100 Valid: 900 Test: 1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(data, test_size=0.1, random_state=42, shuffle=True)\n",
    "X_train, X_valid = train_test_split(X_train, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"Train: {len(X_train)} Valid: {len(X_valid)} Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906be5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MaskedLMDataset(Dataset):\n",
    "    def __init__(self, data, vocabulary, max_length=128, masking_rate=0.2):\n",
    "        self.data = data\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_length = max_length\n",
    "        self.masking_rate = masking_rate\n",
    "        \n",
    "        \n",
    "    def encode(self, data):\n",
    "        return self.vocabulary.encode(data, max_length=self.max_length, masking_rate=self.masking_rate)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.encode(self.data[idx])\n",
    "        \n",
    "        return torch.tensor(X, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "train_dataset = MaskedLMDataset(X_train, vocabulary, max_length=128, masking_rate=0.2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, num_workers=16)\n",
    "\n",
    "valid_dataset = MaskedLMDataset(X_valid, vocabulary, max_length=128, masking_rate=0.2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, num_workers=16)\n",
    "\n",
    "test_dataset = MaskedLMDataset(X_test, vocabulary, max_length=128, masking_rate=0.2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c28a76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.\n",
      "  f\"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated\"\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63779/3247560170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m ]\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ddp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mamp_backend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mamp_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mplugins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         )\n\u001b[1;32m    450\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger_connector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoggerConnector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_gpu_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_processes, devices, tpu_cores, ipus, accelerator, strategy, gpus, gpu_ids, num_nodes, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, precision, amp_type, amp_level, plugins)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_training_type_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_distributed_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_given_plugins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36mset_distributed_mode\u001b[0;34m(self, strategy)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m# finished configuring self._distrib_type, check ipython environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_interactive_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# for DDP overwrite nb processes by requested GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36mcheck_interactive_compatibility\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_IS_INTERACTIVE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distrib_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distrib_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_interactive_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m             raise MisconfigurationException(\n\u001b[0;32m--> 944\u001b[0;31m                 \u001b[0;34mf\"`Trainer(strategy={self._distrib_type.value!r})` or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0;34mf\" `Trainer(accelerator={self._distrib_type.value!r})` is not compatible with an interactive\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;34m\" environment. Run your code as a script, or choose one of the compatible backends:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BertConfig, BertForMaskedLM\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "vocab_size = len(vocabulary.tokens())\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=512,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=128,\n",
    "    type_vocab_size=1,\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\"\n",
    ")\n",
    "\n",
    "\n",
    "class Bert(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = BertForMaskedLM(config)\n",
    "        self.train_accuracy = torchmetrics.Accuracy()\n",
    "        self.valid_accuracy = torchmetrics.Accuracy()\n",
    "        \n",
    "        \n",
    "    def forward(self, X, y):\n",
    "        return self.model(input_ids=X, labels=y)\n",
    "\n",
    "       \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        output = self(X, y)\n",
    "\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        \n",
    "        self.log('train_loss', float(loss), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", self.train_accuracy(preds[y > 0], y[y > 0]), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        output = self(X, y)\n",
    "\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        \n",
    "        self.log('valid_loss', float(loss), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"valid_accuracy\", self.valid_accuracy(preds[y > 0], y[y > 0]), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"valid_loss\"}\n",
    "\n",
    "    \n",
    "model = Bert(config)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor='valid_loss', dirpath='weights/molecule_bert', filename='molecule_bert-{epoch:03d}-{valid_loss:.4f}'),\n",
    "    EarlyStopping('valid_loss', patience=10)\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=100, gpus=2, enable_progress_bar=True, callbacks=callbacks, accelerator=\"ddp\")\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a13af07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
