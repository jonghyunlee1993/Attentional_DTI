{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ProteinCNNEncoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dims=[512, 256], dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.dropout = dropout\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "#         self.branch_1_layer_1 = nn.Conv1d(embedding_dim, hidden_dims[0], kernel_size=5)\n",
    "#         self.branch_1_layer_2 = nn.Conv1d(hidden_dims[0], hidden_dims[1], kernel_size=5)\n",
    "        \n",
    "#         self.branch_2_layer_1 = nn.Conv1d(embedding_dim, hidden_dims[0], kernel_size=10)\n",
    "#         self.branch_2_layer_2 = nn.Conv1d(hidden_dims[0], hidden_dims[1], kernel_size=10)\n",
    "        \n",
    "#         self.branch_3_layer_1 = nn.Conv1d(embedding_dim, hidden_dims[0], kernel_size=15)\n",
    "#         self.branch_3_layer_2 = nn.Conv1d(hidden_dims[0], hidden_dims[1], kernel_size=15)\n",
    "        \n",
    "#         self.branch_4_layer_1 = nn.Conv1d(embedding_dim, hidden_dims[0], kernel_size=20)\n",
    "#         self.branch_4_layer_2 = nn.Conv1d(hidden_dims[0], hidden_dims[1], kernel_size=20)\n",
    "        \n",
    "#         self.branch_5_layer_1 = nn.Conv1d(embedding_dim, hidden_dims[0], kernel_size=30)\n",
    "#         self.branch_5_layer_2 = nn.Conv1d(hidden_dims[0], hidden_dims[1], kernel_size=30)\n",
    "\n",
    "    \n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         x = x.moveaxis(1, 2)\n",
    "        \n",
    "#         branch_1 = F.dropout(F.gelu(self.branch_1_layer_1(x)), self.dropout)\n",
    "#         branch_1 = F.dropout(F.gelu(self.branch_1_layer_2(branch_1)), self.dropout)\n",
    "#         branch_1, _ = torch.max(branch_1, -1)\n",
    "        \n",
    "#         branch_2 = F.dropout(F.gelu(self.branch_2_layer_1(x)), self.dropout)\n",
    "#         branch_2 = F.dropout(F.gelu(self.branch_2_layer_2(branch_2)), self.dropout)\n",
    "#         branch_2, _ = torch.max(branch_2, -1)\n",
    "        \n",
    "#         branch_3 = F.dropout(F.gelu(self.branch_3_layer_1(x)), self.dropout)\n",
    "#         branch_3 = F.dropout(F.gelu(self.branch_3_layer_2(branch_3)), self.dropout)\n",
    "#         branch_3, _ = torch.max(branch_3, -1)\n",
    "        \n",
    "#         branch_4 = F.dropout(F.gelu(self.branch_4_layer_1(x)), self.dropout)\n",
    "#         branch_4 = F.dropout(F.gelu(self.branch_4_layer_2(branch_4)), self.dropout)\n",
    "#         branch_4, _ = torch.max(branch_4, -1)\n",
    "        \n",
    "#         branch_5 = F.dropout(F.gelu(self.branch_5_layer_1(x)), self.dropout)\n",
    "#         branch_5 = F.dropout(F.gelu(self.branch_5_layer_2(branch_5)), self.dropout)\n",
    "#         branch_5, _ = torch.max(branch_5, -1)\n",
    "        \n",
    "#         x = torch.mean(x, -1)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4d5aa003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ProteinCNNEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dims=[512, 256], dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.branch_1_layer_1 = nn.Conv1d(embedding_dim, hidden_dims[0], kernel_size=5, padding=2)\n",
    "        self.branch_1_layer_2 = nn.Conv1d(hidden_dims[0], hidden_dims[1], kernel_size=5, padding=2)\n",
    "        \n",
    "        \n",
    "        self.branch_2_layer_1 = nn.Conv1d(embedding_dim, hidden_dims[0], kernel_size=10, padding=5)\n",
    "        self.branch_2_layer_2 = nn.Conv1d(hidden_dims[0], hidden_dims[1], kernel_size=10, padding=4)\n",
    "        \n",
    "        self.branch_3_layer_1 = nn.Conv1d(embedding_dim, hidden_dims[0], kernel_size=15, padding=7)\n",
    "        self.branch_3_layer_2 = nn.Conv1d(hidden_dims[0], hidden_dims[1], kernel_size=15, padding=7)\n",
    "        \n",
    "        self.branch_4_layer_1 = nn.Conv1d(embedding_dim, hidden_dims[0], kernel_size=20, padding=10)\n",
    "        self.branch_4_layer_2 = nn.Conv1d(hidden_dims[0], hidden_dims[1], kernel_size=20, padding=9)\n",
    "        \n",
    "        self.branch_5_layer_1 = nn.Conv1d(embedding_dim, hidden_dims[0], kernel_size=30, padding=15)\n",
    "        self.branch_5_layer_2 = nn.Conv1d(hidden_dims[0], hidden_dims[1], kernel_size=30, padding=14)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.moveaxis(1, 2)\n",
    "        \n",
    "        branch_1 = F.dropout(F.gelu(self.branch_1_layer_1(x)), self.dropout)\n",
    "        branch_1 = F.dropout(F.gelu(self.branch_1_layer_2(branch_1)), self.dropout)\n",
    "        \n",
    "        branch_2 = F.dropout(F.gelu(self.branch_2_layer_1(x)), self.dropout)\n",
    "        branch_2 = F.dropout(F.gelu(self.branch_2_layer_2(branch_2)), self.dropout)      \n",
    "        \n",
    "        branch_3 = F.dropout(F.gelu(self.branch_3_layer_1(x)), self.dropout)\n",
    "        branch_3 = F.dropout(F.gelu(self.branch_3_layer_2(branch_3)), self.dropout)\n",
    "        \n",
    "        branch_4 = F.dropout(F.gelu(self.branch_4_layer_1(x)), self.dropout)\n",
    "        branch_4 = F.dropout(F.gelu(self.branch_4_layer_2(branch_4)), self.dropout)\n",
    "       \n",
    "        branch_5 = F.dropout(F.gelu(self.branch_5_layer_1(x)), self.dropout)\n",
    "        branch_5 = F.dropout(F.gelu(self.branch_5_layer_2(branch_5)), self.dropout)\n",
    "        \n",
    "        x = (branch_1 + branch_2 + branch_3 + branch_4 + branch_5) / 5\n",
    "        x = torch.moveaxis(x, 1, 2)\n",
    "        \n",
    "        return x\n",
    "\n",
    "fasta_stoi = {\n",
    "    \"[PAD]\": 0,\n",
    "    \"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"H\": 8,\n",
    "    \"I\": 9, \"J\": 10, \"K\": 11, \"L\": 12, \"M\": 13, \"N\": 14, \"O\": 15,\n",
    "    \"P\": 16, \"Q\": 17, \"R\": 18, \"S\": 19, \"T\": 20, \"U\": 21, \"V\": 22, \n",
    "    \"W\": 23, \"Y\": 24, \"Z\": 25, \"X\": 26, \"*\": 27, \"-\": 28\n",
    "}\n",
    "    \n",
    "vocab_size = len(fasta_stoi)\n",
    "embedding_dim = 256\n",
    "protein_cnn_encoder = ProteinCNNEncoder(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c3413789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1024])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = protein_cnn_encoder(torch.randint(0, vocab_size, (1, 1024)))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "560c6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_attention(query, context, eps=1e-8):\n",
    "    batch_size_q, queryL = query.size(0), query.size(1)\n",
    "    batch_size, sourceL = context.size(0), context.size(1)\n",
    "\n",
    "    queryT = torch.transpose(query, 1, 2)\n",
    "\n",
    "    attn = torch.bmm(context, queryT)\n",
    "    \n",
    "    attn = attn.view(batch_size * sourceL, queryL)\n",
    "    attn = F.softmax(attn).view(batch_size, sourceL, queryL)\n",
    "\n",
    "    attn = torch.transpose(attn, 1, 2).contiguous().view(batch_size * queryL, sourceL)\n",
    "    attn = F.softmax(attn).view(batch_size, queryL, sourceL)\n",
    "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "    contextT = torch.transpose(context, 1, 2)\n",
    "\n",
    "    weightedContext = torch.bmm(contextT, attnT)\n",
    "    weightedContext = torch.transpose(weightedContext, 1, 2)\n",
    "\n",
    "    return weightedContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "821cc0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at weights/molecule_bert_pretrained-masking_rate_30 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "molecule_tokenizer = BertTokenizer.from_pretrained(\"data/drug/molecule_tokenizer\")\n",
    "molecule_bert = BertModel.from_pretrained(\"weights/molecule_bert_pretrained-masking_rate_30\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "53ce5997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2, 30, 53, 19,  7, 63, 35,  8, 63, 53, 20, 53, 53, 53,  5, 10, 53, 21,\n",
       "         53, 63, 53, 53,  5, 41, 30, 30,  5, 40,  6, 30, 53, 22, 53, 53, 53, 53,\n",
       "         53, 22,  6, 53, 21,  6, 53, 53, 19, 20,  3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_smiles = molecule_tokenizer(smiles_seq, return_tensors=\"pt\")\n",
    "encoded_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "2d7dc176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[30, 53, 19,  7, 63, 35,  8, 63, 53, 20, 53, 53, 53,  5, 10, 53, 21, 53,\n",
       "         63, 53, 53,  5, 41, 30, 30,  5, 40,  6, 30, 53, 22, 53, 53, 53, 53, 53,\n",
       "         22,  6, 53, 21,  6, 53, 53, 19, 20]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_smiles = molecule_tokenizer(smiles_seq, return_tensors=\"pt\")\n",
    "encoded_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "92e8f99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from tdc.multi_pred import DTI\n",
    "\n",
    "davis = DTI(name=\"Davis\")\n",
    "davis_split = davis.get_split()\n",
    "\n",
    "smiles_seq = \" \".join(davis_split['train'].loc[0, \"Drug\"])\n",
    "encoded_smiles = molecule_tokenizer(smiles_seq, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "983dc080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[30, 53, 19,  7, 63, 35,  8, 63, 53, 20, 53, 53, 53,  5, 10, 53, 21, 53,\n",
       "         63, 53, 53,  5, 41, 30, 30,  5, 40,  6, 30, 53, 22, 53, 53, 53, 53, 53,\n",
       "         22,  6, 53, 21,  6, 53, 53, 19, 20]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "59b8a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False, model_max_length=2048)\n",
    "fasta_seq = \" \".join(davis_split['train'].loc[0, \"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "608c386b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 963])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_tokenizer(fasta_seq, return_tensors=\"pt\")['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "aea9532e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "961"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(davis_split['train'].loc[0, \"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7a344f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(davis_split['train'].loc[0, \"Drug\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6c3f089a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30,\n",
       " 53,\n",
       " 19,\n",
       " 7,\n",
       " 63,\n",
       " 35,\n",
       " 8,\n",
       " 63,\n",
       " 53,\n",
       " 20,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 5,\n",
       " 10,\n",
       " 53,\n",
       " 21,\n",
       " 53,\n",
       " 63,\n",
       " 53,\n",
       " 53,\n",
       " 5,\n",
       " 41,\n",
       " 30,\n",
       " 30,\n",
       " 5,\n",
       " 40,\n",
       " 6,\n",
       " 30,\n",
       " 53,\n",
       " 22,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 22,\n",
       " 6,\n",
       " 53,\n",
       " 21,\n",
       " 6,\n",
       " 53,\n",
       " 53,\n",
       " 19,\n",
       " 20]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecule_tokenizer.encode(smiles_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b54728a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 0,\n",
       " '<pad>': 1,\n",
       " '</s>': 2,\n",
       " '<unk>': 3,\n",
       " '<mask>': 4,\n",
       " '!': 5,\n",
       " '\"': 6,\n",
       " '#': 7,\n",
       " '$': 8,\n",
       " '%': 9,\n",
       " '&': 10,\n",
       " \"'\": 11,\n",
       " '(': 12,\n",
       " ')': 13,\n",
       " '*': 14,\n",
       " '+': 15,\n",
       " ',': 16,\n",
       " '-': 17,\n",
       " '.': 18,\n",
       " '/': 19,\n",
       " '0': 20,\n",
       " '1': 21,\n",
       " '2': 22,\n",
       " '3': 23,\n",
       " '4': 24,\n",
       " '5': 25,\n",
       " '6': 26,\n",
       " '7': 27,\n",
       " '8': 28,\n",
       " '9': 29,\n",
       " ':': 30,\n",
       " ';': 31,\n",
       " '<': 32,\n",
       " '=': 33,\n",
       " '>': 34,\n",
       " '?': 35,\n",
       " '@': 36,\n",
       " 'A': 37,\n",
       " 'B': 38,\n",
       " 'C': 39,\n",
       " 'D': 40,\n",
       " 'E': 41,\n",
       " 'F': 42,\n",
       " 'G': 43,\n",
       " 'H': 44,\n",
       " 'I': 45,\n",
       " 'J': 46,\n",
       " 'K': 47,\n",
       " 'L': 48,\n",
       " 'M': 49,\n",
       " 'N': 50,\n",
       " 'O': 51,\n",
       " 'P': 52,\n",
       " 'Q': 53,\n",
       " 'R': 54,\n",
       " 'S': 55,\n",
       " 'T': 56,\n",
       " 'U': 57,\n",
       " 'V': 58,\n",
       " 'W': 59,\n",
       " 'X': 60,\n",
       " 'Y': 61,\n",
       " 'Z': 62,\n",
       " '[': 63,\n",
       " '\\\\': 64,\n",
       " ']': 65,\n",
       " '^': 66,\n",
       " '_': 67,\n",
       " '`': 68,\n",
       " 'a': 69,\n",
       " 'b': 70,\n",
       " 'c': 71,\n",
       " 'd': 72,\n",
       " 'e': 73,\n",
       " 'f': 74,\n",
       " 'g': 75,\n",
       " 'h': 76,\n",
       " 'i': 77,\n",
       " 'j': 78,\n",
       " 'k': 79,\n",
       " 'l': 80,\n",
       " 'm': 81,\n",
       " 'n': 82,\n",
       " 'o': 83,\n",
       " 'p': 84,\n",
       " 'q': 85,\n",
       " 'r': 86,\n",
       " 's': 87,\n",
       " 't': 88,\n",
       " 'u': 89,\n",
       " 'v': 90,\n",
       " 'w': 91,\n",
       " 'x': 92,\n",
       " 'y': 93,\n",
       " 'z': 94,\n",
       " '{': 95,\n",
       " '|': 96,\n",
       " '}': 97,\n",
       " '~': 98,\n",
       " '¡': 99,\n",
       " '¢': 100,\n",
       " '£': 101,\n",
       " '¤': 102,\n",
       " '¥': 103,\n",
       " '¦': 104,\n",
       " '§': 105,\n",
       " '¨': 106,\n",
       " '©': 107,\n",
       " 'ª': 108,\n",
       " '«': 109,\n",
       " '¬': 110,\n",
       " '®': 111,\n",
       " '¯': 112,\n",
       " '°': 113,\n",
       " '±': 114,\n",
       " '²': 115,\n",
       " '³': 116,\n",
       " '´': 117,\n",
       " 'µ': 118,\n",
       " '¶': 119,\n",
       " '·': 120,\n",
       " '¸': 121,\n",
       " '¹': 122,\n",
       " 'º': 123,\n",
       " '»': 124,\n",
       " '¼': 125,\n",
       " '½': 126,\n",
       " '¾': 127,\n",
       " '¿': 128,\n",
       " 'À': 129,\n",
       " 'Á': 130,\n",
       " 'Â': 131,\n",
       " 'Ã': 132,\n",
       " 'Ä': 133,\n",
       " 'Å': 134,\n",
       " 'Æ': 135,\n",
       " 'Ç': 136,\n",
       " 'È': 137,\n",
       " 'É': 138,\n",
       " 'Ê': 139,\n",
       " 'Ë': 140,\n",
       " 'Ì': 141,\n",
       " 'Í': 142,\n",
       " 'Î': 143,\n",
       " 'Ï': 144,\n",
       " 'Ð': 145,\n",
       " 'Ñ': 146,\n",
       " 'Ò': 147,\n",
       " 'Ó': 148,\n",
       " 'Ô': 149,\n",
       " 'Õ': 150,\n",
       " 'Ö': 151,\n",
       " '×': 152,\n",
       " 'Ø': 153,\n",
       " 'Ù': 154,\n",
       " 'Ú': 155,\n",
       " 'Û': 156,\n",
       " 'Ü': 157,\n",
       " 'Ý': 158,\n",
       " 'Þ': 159,\n",
       " 'ß': 160,\n",
       " 'à': 161,\n",
       " 'á': 162,\n",
       " 'â': 163,\n",
       " 'ã': 164,\n",
       " 'ä': 165,\n",
       " 'å': 166,\n",
       " 'æ': 167,\n",
       " 'ç': 168,\n",
       " 'è': 169,\n",
       " 'é': 170,\n",
       " 'ê': 171,\n",
       " 'ë': 172,\n",
       " 'ì': 173,\n",
       " 'í': 174,\n",
       " 'î': 175,\n",
       " 'ï': 176,\n",
       " 'ð': 177,\n",
       " 'ñ': 178,\n",
       " 'ò': 179,\n",
       " 'ó': 180,\n",
       " 'ô': 181,\n",
       " 'õ': 182,\n",
       " 'ö': 183,\n",
       " '÷': 184,\n",
       " 'ø': 185,\n",
       " 'ù': 186,\n",
       " 'ú': 187,\n",
       " 'û': 188,\n",
       " 'ü': 189,\n",
       " 'ý': 190,\n",
       " 'þ': 191,\n",
       " 'ÿ': 192,\n",
       " 'Ā': 193,\n",
       " 'ā': 194,\n",
       " 'Ă': 195,\n",
       " 'ă': 196,\n",
       " 'Ą': 197,\n",
       " 'ą': 198,\n",
       " 'Ć': 199,\n",
       " 'ć': 200,\n",
       " 'Ĉ': 201,\n",
       " 'ĉ': 202,\n",
       " 'Ċ': 203,\n",
       " 'ċ': 204,\n",
       " 'Č': 205,\n",
       " 'č': 206,\n",
       " 'Ď': 207,\n",
       " 'ď': 208,\n",
       " 'Đ': 209,\n",
       " 'đ': 210,\n",
       " 'Ē': 211,\n",
       " 'ē': 212,\n",
       " 'Ĕ': 213,\n",
       " 'ĕ': 214,\n",
       " 'Ė': 215,\n",
       " 'ė': 216,\n",
       " 'Ę': 217,\n",
       " 'ę': 218,\n",
       " 'Ě': 219,\n",
       " 'ě': 220,\n",
       " 'Ĝ': 221,\n",
       " 'ĝ': 222,\n",
       " 'Ğ': 223,\n",
       " 'ğ': 224,\n",
       " 'Ġ': 225,\n",
       " 'ġ': 226,\n",
       " 'Ģ': 227,\n",
       " 'ģ': 228,\n",
       " 'Ĥ': 229,\n",
       " 'ĥ': 230,\n",
       " 'Ħ': 231,\n",
       " 'ħ': 232,\n",
       " 'Ĩ': 233,\n",
       " 'ĩ': 234,\n",
       " 'Ī': 235,\n",
       " 'ī': 236,\n",
       " 'Ĭ': 237,\n",
       " 'ĭ': 238,\n",
       " 'Į': 239,\n",
       " 'į': 240,\n",
       " 'İ': 241,\n",
       " 'ı': 242,\n",
       " 'Ĳ': 243,\n",
       " 'ĳ': 244,\n",
       " 'Ĵ': 245,\n",
       " 'ĵ': 246,\n",
       " 'Ķ': 247,\n",
       " 'ķ': 248,\n",
       " 'ĸ': 249,\n",
       " 'Ĺ': 250,\n",
       " 'ĺ': 251,\n",
       " 'Ļ': 252,\n",
       " 'ļ': 253,\n",
       " 'Ľ': 254,\n",
       " 'ľ': 255,\n",
       " 'Ŀ': 256,\n",
       " 'ŀ': 257,\n",
       " 'Ł': 258,\n",
       " 'ł': 259,\n",
       " 'Ń': 260,\n",
       " 'LL': 261,\n",
       " 'AA': 262,\n",
       " 'AL': 263,\n",
       " 'VL': 264,\n",
       " 'GL': 265,\n",
       " 'EL': 266,\n",
       " 'SL': 267,\n",
       " 'GG': 268,\n",
       " 'SS': 269,\n",
       " 'EE': 270,\n",
       " 'TL': 271,\n",
       " 'DL': 272,\n",
       " 'RL': 273,\n",
       " 'IL': 274,\n",
       " 'AV': 275,\n",
       " 'KL': 276,\n",
       " 'AG': 277,\n",
       " 'VV': 278,\n",
       " 'AE': 279,\n",
       " 'KK': 280,\n",
       " 'SG': 281,\n",
       " 'AI': 282,\n",
       " 'PL': 283,\n",
       " 'AR': 284,\n",
       " 'AD': 285,\n",
       " 'AS': 286,\n",
       " 'QL': 287,\n",
       " 'TG': 288,\n",
       " 'AK': 289,\n",
       " 'VE': 290,\n",
       " 'NL': 291,\n",
       " 'FL': 292,\n",
       " 'VI': 293,\n",
       " 'VG': 294,\n",
       " 'AT': 295,\n",
       " 'RR': 296,\n",
       " 'KE': 297,\n",
       " 'VD': 298,\n",
       " 'VS': 299,\n",
       " 'PG': 300,\n",
       " 'IE': 301,\n",
       " 'PE': 302,\n",
       " 'IG': 303,\n",
       " 'ID': 304,\n",
       " 'VT': 305,\n",
       " 'RE': 306,\n",
       " 'IS': 307,\n",
       " 'ĠM': 308,\n",
       " 'AQ': 309,\n",
       " 'VK': 310,\n",
       " 'DG': 311,\n",
       " 'DE': 312,\n",
       " 'RG': 313,\n",
       " 'PS': 314,\n",
       " 'YL': 315,\n",
       " 'IT': 316,\n",
       " 'AF': 317,\n",
       " 'NG': 318,\n",
       " 'KG': 319,\n",
       " 'AP': 320,\n",
       " 'VR': 321,\n",
       " 'TT': 322,\n",
       " 'IK': 323,\n",
       " 'FG': 324,\n",
       " 'SE': 325,\n",
       " 'AN': 326,\n",
       " 'VP': 327,\n",
       " 'HL': 328,\n",
       " 'IN': 329,\n",
       " 'IR': 330,\n",
       " 'TS': 331,\n",
       " 'TE': 332,\n",
       " 'QQ': 333,\n",
       " 'VN': 334,\n",
       " 'FS': 335,\n",
       " 'DD': 336,\n",
       " 'IP': 337,\n",
       " 'KS': 338,\n",
       " 'FE': 339,\n",
       " 'QE': 340,\n",
       " 'ML': 341,\n",
       " 'QG': 342,\n",
       " 'RS': 343,\n",
       " 'DS': 344,\n",
       " 'YG': 345,\n",
       " 'TP': 346,\n",
       " 'KR': 347,\n",
       " 'KN': 348,\n",
       " 'KD': 349,\n",
       " 'VF': 350,\n",
       " 'AY': 351,\n",
       " 'PP': 352,\n",
       " 'KT': 353,\n",
       " 'NN': 354,\n",
       " 'AM': 355,\n",
       " 'RD': 356,\n",
       " 'VQ': 357,\n",
       " 'NE': 358,\n",
       " 'FD': 359,\n",
       " 'II': 360,\n",
       " 'HG': 361,\n",
       " 'YE': 362,\n",
       " 'NS': 363,\n",
       " 'TD': 364,\n",
       " 'PD': 365,\n",
       " 'QS': 366,\n",
       " 'YS': 367,\n",
       " 'AH': 368,\n",
       " 'KP': 369,\n",
       " 'IF': 370,\n",
       " 'TR': 371,\n",
       " 'QR': 372,\n",
       " 'VY': 373,\n",
       " 'MG': 374,\n",
       " 'KI': 375,\n",
       " 'EG': 376,\n",
       " 'KQ': 377,\n",
       " 'CL': 378,\n",
       " 'NP': 379,\n",
       " 'IQ': 380,\n",
       " 'WL': 381,\n",
       " 'VM': 382,\n",
       " 'TF': 383,\n",
       " 'RP': 384,\n",
       " 'VH': 385,\n",
       " 'YD': 386,\n",
       " 'ND': 387,\n",
       " 'CG': 388,\n",
       " 'RF': 389,\n",
       " 'KF': 390,\n",
       " 'SD': 391,\n",
       " 'QD': 392,\n",
       " 'IY': 393,\n",
       " 'ME': 394,\n",
       " 'RT': 395,\n",
       " 'KY': 396,\n",
       " 'RQ': 397,\n",
       " 'IH': 398,\n",
       " 'RN': 399,\n",
       " 'QP': 400,\n",
       " 'TN': 401,\n",
       " 'RK': 402,\n",
       " 'FF': 403,\n",
       " 'SP': 404,\n",
       " 'RI': 405,\n",
       " 'TQ': 406,\n",
       " 'HS': 407,\n",
       " 'AC': 408,\n",
       " 'DP': 409,\n",
       " 'RY': 410,\n",
       " 'TI': 411,\n",
       " 'HE': 412,\n",
       " 'ALL': 413,\n",
       " 'MS': 414,\n",
       " 'GE': 415,\n",
       " 'FP': 416,\n",
       " 'HP': 417,\n",
       " 'TK': 418,\n",
       " 'TY': 419,\n",
       " 'AAL': 420,\n",
       " 'VC': 421,\n",
       " 'NF': 422,\n",
       " 'ES': 423,\n",
       " 'DF': 424,\n",
       " 'AW': 425,\n",
       " 'IV': 426,\n",
       " 'MD': 427,\n",
       " 'ED': 428,\n",
       " 'QN': 429,\n",
       " 'TV': 430,\n",
       " 'LLL': 431,\n",
       " 'QK': 432,\n",
       " 'RH': 433,\n",
       " 'FN': 434,\n",
       " 'YP': 435,\n",
       " 'EK': 436,\n",
       " 'GS': 437,\n",
       " 'RV': 438,\n",
       " 'CS': 439,\n",
       " 'MP': 440,\n",
       " 'NK': 441,\n",
       " 'QT': 442,\n",
       " 'FT': 443,\n",
       " 'NI': 444,\n",
       " 'NY': 445,\n",
       " 'DI': 446,\n",
       " 'KV': 447,\n",
       " 'QF': 448,\n",
       " 'RM': 449,\n",
       " 'QI': 450,\n",
       " 'NT': 451,\n",
       " 'YF': 452,\n",
       " 'KH': 453,\n",
       " 'KM': 454,\n",
       " 'AGL': 455,\n",
       " 'DT': 456,\n",
       " 'DY': 457,\n",
       " 'PT': 458,\n",
       " 'SF': 459,\n",
       " 'ER': 460,\n",
       " 'DR': 461,\n",
       " 'WG': 462,\n",
       " 'ST': 463,\n",
       " 'QY': 464,\n",
       " 'AVL': 465,\n",
       " 'NR': 466,\n",
       " 'IM': 467,\n",
       " 'PF': 468,\n",
       " 'DK': 469,\n",
       " 'AEL': 470,\n",
       " 'QH': 471,\n",
       " 'AAG': 472,\n",
       " 'SR': 473,\n",
       " 'CE': 474,\n",
       " 'TH': 475,\n",
       " 'VW': 476,\n",
       " 'FY': 477,\n",
       " 'DN': 478,\n",
       " 'SK': 479,\n",
       " 'LLG': 480,\n",
       " 'EN': 481,\n",
       " 'PR': 482,\n",
       " 'ASL': 483,\n",
       " 'YR': 484,\n",
       " 'TM': 485,\n",
       " 'QM': 486,\n",
       " 'SN': 487,\n",
       " 'ALG': 488,\n",
       " 'FR': 489,\n",
       " 'IC': 490,\n",
       " 'AGG': 491,\n",
       " 'QV': 492,\n",
       " 'PN': 493,\n",
       " 'ADL': 494,\n",
       " 'HF': 495,\n",
       " 'YN': 496,\n",
       " 'HD': 497,\n",
       " 'EP': 498,\n",
       " 'ET': 499,\n",
       " 'SI': 500,\n",
       " 'ARL': 501,\n",
       " 'EI': 502,\n",
       " 'YT': 503,\n",
       " 'AIL': 504,\n",
       " 'DV': 505,\n",
       " 'FK': 506,\n",
       " 'PK': 507,\n",
       " 'HH': 508,\n",
       " 'ATL': 509,\n",
       " 'GGG': 510,\n",
       " 'SQ': 511,\n",
       " 'YY': 512,\n",
       " 'PV': 513,\n",
       " 'FI': 514,\n",
       " 'AEE': 515,\n",
       " 'EQ': 516,\n",
       " 'PQ': 517,\n",
       " 'HR': 518,\n",
       " 'AKL': 519,\n",
       " 'SY': 520,\n",
       " 'ASS': 521,\n",
       " 'NQ': 522,\n",
       " 'PI': 523,\n",
       " 'DM': 524,\n",
       " 'SV': 525,\n",
       " 'GLG': 526,\n",
       " 'VLG': 527,\n",
       " 'PY': 528,\n",
       " 'ASG': 529,\n",
       " 'LLE': 530,\n",
       " 'WE': 531,\n",
       " 'YK': 532,\n",
       " 'DQ': 533,\n",
       " 'VAA': 534,\n",
       " 'MT': 535,\n",
       " 'HT': 536,\n",
       " 'MN': 537,\n",
       " 'SLG': 538,\n",
       " 'MR': 539,\n",
       " 'VLL': 540,\n",
       " 'MK': 541,\n",
       " 'AKK': 542,\n",
       " 'HY': 543,\n",
       " 'CP': 544,\n",
       " 'WS': 545,\n",
       " 'CD': 546,\n",
       " 'ELG': 547,\n",
       " 'HI': 548,\n",
       " 'HN': 549,\n",
       " 'ALE': 550,\n",
       " 'FQ': 551,\n",
       " 'YQ': 552,\n",
       " 'HQ': 553,\n",
       " 'FV': 554,\n",
       " 'RLG': 555,\n",
       " 'NV': 556,\n",
       " 'AVG': 557,\n",
       " 'ILG': 558,\n",
       " 'ATG': 559,\n",
       " 'LLS': 560,\n",
       " 'MI': 561,\n",
       " 'TC': 562,\n",
       " 'AAE': 563,\n",
       " 'AQL': 564,\n",
       " 'YI': 565,\n",
       " 'HK': 566,\n",
       " 'APL': 567,\n",
       " 'EF': 568,\n",
       " 'CR': 569,\n",
       " 'WD': 570,\n",
       " 'SSS': 571,\n",
       " 'MM': 572,\n",
       " 'VVG': 573,\n",
       " 'AFL': 574,\n",
       " 'AEG': 575,\n",
       " 'TLG': 576,\n",
       " 'ELE': 577,\n",
       " 'PH': 578,\n",
       " 'VAL': 579,\n",
       " 'NM': 580,\n",
       " 'VLE': 581,\n",
       " 'EEE': 582,\n",
       " 'SSG': 583,\n",
       " 'TW': 584,\n",
       " 'KLG': 585,\n",
       " 'DLG': 586,\n",
       " 'ADG': 587,\n",
       " 'FH': 588,\n",
       " 'VIG': 589,\n",
       " 'SH': 590,\n",
       " 'FC': 591,\n",
       " 'SM': 592,\n",
       " 'LLD': 593,\n",
       " 'NH': 594,\n",
       " 'AIG': 595,\n",
       " 'RW': 596,\n",
       " 'APG': 597,\n",
       " 'DH': 598,\n",
       " 'RC': 599,\n",
       " 'IW': 600,\n",
       " 'ANL': 601,\n",
       " 'ALS': 602,\n",
       " 'FM': 603,\n",
       " 'PM': 604,\n",
       " 'SLS': 605,\n",
       " 'KW': 606,\n",
       " 'KC': 607,\n",
       " 'EV': 608,\n",
       " 'RLL': 609,\n",
       " 'EEG': 610,\n",
       " 'ARG': 611,\n",
       " 'VAV': 612,\n",
       " 'VEG': 613,\n",
       " 'VDG': 614,\n",
       " 'GLS': 615,\n",
       " 'VVL': 616,\n",
       " 'EY': 617,\n",
       " 'MQ': 618,\n",
       " 'CT': 619,\n",
       " 'KLL': 620,\n",
       " 'DW': 621,\n",
       " 'AKG': 622,\n",
       " 'PEG': 623,\n",
       " 'YH': 624,\n",
       " 'VGG': 625,\n",
       " 'AAS': 626,\n",
       " 'VAG': 627,\n",
       " 'TLL': 628,\n",
       " 'VSL': 629,\n",
       " 'KAL': 630,\n",
       " 'RAL': 631,\n",
       " 'KEG': 632,\n",
       " 'VLS': 633,\n",
       " 'FW': 634,\n",
       " 'KEL': 635,\n",
       " 'KKL': 636,\n",
       " 'EM': 637,\n",
       " 'ĠMS': 638,\n",
       " 'VTL': 639,\n",
       " 'VEL': 640,\n",
       " 'FLG': 641,\n",
       " 'CN': 642,\n",
       " 'APE': 643,\n",
       " 'CF': 644,\n",
       " 'VDL': 645,\n",
       " 'VAE': 646,\n",
       " 'VGL': 647,\n",
       " 'APS': 648,\n",
       " 'PLG': 649,\n",
       " 'ANG': 650,\n",
       " 'MF': 651,\n",
       " 'IDG': 652,\n",
       " 'SLE': 653,\n",
       " 'KKG': 654,\n",
       " 'VEE': 655,\n",
       " 'AKE': 656,\n",
       " 'ILL': 657,\n",
       " 'TAL': 658,\n",
       " 'AYL': 659,\n",
       " 'IAA': 660,\n",
       " 'ILE': 661,\n",
       " 'WQ': 662,\n",
       " 'WR': 663,\n",
       " 'AVE': 664,\n",
       " 'GLP': 665,\n",
       " 'CY': 666,\n",
       " 'CQ': 667,\n",
       " 'REG': 668,\n",
       " 'GLD': 669,\n",
       " 'VSG': 670,\n",
       " 'GLE': 671,\n",
       " 'VSS': 672,\n",
       " 'ILS': 673,\n",
       " 'RRG': 674,\n",
       " 'ARE': 675,\n",
       " 'TAA': 676,\n",
       " 'DLL': 677,\n",
       " 'WN': 678,\n",
       " 'DLE': 679,\n",
       " 'YM': 680,\n",
       " 'DLS': 681,\n",
       " 'VKG': 682,\n",
       " 'IAL': 683,\n",
       " 'NLG': 684,\n",
       " 'DAL': 685,\n",
       " 'RAA': 686,\n",
       " 'TGE': 687,\n",
       " 'REL': 688,\n",
       " 'CK': 689,\n",
       " 'QLG': 690,\n",
       " 'VVE': 691,\n",
       " 'QAL': 692,\n",
       " 'RLE': 693,\n",
       " 'IEG': 694,\n",
       " 'AML': 695,\n",
       " 'RLS': 696,\n",
       " 'VLD': 697,\n",
       " 'QLL': 698,\n",
       " 'CI': 699,\n",
       " 'AIE': 700,\n",
       " 'GGS': 701,\n",
       " 'TLS': 702,\n",
       " 'TLE': 703,\n",
       " 'KAA': 704,\n",
       " 'VPL': 705,\n",
       " 'ELS': 706,\n",
       " 'AFG': 707,\n",
       " 'AQG': 708,\n",
       " 'KLE': 709,\n",
       " 'NW': 710,\n",
       " 'VAD': 711,\n",
       " 'RVL': 712,\n",
       " 'SLP': 713,\n",
       " 'VTG': 714,\n",
       " 'AGE': 715,\n",
       " 'VIL': 716,\n",
       " 'VKE': 717,\n",
       " 'IDE': 718,\n",
       " 'KVL': 719,\n",
       " 'MY': 720,\n",
       " 'SGS': 721,\n",
       " 'VAS': 722,\n",
       " 'LLP': 723,\n",
       " 'QW': 724,\n",
       " 'AHL': 725,\n",
       " 'AVS': 726,\n",
       " 'DC': 727,\n",
       " 'IGG': 728,\n",
       " 'VAI': 729,\n",
       " 'NC': 730,\n",
       " 'PW': 731,\n",
       " 'IAG': 732,\n",
       " 'KLS': 733,\n",
       " 'VAT': 734,\n",
       " 'KSL': 735,\n",
       " 'VAR': 736,\n",
       " 'IAE': 737,\n",
       " 'KTL': 738,\n",
       " 'VKK': 739,\n",
       " 'IEE': 740,\n",
       " 'VDE': 741,\n",
       " 'VVS': 742,\n",
       " 'MH': 743,\n",
       " 'ALD': 744,\n",
       " 'WF': 745,\n",
       " 'KIL': 746,\n",
       " 'AGS': 747,\n",
       " 'WT': 748,\n",
       " 'RRL': 749,\n",
       " 'KEE': 750,\n",
       " 'CH': 751,\n",
       " 'VRL': 752,\n",
       " 'ISS': 753,\n",
       " 'TSS': 754,\n",
       " 'VRE': 755,\n",
       " 'PLS': 756,\n",
       " 'VLP': 757,\n",
       " 'VKL': 758,\n",
       " 'QAA': 759,\n",
       " 'KRL': 760,\n",
       " 'ASE': 761,\n",
       " 'ADE': 762,\n",
       " 'QC': 763,\n",
       " 'KAV': 764,\n",
       " 'PAA': 765,\n",
       " 'YLG': 766,\n",
       " 'VGE': 767,\n",
       " 'QLE': 768,\n",
       " 'PLL': 769,\n",
       " 'VAK': 770,\n",
       " 'NLL': 771,\n",
       " 'VIE': 772,\n",
       " 'TGG': 773,\n",
       " 'TVL': 774,\n",
       " 'ALR': 775,\n",
       " 'DAA': 776,\n",
       " 'ĠML': 777,\n",
       " 'SGE': 778,\n",
       " 'KKE': 779,\n",
       " 'KDL': 780,\n",
       " 'YW': 781,\n",
       " 'YC': 782,\n",
       " 'KGL': 783,\n",
       " 'TGS': 784,\n",
       " 'PC': 785,\n",
       " 'IVG': 786,\n",
       " 'IDL': 787,\n",
       " 'VSE': 788,\n",
       " 'SLD': 789,\n",
       " 'VIS': 790,\n",
       " 'VVD': 791,\n",
       " 'VPG': 792,\n",
       " 'RGG': 793,\n",
       " 'HM': 794,\n",
       " 'NNNN': 795,\n",
       " 'RSL': 796,\n",
       " 'TGL': 797,\n",
       " 'NAL': 798,\n",
       " 'VRR': 799,\n",
       " 'IAR': 800,\n",
       " 'ISL': 801,\n",
       " 'AYG': 802,\n",
       " 'IKE': 803,\n",
       " 'RIL': 804,\n",
       " 'KAI': 805,\n",
       " 'AAAA': 806,\n",
       " 'WP': 807,\n",
       " 'FLS': 808,\n",
       " 'ELD': 809,\n",
       " 'IGL': 810,\n",
       " 'CC': 811,\n",
       " 'IAV': 812,\n",
       " 'TSL': 813,\n",
       " 'FLL': 814,\n",
       " 'SEG': 815,\n",
       " 'PLP': 816,\n",
       " 'ISE': 817,\n",
       " 'QLS': 818,\n",
       " 'QRL': 819,\n",
       " 'DEG': 820,\n",
       " 'NLS': 821,\n",
       " 'RGL': 822,\n",
       " 'HC': 823,\n",
       " 'RAV': 824,\n",
       " 'IIG': 825,\n",
       " 'ISG': 826,\n",
       " 'DVL': 827,\n",
       " 'FEG': 828,\n",
       " 'IVL': 829,\n",
       " 'FLE': 830,\n",
       " 'KAG': 831,\n",
       " 'RKL': 832,\n",
       " 'HLG': 833,\n",
       " 'IAS': 834,\n",
       " 'IKK': 835,\n",
       " 'NLE': 836,\n",
       " 'AIS': 837,\n",
       " 'TAE': 838,\n",
       " 'DEE': 839,\n",
       " 'DGS': 840,\n",
       " 'RAG': 841,\n",
       " 'TTL': 842,\n",
       " 'VNG': 843,\n",
       " 'ATS': 844,\n",
       " 'RDL': 845,\n",
       " 'RGE': 846,\n",
       " 'WY': 847,\n",
       " 'IVE': 848,\n",
       " 'VID': 849,\n",
       " 'ALP': 850,\n",
       " 'KVV': 851,\n",
       " 'TEE': 852,\n",
       " 'IAD': 853,\n",
       " 'RVV': 854,\n",
       " 'RTL': 855,\n",
       " 'AAD': 856,\n",
       " 'IGE': 857,\n",
       " 'ITL': 858,\n",
       " 'VTE': 859,\n",
       " 'QEL': 860,\n",
       " 'KAK': 861,\n",
       " 'RAR': 862,\n",
       " 'SSE': 863,\n",
       " 'IEL': 864,\n",
       " 'VGS': 865,\n",
       " 'PPG': 866,\n",
       " 'DIL': 867,\n",
       " 'TAV': 868,\n",
       " 'KAE': 869,\n",
       " 'PLE': 870,\n",
       " 'IAK': 871,\n",
       " 'FGG': 872,\n",
       " 'KGE': 873,\n",
       " 'TEL': 874,\n",
       " 'KNL': 875,\n",
       " 'VRG': 876,\n",
       " 'VAQ': 877,\n",
       " 'AHG': 878,\n",
       " 'HLL': 879,\n",
       " 'VPE': 880,\n",
       " 'QVL': 881,\n",
       " 'TVE': 882,\n",
       " 'DEL': 883,\n",
       " 'QQQQ': 884,\n",
       " 'PEL': 885,\n",
       " 'VNL': 886,\n",
       " 'VPS': 887,\n",
       " 'KDG': 888,\n",
       " 'ATE': 889,\n",
       " 'IIL': 890,\n",
       " 'TAG': 891,\n",
       " 'KSS': 892,\n",
       " 'TVV': 893,\n",
       " 'VTS': 894,\n",
       " 'QKL': 895,\n",
       " 'PGS': 896,\n",
       " 'RRE': 897,\n",
       " 'MLG': 898,\n",
       " 'IVS': 899,\n",
       " 'ITS': 900,\n",
       " 'NSL': 901,\n",
       " 'ITG': 902,\n",
       " 'QTL': 903,\n",
       " 'VFL': 904,\n",
       " 'QAV': 905,\n",
       " 'AES': 906,\n",
       " 'QQL': 907,\n",
       " 'TAT': 908,\n",
       " 'IVV': 909,\n",
       " 'IPL': 910,\n",
       " 'IID': 911,\n",
       " 'TPE': 912,\n",
       " 'TAS': 913,\n",
       " 'KQL': 914,\n",
       " 'IIE': 915,\n",
       " 'TDL': 916,\n",
       " 'TEG': 917,\n",
       " 'AAR': 918,\n",
       " 'IAT': 919,\n",
       " 'PAL': 920,\n",
       " 'GGE': 921,\n",
       " 'VGD': 922,\n",
       " 'ILD': 923,\n",
       " 'IVD': 924,\n",
       " 'TPL': 925,\n",
       " 'IGS': 926,\n",
       " 'LLN': 927,\n",
       " 'QSL': 928,\n",
       " 'KVI': 929,\n",
       " 'ITE': 930,\n",
       " 'HW': 931,\n",
       " 'IRE': 932,\n",
       " 'TVT': 933,\n",
       " 'VQL': 934,\n",
       " 'PSS': 935,\n",
       " 'IKL': 936,\n",
       " 'KVE': 937,\n",
       " 'AGD': 938,\n",
       " 'REE': 939,\n",
       " 'VKS': 940,\n",
       " 'RAE': 941,\n",
       " 'DGE': 942,\n",
       " 'NKL': 943,\n",
       " 'AQE': 944,\n",
       " 'QEG': 945,\n",
       " 'IKS': 946,\n",
       " 'IIS': 947,\n",
       " 'PGE': 948,\n",
       " 'KGG': 949,\n",
       " 'ARS': 950,\n",
       " 'TIL': 951,\n",
       " 'IAI': 952,\n",
       " 'FGS': 953,\n",
       " 'VDS': 954,\n",
       " 'KSG': 955,\n",
       " 'DLP': 956,\n",
       " 'RQL': 957,\n",
       " 'AVD': 958,\n",
       " 'KKS': 959,\n",
       " 'TAI': 960,\n",
       " 'VAN': 961,\n",
       " 'NIL': 962,\n",
       " 'TSG': 963,\n",
       " 'KPL': 964,\n",
       " 'SSSS': 965,\n",
       " 'AAP': 966,\n",
       " 'ĠMG': 967,\n",
       " 'AMG': 968,\n",
       " 'VAP': 969,\n",
       " 'RSG': 970,\n",
       " 'INE': 971,\n",
       " 'IES': 972,\n",
       " 'PSP': 973,\n",
       " 'RAI': 974,\n",
       " 'AKS': 975,\n",
       " 'DLD': 976,\n",
       " 'VES': 977,\n",
       " 'TKL': 978,\n",
       " 'VAF': 979,\n",
       " 'NVL': 980,\n",
       " 'FGL': 981,\n",
       " 'IRL': 982,\n",
       " 'TVS': 983,\n",
       " 'NAA': 984,\n",
       " 'KTT': 985,\n",
       " 'WM': 986,\n",
       " 'TRL': 987,\n",
       " 'FAA': 988,\n",
       " 'FSL': 989,\n",
       " 'NSS': 990,\n",
       " 'TLP': 991,\n",
       " 'DVV': 992,\n",
       " 'KAF': 993,\n",
       " 'VRS': 994,\n",
       " 'ADS': 995,\n",
       " 'RRS': 996,\n",
       " 'NGE': 997,\n",
       " 'RPL': 998,\n",
       " 'QIL': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, BertForMaskedLM\n",
    "\n",
    "protein_tokenizer = RobertaTokenizer(\n",
    "    vocab_file=\"data/target/bpe_tokenizer/vocab.json\", \n",
    "    merges_file=\"data/target/bpe_tokenizer/merges.txt\"\n",
    ")\n",
    "\n",
    "protein_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "303358d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from tdc.multi_pred import DTI\n",
    "\n",
    "davis = DTI(name=\"Davis\")\n",
    "davis_split = davis.get_split()\n",
    "\n",
    "fasta_seq = \" \".join(davis_split['train'].loc[0, \"Target\"])\n",
    "protein_smiles = protein_tokenizer(fasta_seq, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd1d7db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  0,  49, 225,  ..., 225,  48,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21db4f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  6817,  2268,   274,  1124,  3444,   467,    58,  1044,   362,\n",
       "           427,  1357,   385,  2455,  6871,   500,   357,   369,   364,   352,\n",
       "           349,   369,  7345,   299,   510,   383,  2787,   458,  5887,   423,\n",
       "           565,  9333,  1307,   486,  3990,  1387,   386,   350,   372,   307,\n",
       "           496,   334,   451,   381,  2917,   643,   851,   313,  1110,  6242,\n",
       "          1403,   384,   619,  1557,   283,   372,    59,  5076,   363,   634,\n",
       "          6876,   477,  6195,   418,  4034,  1678,   399,  1309,   410,   594,\n",
       "          1059,   269,  2939,  3371,  5455,   300,  1201,   380,   331,  2926,\n",
       "           272,   990,   462,   358,   410,  1605,  2228,   391,   348,   322,\n",
       "           283,  3422,   302,   281,   548,   309,   386,   523,  8763,   782,\n",
       "          3355,  1192,   338,    50,   678,  2578,    59,  2085,   391,   320,\n",
       "          9262,   379,   317,   678,  6753,   852,   268,    49,  1261,  1002,\n",
       "            50,   775,   305,  1641,   401,   462,  1233,   262,   733,  1044,\n",
       "           349,   322,   354,   458,  4242,   791,   348,   593,  4267,   503,\n",
       "           286,   684,  1202,   464,   544,   597,   522,   521,   475,  6040,\n",
       "           855,  1711,  1753,   332,   724,  8417,   362,   316,   343,   963,\n",
       "           391,  5097,  1580,   421,   261,   361,  5251,  3066,   340,  3388,\n",
       "           313,   283,   366,   268,   452,   359,  1707,  1930,   468,   307,\n",
       "           324,   372,   416,   291,   451,  4797,   899,  1660,   458,  7789,\n",
       "           749,  6550,   284,  1800,   517,   362,  2278,   863,  4035,   391,\n",
       "           341,   590,   380,  4911,   404,   300,   500,   381,   264,  1967,\n",
       "          4630,   311,   891,  2491, 10091,  1517,  1013,   278,   337,   908,\n",
       "          1182,  1745,   422,   397,  5180,  1162,  2258,   293,    54,   836,\n",
       "           284,   365,   845,  9375,   411,  9109,  5247,   379,  1569,  1961,\n",
       "          2135,  2208,   376,   428,   674,   778,   416,   331,  9057,   763,\n",
       "           670,   599,  9168,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_tokenizer(new_X[0], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a13fc27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tokenizer\n",
      "vocab size: 10261\n",
      "special tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, BertForMaskedLM\n",
    "\n",
    "protein_tokenizer = RobertaTokenizer.from_pretrained(\"data/target/protein_tokenizer\")\n",
    "vocab_size = len(protein_tokenizer.get_vocab().keys())\n",
    "\n",
    "print(f\"load tokenizer\\nvocab size: {vocab_size}\\nspecial tokens: {protein_tokenizer.all_special_tokens}\")\n",
    "\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=512,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=128,\n",
    "    type_vocab_size=1,\n",
    "    pad_token_id=1,\n",
    "    position_embedding_type=\"absolute\"\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2db90f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(10261, 128, padding_idx=1)\n",
       "      (position_embeddings): Embedding(128, 128, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=128, out_features=10261, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0987791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Downloading...\n",
      "100%|█████████████████████████████████████| 54.4M/54.4M [00:06<00:00, 8.70MiB/s]\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from tdc.multi_pred import DTI\n",
    "\n",
    "davis = DTI(name=\"Davis\")\n",
    "kiba = DTI(name=\"Kiba\")\n",
    "binding_db = DTI(name=\"BindingDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3613676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2021,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "davis_unique_target = np.unique(davis.get_data().Target.values)\n",
    "kiba_unique_target = np.unique(kiba.get_data().Target.values)\n",
    "binding_db_unique_target = np.unique(binding_db.get_data().Target.values)\n",
    "\n",
    "target_sequences = np.concatenate((davis_unique_target, kiba_unique_target, binding_db_unique_target))\n",
    "target_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2531ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/target/target_sequences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(target_sequences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f8c58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
