{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305094b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/interaction/kiba/train_molecule.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12625/1447363121.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180595841/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  data = torch.tensor([d.squeeze(0).numpy() for d in data])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/interaction/kiba/train_protein.npy\n",
      "data/interaction/kiba/train_y.npy\n",
      "data/interaction/kiba/valid_molecule.npy\n",
      "data/interaction/kiba/valid_protein.npy\n",
      "data/interaction/kiba/valid_y.npy\n",
      "data/interaction/kiba/test_molecule.npy\n",
      "data/interaction/kiba/test_protein.npy\n",
      "data/interaction/kiba/test_y.npy\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    def get_split_dataset(mode):\n",
    "        dataset = {}\n",
    "        for f in [\"molecule\", \"protein\", \"y\"]:\n",
    "            f_path = os.path.join(path, mode + \"_\" + f + \".npy\")\n",
    "            print(f_path)\n",
    "            data = np.load(f_path, allow_pickle=True)\n",
    "            try:\n",
    "                data = torch.tensor([d.squeeze(0).numpy() for d in data])\n",
    "            except:\n",
    "                data = torch.tensor(data)\n",
    "            dataset[f] = data\n",
    "            \n",
    "        return dataset\n",
    "            \n",
    "    train_data = get_split_dataset(\"train\")\n",
    "    valid_data = get_split_dataset(\"valid\")\n",
    "    test_data = get_split_dataset(\"test\")\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "    \n",
    "train_data, valid_data, test_data = load_dataset(\"data/interaction/kiba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aef7dd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_data['molecule'], train_data['protein'], train_data['y'])\n",
    "valid_dataset = TensorDataset(valid_data['molecule'], valid_data['protein'], valid_data['y'])\n",
    "test_dataset = TensorDataset(test_data['molecule'], test_data['protein'], test_data['y'])\n",
    "    \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, num_workers=16, \n",
    "                              shuffle=True, pin_memory=True, prefetch_factor=10, \n",
    "                              drop_last=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=512, num_workers=16, \n",
    "                              shuffle=False, pin_memory=True, prefetch_factor=10, \n",
    "                              drop_last=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, num_workers=16, \n",
    "                             shuffle=False, pin_memory=True, prefetch_factor=10, \n",
    "                             drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93767d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConcatenateDTI(\n",
       "  (mol_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "  (prot_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc_1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc_out): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConcatenateDTI(nn.Module):\n",
    "    def __init__(self, molecule_dim=128, protein_dim=1024, inner_dim=512, projection=True):\n",
    "        super().__init__()\n",
    "        self.is_projection = projection\n",
    "\n",
    "        if self.is_projection:\n",
    "            self.mol_proj = nn.Linear(molecule_dim, inner_dim)        \n",
    "            self.prot_proj = nn.Linear(protein_dim, inner_dim)            \n",
    "            self.fc_1 = nn.Linear(inner_dim * 2, inner_dim)\n",
    "        else:\n",
    "            self.fc_1 = nn.Linear(molecule_dim + protein_dim, inner_dim)\n",
    "        \n",
    "        self.fc_2 = nn.Linear(inner_dim, int(inner_dim / 2))\n",
    "        self.fc_out = nn.Linear(int(inner_dim / 2), 1)\n",
    "   \n",
    "\n",
    "    def forward(self, molecule, protein):\n",
    "        if self.is_projection:\n",
    "            molecule = self.mol_proj(molecule)\n",
    "            protein = self.prot_proj(protein)\n",
    "            \n",
    "        x = torch.cat((molecule, protein), -1)\n",
    "        x = F.dropout(F.gelu(self.fc_1(x)), 0.1)\n",
    "        x = F.dropout(F.gelu(self.fc_2(x)), 0.1)\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "concatenate_dti = ConcatenateDTI()\n",
    "concatenate_dti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce531cdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0136],\n",
      "        [0.0205],\n",
      "        [0.0256],\n",
      "        [0.0158],\n",
      "        [0.0172],\n",
      "        [0.0151],\n",
      "        [0.0186],\n",
      "        [0.0225],\n",
      "        [0.0138],\n",
      "        [0.0127],\n",
      "        [0.0152],\n",
      "        [0.0235],\n",
      "        [0.0188],\n",
      "        [0.0077],\n",
      "        [0.0251],\n",
      "        [0.0230],\n",
      "        [0.0149],\n",
      "        [0.0187],\n",
      "        [0.0127],\n",
      "        [0.0133],\n",
      "        [0.0125],\n",
      "        [0.0238],\n",
      "        [0.0221],\n",
      "        [0.0179],\n",
      "        [0.0170],\n",
      "        [0.0096],\n",
      "        [0.0205],\n",
      "        [0.0196],\n",
      "        [0.0221],\n",
      "        [0.0146],\n",
      "        [0.0198],\n",
      "        [0.0214],\n",
      "        [0.0227],\n",
      "        [0.0151],\n",
      "        [0.0235],\n",
      "        [0.0135],\n",
      "        [0.0177],\n",
      "        [0.0206],\n",
      "        [0.0177],\n",
      "        [0.0152],\n",
      "        [0.0139],\n",
      "        [0.0239],\n",
      "        [0.0178],\n",
      "        [0.0151],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.0112],\n",
      "        [0.0142],\n",
      "        [0.0130],\n",
      "        [0.0222],\n",
      "        [0.0216],\n",
      "        [0.0251],\n",
      "        [0.0133],\n",
      "        [0.0142],\n",
      "        [0.0149],\n",
      "        [0.0123],\n",
      "        [0.0172],\n",
      "        [0.0164],\n",
      "        [0.0175],\n",
      "        [0.0237],\n",
      "        [0.0170],\n",
      "        [0.0167],\n",
      "        [0.0179],\n",
      "        [0.0179],\n",
      "        [0.0168],\n",
      "        [0.0200],\n",
      "        [0.0284],\n",
      "        [0.0087],\n",
      "        [0.0141],\n",
      "        [0.0137],\n",
      "        [0.0161],\n",
      "        [0.0208],\n",
      "        [0.0191],\n",
      "        [0.0217],\n",
      "        [0.0224],\n",
      "        [0.0168],\n",
      "        [0.0146],\n",
      "        [0.0141],\n",
      "        [0.0218],\n",
      "        [0.0237],\n",
      "        [0.0180],\n",
      "        [0.0188],\n",
      "        [0.0272],\n",
      "        [0.0229],\n",
      "        [0.0157],\n",
      "        [0.0221],\n",
      "        [0.0156],\n",
      "        [0.0094],\n",
      "        [0.0162],\n",
      "        [0.0154],\n",
      "        [0.0070],\n",
      "        [0.0154],\n",
      "        [0.0259],\n",
      "        [0.0255],\n",
      "        [0.0154],\n",
      "        [0.0170],\n",
      "        [0.0203],\n",
      "        [0.0224],\n",
      "        [0.0231],\n",
      "        [0.0158],\n",
      "        [0.0126],\n",
      "        [0.0209],\n",
      "        [0.0131],\n",
      "        [0.0208],\n",
      "        [0.0161],\n",
      "        [0.0118],\n",
      "        [0.0187],\n",
      "        [0.0182],\n",
      "        [0.0242],\n",
      "        [0.0146],\n",
      "        [0.0192],\n",
      "        [0.0196],\n",
      "        [0.0149],\n",
      "        [0.0235],\n",
      "        [0.0150],\n",
      "        [0.0135],\n",
      "        [0.0116],\n",
      "        [0.0088],\n",
      "        [0.0213],\n",
      "        [0.0083],\n",
      "        [0.0165],\n",
      "        [0.0195],\n",
      "        [0.0230],\n",
      "        [0.0165],\n",
      "        [0.0160],\n",
      "        [0.0164],\n",
      "        [0.0171],\n",
      "        [0.0132],\n",
      "        [0.0145],\n",
      "        [0.0139],\n",
      "        [0.0158],\n",
      "        [0.0160],\n",
      "        [0.0203],\n",
      "        [0.0152],\n",
      "        [0.0146],\n",
      "        [0.0085],\n",
      "        [0.0124],\n",
      "        [0.0165],\n",
      "        [0.0142],\n",
      "        [0.0204],\n",
      "        [0.0224],\n",
      "        [0.0203],\n",
      "        [0.0309],\n",
      "        [0.0131],\n",
      "        [0.0169],\n",
      "        [0.0190],\n",
      "        [0.0123],\n",
      "        [0.0164],\n",
      "        [0.0162],\n",
      "        [0.0266],\n",
      "        [0.0171],\n",
      "        [0.0142],\n",
      "        [0.0212],\n",
      "        [0.0147],\n",
      "        [0.0073],\n",
      "        [0.0273],\n",
      "        [0.0159],\n",
      "        [0.0155],\n",
      "        [0.0145],\n",
      "        [0.0144],\n",
      "        [0.0192],\n",
      "        [0.0232],\n",
      "        [0.0123],\n",
      "        [0.0177],\n",
      "        [0.0169],\n",
      "        [0.0072],\n",
      "        [0.0177],\n",
      "        [0.0218],\n",
      "        [0.0136],\n",
      "        [0.0212],\n",
      "        [0.0163],\n",
      "        [0.0127],\n",
      "        [0.0233],\n",
      "        [0.0115],\n",
      "        [0.0207],\n",
      "        [0.0142],\n",
      "        [0.0288],\n",
      "        [0.0309],\n",
      "        [0.0135],\n",
      "        [0.0199],\n",
      "        [0.0161],\n",
      "        [0.0238],\n",
      "        [0.0142],\n",
      "        [0.0194],\n",
      "        [0.0180],\n",
      "        [0.0165],\n",
      "        [0.0186],\n",
      "        [0.0143],\n",
      "        [0.0158],\n",
      "        [0.0116],\n",
      "        [0.0154],\n",
      "        [0.0119],\n",
      "        [0.0113],\n",
      "        [0.0246],\n",
      "        [0.0193],\n",
      "        [0.0162],\n",
      "        [0.0215],\n",
      "        [0.0207],\n",
      "        [0.0186],\n",
      "        [0.0179],\n",
      "        [0.0209],\n",
      "        [0.0237],\n",
      "        [0.0103],\n",
      "        [0.0214],\n",
      "        [0.0148],\n",
      "        [0.0139],\n",
      "        [0.0236],\n",
      "        [0.0222],\n",
      "        [0.0113],\n",
      "        [0.0241],\n",
      "        [0.0139],\n",
      "        [0.0168],\n",
      "        [0.0213],\n",
      "        [0.0188],\n",
      "        [0.0236],\n",
      "        [0.0176],\n",
      "        [0.0196],\n",
      "        [0.0203],\n",
      "        [0.0208],\n",
      "        [0.0266],\n",
      "        [0.0209],\n",
      "        [0.0171],\n",
      "        [0.0186],\n",
      "        [0.0176],\n",
      "        [0.0136],\n",
      "        [0.0252],\n",
      "        [0.0208],\n",
      "        [0.0240],\n",
      "        [0.0187],\n",
      "        [0.0197],\n",
      "        [0.0153],\n",
      "        [0.0218],\n",
      "        [0.0158],\n",
      "        [0.0206],\n",
      "        [0.0184],\n",
      "        [0.0198],\n",
      "        [0.0189],\n",
      "        [0.0245],\n",
      "        [0.0185],\n",
      "        [0.0171],\n",
      "        [0.0129],\n",
      "        [0.0159],\n",
      "        [0.0259],\n",
      "        [0.0186],\n",
      "        [0.0191],\n",
      "        [0.0105],\n",
      "        [0.0240],\n",
      "        [0.0204],\n",
      "        [0.0201],\n",
      "        [0.0182],\n",
      "        [0.0251],\n",
      "        [0.0214],\n",
      "        [0.0201],\n",
      "        [0.0193],\n",
      "        [0.0130],\n",
      "        [0.0218],\n",
      "        [0.0149],\n",
      "        [0.0116],\n",
      "        [0.0150],\n",
      "        [0.0130],\n",
      "        [0.0130],\n",
      "        [0.0249],\n",
      "        [0.0179],\n",
      "        [0.0084],\n",
      "        [0.0232],\n",
      "        [0.0187],\n",
      "        [0.0216],\n",
      "        [0.0195],\n",
      "        [0.0143],\n",
      "        [0.0117],\n",
      "        [0.0258],\n",
      "        [0.0180],\n",
      "        [0.0182],\n",
      "        [0.0247],\n",
      "        [0.0194],\n",
      "        [0.0157],\n",
      "        [0.0197],\n",
      "        [0.0128],\n",
      "        [0.0137],\n",
      "        [0.0199],\n",
      "        [0.0167],\n",
      "        [0.0142],\n",
      "        [0.0208],\n",
      "        [0.0187],\n",
      "        [0.0187],\n",
      "        [0.0185],\n",
      "        [0.0196],\n",
      "        [0.0166],\n",
      "        [0.0156],\n",
      "        [0.0116],\n",
      "        [0.0200],\n",
      "        [0.0128],\n",
      "        [0.0179],\n",
      "        [0.0180],\n",
      "        [0.0159],\n",
      "        [0.0126],\n",
      "        [0.0187],\n",
      "        [0.0186],\n",
      "        [0.0176],\n",
      "        [0.0207],\n",
      "        [0.0198],\n",
      "        [0.0209],\n",
      "        [0.0205],\n",
      "        [0.0150],\n",
      "        [0.0158],\n",
      "        [0.0169],\n",
      "        [0.0145],\n",
      "        [0.0174],\n",
      "        [0.0131],\n",
      "        [0.0175],\n",
      "        [0.0200],\n",
      "        [0.0176],\n",
      "        [0.0170],\n",
      "        [0.0257],\n",
      "        [0.0202],\n",
      "        [0.0160],\n",
      "        [0.0114],\n",
      "        [0.0089],\n",
      "        [0.0177],\n",
      "        [0.0195],\n",
      "        [0.0219],\n",
      "        [0.0081],\n",
      "        [0.0113],\n",
      "        [0.0194],\n",
      "        [0.0188],\n",
      "        [0.0187],\n",
      "        [0.0176],\n",
      "        [0.0207],\n",
      "        [0.0176],\n",
      "        [0.0175],\n",
      "        [0.0164],\n",
      "        [0.0062],\n",
      "        [0.0172],\n",
      "        [0.0232],\n",
      "        [0.0211],\n",
      "        [0.0068],\n",
      "        [0.0211],\n",
      "        [0.0110],\n",
      "        [0.0197],\n",
      "        [0.0144],\n",
      "        [0.0155],\n",
      "        [0.0159],\n",
      "        [0.0247],\n",
      "        [0.0213],\n",
      "        [0.0176],\n",
      "        [0.0218],\n",
      "        [0.0148],\n",
      "        [0.0153],\n",
      "        [0.0209],\n",
      "        [0.0173],\n",
      "        [0.0163],\n",
      "        [0.0188],\n",
      "        [0.0177],\n",
      "        [0.0214],\n",
      "        [0.0172],\n",
      "        [0.0100],\n",
      "        [0.0167],\n",
      "        [0.0264],\n",
      "        [0.0246],\n",
      "        [0.0186],\n",
      "        [0.0122],\n",
      "        [0.0171],\n",
      "        [0.0130],\n",
      "        [0.0162],\n",
      "        [0.0231],\n",
      "        [0.0190],\n",
      "        [0.0175],\n",
      "        [0.0203],\n",
      "        [0.0112],\n",
      "        [0.0237],\n",
      "        [0.0178],\n",
      "        [0.0196],\n",
      "        [0.0150],\n",
      "        [0.0211],\n",
      "        [0.0065],\n",
      "        [0.0154],\n",
      "        [0.0164],\n",
      "        [0.0220],\n",
      "        [0.0251],\n",
      "        [0.0157],\n",
      "        [0.0218],\n",
      "        [0.0188],\n",
      "        [0.0214],\n",
      "        [0.0241],\n",
      "        [0.0168],\n",
      "        [0.0209],\n",
      "        [0.0158],\n",
      "        [0.0160],\n",
      "        [0.0210],\n",
      "        [0.0154],\n",
      "        [0.0188],\n",
      "        [0.0121],\n",
      "        [0.0131],\n",
      "        [0.0244],\n",
      "        [0.0214],\n",
      "        [0.0251],\n",
      "        [0.0157],\n",
      "        [0.0111],\n",
      "        [0.0147],\n",
      "        [0.0149],\n",
      "        [0.0197],\n",
      "        [0.0207],\n",
      "        [0.0177],\n",
      "        [0.0188],\n",
      "        [0.0225],\n",
      "        [0.0224],\n",
      "        [0.0140],\n",
      "        [0.0180],\n",
      "        [0.0208],\n",
      "        [0.0305],\n",
      "        [0.0168],\n",
      "        [0.0188],\n",
      "        [0.0209],\n",
      "        [0.0190],\n",
      "        [0.0195],\n",
      "        [0.0154],\n",
      "        [0.0149],\n",
      "        [0.0170],\n",
      "        [0.0188],\n",
      "        [0.0210],\n",
      "        [0.0234],\n",
      "        [0.0123],\n",
      "        [0.0170],\n",
      "        [0.0133],\n",
      "        [0.0183],\n",
      "        [0.0167],\n",
      "        [0.0210],\n",
      "        [0.0125],\n",
      "        [0.0141],\n",
      "        [0.0120],\n",
      "        [0.0094],\n",
      "        [0.0125],\n",
      "        [0.0230],\n",
      "        [0.0182],\n",
      "        [0.0131],\n",
      "        [0.0107],\n",
      "        [0.0123],\n",
      "        [0.0167],\n",
      "        [0.0141],\n",
      "        [0.0102],\n",
      "        [0.0170],\n",
      "        [0.0175],\n",
      "        [0.0061],\n",
      "        [0.0144],\n",
      "        [0.0170],\n",
      "        [0.0164],\n",
      "        [0.0183],\n",
      "        [0.0181],\n",
      "        [0.0193],\n",
      "        [0.0209],\n",
      "        [0.0188],\n",
      "        [0.0179],\n",
      "        [0.0157],\n",
      "        [0.0214],\n",
      "        [0.0108],\n",
      "        [0.0221],\n",
      "        [0.0221],\n",
      "        [0.0206],\n",
      "        [0.0264],\n",
      "        [0.0191],\n",
      "        [0.0253],\n",
      "        [0.0138],\n",
      "        [0.0178],\n",
      "        [0.0148],\n",
      "        [0.0154],\n",
      "        [0.0176],\n",
      "        [0.0178],\n",
      "        [0.0241],\n",
      "        [0.0196],\n",
      "        [0.0120],\n",
      "        [0.0206],\n",
      "        [0.0256],\n",
      "        [0.0140],\n",
      "        [0.0179],\n",
      "        [0.0144],\n",
      "        [0.0250],\n",
      "        [0.0207],\n",
      "        [0.0199],\n",
      "        [0.0214],\n",
      "        [0.0219],\n",
      "        [0.0136],\n",
      "        [0.0106],\n",
      "        [0.0087],\n",
      "        [0.0221],\n",
      "        [0.0188],\n",
      "        [0.0238],\n",
      "        [0.0162],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.0164],\n",
      "        [0.0189],\n",
      "        [0.0147],\n",
      "        [0.0224],\n",
      "        [0.0274],\n",
      "        [0.0113],\n",
      "        [0.0263],\n",
      "        [0.0116],\n",
      "        [0.0210],\n",
      "        [0.0135],\n",
      "        [0.0218],\n",
      "        [0.0201],\n",
      "        [0.0253],\n",
      "        [0.0232],\n",
      "        [0.0187],\n",
      "        [0.0265],\n",
      "        [0.0200],\n",
      "        [0.0236],\n",
      "        [0.0214],\n",
      "        [0.0188],\n",
      "        [0.0237],\n",
      "        [0.0188],\n",
      "        [0.0162]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    y_hat = concatenate_dti(batch[0], batch[1])\n",
    "    print(y_hat)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573e317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
