{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f68e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torchtext import data, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# with open(\"data/chem_total.pickle\", 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "with open(\"data/molecule_small.pickle\", 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf5d1d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CC1=CC=C(C=C1)N2C(=CC(=C2C)C(=O)CN3CCN(CC3)CC(=O)NC4=C(C=CC=C4C)C)C'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6dca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "SRC = torchtext.legacy.data.Field(tokenize=None,\n",
    "                                init_token='<CLS>',\n",
    "                                eos_token='<SEP>',\n",
    "                                pad_token='<PAD>',\n",
    "                                unk_token='<MASK>',\n",
    "                                lower=False,\n",
    "                                batch_first=False,\n",
    "                                include_lengths=False)\n",
    "\n",
    "SRC.build_vocab(data, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9daeb330",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC1=CC=C(C=C1)N2C(=CC(=C2C)C(=O)CN3CCN(CC3)CC(=O)NC4=C(C=CC=C4C)C)C']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.preprocess(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6cc9acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularLangaugeModelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, seq_len=128, masking_rate=0.15):\n",
    "        super(MolecularLangaugeModelDataset, self).__init__()\n",
    "\n",
    "        self.data          = data        \n",
    "        self.tokenizer     = tokenizer\n",
    "        self.vocab         = tokenizer.vocab\n",
    "        self.seq_len       = seq_len\n",
    "        self.masking_rate  = masking_rate\n",
    "        \n",
    "        self.cls_token_id  = self.tokenizer.vocab.stoi[self.tokenizer.init_token]\n",
    "        self.sep_token_id  = self.tokenizer.vocab.stoi[self.tokenizer.eos_token]\n",
    "        self.pad_token_id  = self.tokenizer.vocab.stoi[self.tokenizer.pad_token]\n",
    "        self.mask_token_id = self.tokenizer.vocab.stoi[self.tokenizer.unk_token]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        target = self.tokenizer.numericalize(self.data[idx]).squeeze()\n",
    "        \n",
    "        if len(target) < self.seq_len - 2:\n",
    "            pad_length = self.seq_len - len(target) - 2\n",
    "        else:\n",
    "            target = target[:self.seq_len-2]\n",
    "            pad_length = 0\n",
    "               \n",
    "        masked_sent, masking_label = self.masking(target)\n",
    "        \n",
    "        # MLM\n",
    "        train = torch.cat([\n",
    "            torch.tensor([self.cls_token_id]), \n",
    "            masked_sent,\n",
    "            torch.tensor([self.sep_token_id]),\n",
    "            torch.tensor([self.pad_token_id] * pad_length)\n",
    "        ]).long().contiguous()\n",
    "        \n",
    "        target = torch.cat([\n",
    "            torch.tensor([self.cls_token_id]), \n",
    "            target,\n",
    "            torch.tensor([self.sep_token_id]),\n",
    "            torch.tensor([self.pad_token_id] * pad_length)\n",
    "        ]).long().contiguous()\n",
    "        \n",
    "        masking_label = torch.cat([\n",
    "            torch.zeros(1), \n",
    "            masking_label,\n",
    "            torch.zeros(1),\n",
    "            torch.zeros(pad_length)\n",
    "        ])\n",
    "                \n",
    "        segment_embedding = torch.zeros(target.size(0))\n",
    "        \n",
    "        return train, target, segment_embedding, masking_label\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        for x in self.data:\n",
    "            yield x\n",
    "            \n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    \n",
    "    # TODO mask 안에서 random 으로 바꿔주는 것 추가\n",
    "    def masking(self, x):\n",
    "        x             = torch.tensor(x).long().contiguous()\n",
    "        masking_idx   = torch.randperm(x.size()[0])[:round(x.size()[0] * self.masking_rate) + 1]       \n",
    "        masking_label = torch.zeros(x.size()[0])\n",
    "        masking_label[masking_idx] = 1\n",
    "        x             = x.masked_fill(masking_label.bool(), self.mask_token_id)\n",
    "        \n",
    "        return x, masking_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d670ddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  4,  8, 10,  4,  6,  5,  8,  4,  5,  8, 10,  7,  4, 11,  5,  0,  0,\n",
      "          6,  5,  8,  0,  5,  4, 11,  0,  8,  0,  4, 12,  5,  4,  8,  6,  8,  0,\n",
      "          4, 12,  7,  4,  4, 13,  0,  4,  4,  0,  4,  4,  5,  0, 13,  3,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1]])\n",
      "tensor([[ 2,  4,  8, 10,  4,  6,  5,  8,  4,  5,  8, 10,  7,  4, 11,  5,  8,  4,\n",
      "          6,  5,  8,  4,  5,  4, 11,  7,  8,  4,  4, 12,  5,  4,  8,  6,  8,  5,\n",
      "          4, 12,  7,  4,  4, 13,  5,  4,  4,  5,  4,  4,  5,  4, 13,  3,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-72-d0a8450635fa>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x             = torch.tensor(x).long().contiguous()\n"
     ]
    }
   ],
   "source": [
    "dataset = MolecularLangaugeModelDataset(data, SRC, seq_len=128, masking_rate=0.15)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for train, target, sengment_embedding, masking_label in data_loader:\n",
    "    print(train)\n",
    "    print(target)\n",
    "#     print(sengment_embedding)\n",
    "#     print(masking_label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887bacf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
