{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53b247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/rishikksh20/CrossViT-pytorch/blob/master/crossvit.py\n",
    "\n",
    "import torch\n",
    "from torch import einsum, nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, input_dim=128, intermediate_dim=256, heads=4, dropout=0.):\n",
    "        super().__init__()\n",
    "        project_out = input_dim\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = (input_dim / heads) ** -0.5\n",
    "\n",
    "        self.key = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "        self.value = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "        self.query = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(intermediate_dim, project_out),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, data):\n",
    "        b, n, d, h = *data.shape, self.heads\n",
    "\n",
    "        k = self.key(data)\n",
    "        k = rearrange(k, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        v = self.value(data)\n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h=h)\n",
    "        \n",
    "        # get only cls token\n",
    "        q = self.query(data[:, 0].unsqueeze(1))\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attention = dots.softmax(dim=-1)\n",
    "\n",
    "        output = einsum('b h i j, b h j d -> b h i d', attention, v)\n",
    "        output = rearrange(output, 'b h n d -> b n (h d)')\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "    \n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 molecule_dim=128, molecule_intermediate_dim=256,\n",
    "                 protein_dim=1024, protein_intermediate_dim=2048,\n",
    "                 cross_attn_depth=1, cross_attn_heads=4, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.cross_attn_layers = nn.ModuleList([])\n",
    "        for _ in range(cross_attn_depth):\n",
    "            self.cross_attn_layers.append(nn.ModuleList([\n",
    "                nn.Linear(molecule_dim, protein_dim),\n",
    "                nn.Linear(protein_dim, molecule_dim),\n",
    "                PreNorm(protein_dim, CrossAttention(\n",
    "                    protein_dim, protein_intermediate_dim, cross_attn_heads, dropout\n",
    "                )),\n",
    "                nn.Linear(protein_dim, molecule_dim),\n",
    "                nn.Linear(molecule_dim, protein_dim),\n",
    "                PreNorm(molecule_dim, CrossAttention(\n",
    "                    molecule_dim, molecule_intermediate_dim, cross_attn_heads, dropout\n",
    "                ))\n",
    "            ]))\n",
    "\n",
    "            \n",
    "    def forward(self, molecule, protein):\n",
    "        for f_sl, g_ls, cross_attn_s, f_ls, g_sl, cross_attn_l in self.cross_attn_layers:\n",
    "            \n",
    "            cls_molecule = molecule[:, 0]\n",
    "            x_molecule = molecule[:, 1:]\n",
    "            \n",
    "            cls_protein = protein[:, 0]\n",
    "            x_protein = protein[:, 1:]\n",
    "\n",
    "            # Cross attention for protein sequence\n",
    "            cal_q = f_ls(cls_protein.unsqueeze(1))\n",
    "            cal_qkv = torch.cat((cal_q, x_molecule), dim=1)\n",
    "            cal_out = cal_q + cross_attn_l(cal_qkv)\n",
    "            cal_out = g_sl(cal_out)\n",
    "            protein = torch.cat((cal_out, x_protein), dim=1)\n",
    "\n",
    "            # Cross attention for molecule sequence\n",
    "            cal_q = f_sl(cls_molecule.unsqueeze(1))\n",
    "            cal_qkv = torch.cat((cal_q, x_protein), dim=1)\n",
    "            cal_out = cal_q + cross_attn_s(cal_qkv)\n",
    "            cal_out = g_ls(cal_out)\n",
    "            molecule = torch.cat((cal_out, x_molecule), dim=1)\n",
    "\n",
    "        return molecule, protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5434271a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossAttentionLayer(\n",
       "  (cross_attn_layers): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      (2): PreNorm(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fn): CrossAttention(\n",
       "          (key): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (value): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (query): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      (4): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      (5): PreNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (fn): CrossAttention(\n",
       "          (key): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (value): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (query): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      (2): PreNorm(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fn): CrossAttention(\n",
       "          (key): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (value): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (query): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      (4): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      (5): PreNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (fn): CrossAttention(\n",
       "          (key): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (value): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (query): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_attention_layer = CrossAttentionLayer(cross_attn_depth=2)\n",
    "cross_attention_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b8fafbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0765, -0.0243,  0.0252,  ..., -0.0181, -0.1369, -0.0219],\n",
       "          [ 0.0341,  0.0771,  0.2278,  ...,  0.6557,  0.9227,  0.6813],\n",
       "          [ 0.6740,  0.3114,  0.1296,  ...,  0.5097,  0.7271,  0.6912],\n",
       "          ...,\n",
       "          [ 0.9594,  0.0214,  0.1743,  ...,  0.4185,  0.4760,  0.4756],\n",
       "          [ 0.4065,  0.2358,  0.2313,  ...,  0.9924,  0.4578,  0.0556],\n",
       "          [ 0.4851,  0.2601,  0.6943,  ...,  0.8129,  0.1236,  0.8042]]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " tensor([[[-0.0437, -0.0517, -0.1115,  ...,  0.0574, -0.0288, -0.0726],\n",
       "          [ 0.4962,  0.3244,  0.2906,  ...,  0.1367,  0.6601,  0.1835],\n",
       "          [ 0.5011,  0.4542,  0.6685,  ...,  0.2757,  0.8375,  0.4043],\n",
       "          ...,\n",
       "          [ 0.6170,  0.4904,  0.1649,  ...,  0.9705,  0.5077,  0.9278],\n",
       "          [ 0.2185,  0.5124,  0.2504,  ...,  0.6507,  0.9039,  0.9518],\n",
       "          [ 0.3343,  0.7399,  0.8789,  ...,  0.6956,  0.2932,  0.1575]]],\n",
       "        grad_fn=<CatBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecule_seq = torch.rand((1, 23, 128))\n",
    "protein_seq = torch.rand((1, 512, 1024))\n",
    "\n",
    "cross_attention_layer(molecule_seq, protein_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afdb16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
